<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>FengerBlog</title>
  
  <subtitle>FengerBlog</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://num-github.github.io/"/>
  <updated>2019-09-02T17:06:06.792Z</updated>
  <id>https://num-github.github.io/</id>
  
  <author>
    <name>Fenger</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>imitation learning 2</title>
    <link href="https://num-github.github.io/2019/09/02/imitation-learning-2/"/>
    <id>https://num-github.github.io/2019/09/02/imitation-learning-2/</id>
    <published>2019-09-02T17:06:06.000Z</published>
    <updated>2019-09-02T17:06:06.792Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>imitation learning</title>
    <link href="https://num-github.github.io/2019/08/12/imitation-learning-md/"/>
    <id>https://num-github.github.io/2019/08/12/imitation-learning-md/</id>
    <published>2019-08-12T19:25:12.000Z</published>
    <updated>2019-08-30T22:57:36.130Z</updated>
    
    <content type="html"><![CDATA[<p><em>This note is mainly based on <a href="https://www.youtube.com/watch?v=6rZTaboSY4k&amp;feature=youtu.be" target="_blank" rel="noopener">Artificial Intelligence Imitation Learning - Tutorial - 2018 ICML</a></em></p><h4 id="Notation-amp-Setup"><a href="#Notation-amp-Setup" class="headerlink" title="Notation &amp; Setup"></a>Notation &amp; Setup</h4><p>Same as reinforcement learning: </p><p>State: <strong>s</strong>       </p><p>Action: <strong>a</strong>      </p><p>Policy:  <strong>π<sub>θ</sub></strong>   </p><p>State Dynamics (or environment): <strong>P(s’|s,a)</strong> —— given current state &amp; actions, what is the next state.</p><p>Rollout: sequential execute <strong>π(s<sub>0</sub>)</strong> on an initial state —-produce trajectory <strong>τ=(s<sub>0</sub>, a<sub>0</sub>, s<sub>1</sub>, a<sub>1</sub> ……)</strong></p><p><strong>P(τ|π)</strong>: distribution of trajectories induced by a policy—- depend on the randomness of policy and randomness of state dynamics</p><p>​        1. Sample s<sub>0</sub> from P<sub>0</sub> (distribution over initial state), initialize t=1.</p><p>​        2. Sample action a<sub>i</sub> from π(s<sub>t-1</sub>)</p><p>​        3. Sample next state s<sub>t</sub> from applying a<sub>t</sub> to s<sub> t-1</sub> (require access to environment)</p><p>​        4. Repeat from Step 2 with t=t+1</p><p><strong>P(s|π)</strong>: distribution of states induced by a policy</p><p>​        Let P<sub>t</sub> (s|π) denote distribution over t-th state</p><p>​        P(s|π)=(1/T)∑P<sub>t</sub>(s|π) </p><h5 id="Behavioral-Cloning-Reduction-to-supervised-learning"><a href="#Behavioral-Cloning-Reduction-to-supervised-learning" class="headerlink" title="Behavioral Cloning (Reduction to supervised learning ):"></a>Behavioral Cloning (Reduction to supervised learning ):</h5><p>Define P<em> = P(s|π</em>) (distribution of states visited by expert ) ( the * notation here means expert)</p><h5 id="The-learning-objective"><a href="#The-learning-objective" class="headerlink" title="The learning objective:"></a>The learning objective:</h5><p> apply supervised learning to minimize the error between the action provided by experts and the action provided by our policy, where L is the loss function.</p><script type="math/tex; mode=display">arg min_θ E_{(s,a^*)\sim P^*} L(a^*,π_θ(s))</script><p>Note: the training data was collected by exogenously queuing from experts to provide state and action pairs. </p><h5 id="Two-interpretations-of-what-behavioral-cloning-is-doing"><a href="#Two-interpretations-of-what-behavioral-cloning-is-doing" class="headerlink" title="Two interpretations of what behavioral cloning is doing:"></a>Two interpretations of what behavioral cloning is doing:</h5><p>​        Assuming perfect imitation so far, learning to continue imitating perfectly </p><p>​        Minimize 1-step deviation error along the expert trajectories</p><h5 id="Compared-with-general-imitation-learning"><a href="#Compared-with-general-imitation-learning" class="headerlink" title="Compared with general imitation learning"></a>Compared with general imitation learning</h5><p>The learning objective for general imitation learning</p><script type="math/tex; mode=display">arg min_θ E_{s\sim P(s|θ)} L(π^*(s),π_θ(s))</script><p>The key difference here is the distribution of states visited by expert depends on our policy or rollout instead of directly sample data exogenously, which means in behavioral cloning presume all states was sampled, but general imitation learning is not. This assumption make behavioral cloning hard to handle unseen state.</p><p>P(s|θ) = state distribution of π<sub>θ</sub>  </p><h5 id="When-to-use-behavioral-cloning"><a href="#When-to-use-behavioral-cloning" class="headerlink" title="When to use behavioral cloning"></a>When to use behavioral cloning</h5><p>Advantages: Simple and efficient</p><p>Use When: </p><p>​        1-step deviation not too bad</p><p>​        Learning reactive behaviors</p><p>​        Expert trajectories “cover” state space</p><p>Disadvantages </p><p>​        Distribution mismatch between training and testing </p><p>​        No long term planning</p><p>Don’t Use When </p><p>​        1-step deviations can lead to catastrophic error</p><p>​        Optimizing long-term objective </p><h5 id="Direct-policy-learning"><a href="#Direct-policy-learning" class="headerlink" title="Direct policy learning"></a>Direct policy learning</h5><p>Basic idea: construct a sequence of supervise learning problems that ideally converge the best policy  for the general imitation problem, which typically start from imitation learning.</p><p><img src="/2019/08/12/imitation-learning-md/dpL.png" alt="dpL"></p><p>If apply train a policy from the first stage and then move on the next stage and so on, the policy will probably forget what has learned before. To solve this problem, data aggregation and policy aggregation was applied, which will be discussed in sequential learning reductions. </p><h5 id="Interactive-Expert"><a href="#Interactive-Expert" class="headerlink" title="Interactive Expert"></a>Interactive Expert</h5><p>The human or computation, etc that is available at training time to provide feedback on state distribution induced by the policy that we are training. The expert that we can query an at state, which can be use to construct loss function to get the mismatch between our policy and expert for any state (give feedback). </p><h5 id="Sequential-Learning-Reductions"><a href="#Sequential-Learning-Reductions" class="headerlink" title="Sequential Learning Reductions"></a>Sequential Learning Reductions</h5><p>Start from a initial predictor: π<sub>0</sub>, which is initial expert demonstrations.</p><p>For m=1 (m is an index)</p><p>​        Collect trajectories τ via previous policy π<sub>m-1</sub> and rolling out mutiple times</p><p>​        Estimate state distribution P<sub>m</sub> using these trajectories (s∈τ)</p><p>​        For each state in the rollout, we collection interactive feedback from experts {π*(s)|s∈τ}</p><p>​        Do data aggregation, which means train the policy on all data or distributions that we have </p><p>​                        e.g. do online learning on our data to train the policy</p><p>​        Or do policy aggregation, train policy on current data or distribution and then blend them.</p><p>​                        e.g. train π’<sub>m</sub> on the data or distribution at current stage P<sub>m</sub> and blend them with                         previous trained policy, which can be expressed as below.</p><script type="math/tex; mode=display">π_m=β π_m'+(1-β)π_{m-1}</script><p>​                                        and finally (M mean the final index,π<sub>0</sub> is the expert)</p><script type="math/tex; mode=display">π_m=(1-β)^M π_0+β∑_{m'} (1-β)^{(M-m')}π_{m'}</script><p>​                            The ways to analyse policy aggregation :</p><p>​                                                    π<sub>m</sub> not much worse than π<sub>m-1</sub> </p><p>​                                                    The final policy π<sub>M</sub> not much worse than expert π<sub>0</sub> </p><h4 id="Inverse-reinforcement-learning-IRL-—-do-training-to-learn-a-good-reward-function"><a href="#Inverse-reinforcement-learning-IRL-—-do-training-to-learn-a-good-reward-function" class="headerlink" title="Inverse reinforcement learning (IRL) —- do training to learn a good reward function"></a>Inverse reinforcement learning (IRL) —- do training to learn a good reward function</h4><p>For given trajectories (or data) from expert π*</p><script type="math/tex; mode=display">D=\left\{τ_1,τ_2,...\right\}=\left\{ (s_0^i,s_0^i,s_1^i,a_1^i,......)\right\} \sim π^*</script><p>The goal is to learn a reward function r* so as to maximize the reward of expert policy</p><script type="math/tex; mode=display">π^*=argmax( E_π[r^*(s,a)])</script><h5 id="General-recipe-for-IRL"><a href="#General-recipe-for-IRL" class="headerlink" title="General recipe for IRL"></a>General recipe for IRL</h5><ol><li>Get a set of expert demonstration </li></ol><script type="math/tex; mode=display">D=\left\{τ_1,...,τ_m\right\}</script><ol><li><p>Learn reward function: r<sub>θ</sub>(s<sub>t</sub>,a<sub>t</sub>)</p></li><li><p>Learn policy for given reward function with reinforcement learning</p></li><li><p>Compare the learned policy with expert to see whether the policy is good enough. If not, then repeat the same process from the second step. </p><p><img src="/2019/08/12/imitation-learning-md/IRL.png" alt="IRL"></p></li></ol><p>Relaxation of IRL</p><p>The first method: Instead of trying to find the optimal reward function r<em> and policy P</em>, as long as a reward function was found so that the performance of the optimal function policy with respect to that reward is not much worse than the performance of the true expert performance then “I’m happy”. </p><script type="math/tex; mode=display">\operatorname*{max}\limits_{π∈Π}(E_π[r(s,a)])≥E_{π^*}[r*(s,a)]-ε</script><p>Then second method: The goal is to find policy π performing better than expert π* over a restricted class of rewards function. In this case, IRL is formulated as two player game: one player tries to find the best reward function, another player tries to maximize the difference between the trained policy and the experts.</p><script type="math/tex; mode=display">\operatorname*{max}\limits_{π∈Π}(\operatorname*{min}\limits_{r∈R}(E_π[r(s,a)])-E_{π*}[r(s,a)])</script><p>Assumptions behand these two method:</p><ol><li>the model of the environment is given</li><li>the oracle of the reinforcement learning is given</li><li>The reward is linear is known features of state in actions (r(s|θ)=θ<sup>T</sup> φ(s), where ‖θ‖<sub>1</sub>=1, ‖θ‖<sub>2</sub>≤1, φ(s) is features of state in actions)</li></ol><p>How the third assumption linear reward function can help us:</p><p>Rewrite the reward function </p><script type="math/tex; mode=display">r(s)=θ·φ(s)</script><p>The value function of a policy would be </p><script type="math/tex; mode=display">V(π|s_0)=E[\sum_{π∈Π}^∞ γ^tθ·φ(s_t)|π]=θ·E[\sum_{π∈Π}^∞ γ^tφ(s_t)|π]</script><p>From the equation above, we can see that the parameter θ can be pulled out and leave a term that only depend on feature φ(s). The remain term  is also called feature expectations pf the policy π</p><script type="math/tex; mode=display">μ(π)=E[\sum_{π∈Π}^∞ γ^tφ(s_t)|π]</script><p>Another way of interpreting feature expectation μ(π) is that is is the average of visited state features given a policy and if the reward is linear then finding a policy that matches the expert features imply that the policy is optimal </p><script type="math/tex; mode=display">μ(π)=E[visited\, state\, feature|π]</script><script type="math/tex; mode=display">μ(π)=μ(π^*)→V(π)=V(π^*)</script><p>so, ideally,  the reinforcement learning problem can be reduced to a feature matching problem.</p><p>However, since we only have limited demonstration, we cannot estimate μ(π*) exactly. To solve this problem, some solution was proposed. </p><h6 id="Solution-1-Find-a-policy-that-have-feature-expectations-difference-between-expert-policy-by-no-more-than-some-ε"><a href="#Solution-1-Find-a-policy-that-have-feature-expectations-difference-between-expert-policy-by-no-more-than-some-ε" class="headerlink" title="Solution 1: Find a policy that have feature expectations difference between expert policy by no more than some ε"></a>Solution 1: Find a policy that have feature expectations difference between expert policy by no more than some ε</h6><script type="math/tex; mode=display">‖μ(π)-μ(π^*)‖_2≤ε</script><p>​    Algorithm for update reward for this ( learn reward function step in ‘General recipe for IRL section’): solve max-margin problem in each loop which maximize the margin of the expert feature from all the features that the policy we have found so far. At every time during this optimization, we can get a new reward estimated. ——I think this can view as to train a reward function that can distinguish the performance different between expert policy and our policy.</p><p>After some iteration with this reward-updating method, the train will stop until the error ε is less than certain value.</p><p>Theory has proved that it requires at most </p><script type="math/tex; mode=display">O(\frac{k}{(1-γ)^2ε^2}log\frac{k}{(1-γ)ε})</script><p>iteration.</p><h6 id="Maximum-entropy"><a href="#Maximum-entropy" class="headerlink" title="Maximum entropy"></a>Maximum entropy</h6><p> If we want to find a policy that can explain the policy behaviour of expert, then we create any policy that match the expert’s behavior so that we can create any stochastic combination of them and the resulting policy will also satisfy the feature matching requirement. Thus, in this case we still don’t know which one is the right policy, which can be resolved by maximum entropy.</p><p>Suppose policy π induces distribution over trajectories P(τ), which satisfy </p><script type="math/tex; mode=display">\sum_{τ}P(τ)μ(τ)=μ(π^*)</script><script type="math/tex; mode=display">\sum_{τ}P(τ)=1</script><p>Then we should choose the one with the largest entropy (pick one the explained the data).</p><p><em>Maximum entropy principle: The probability distribution which best represents the current state of knowledge is the one with the largest entropy</em></p><p>After some mathematical derivation if the reward is linear the max entropy distribution trajectories given the reward parameters θ will depend exponentially on the inner product of  θ in the feature of trajectory τ</p><script type="math/tex; mode=display">r_θ(s)=θ^\topφ(s)→P(τ|θ)∝e^{θ^\top φ(s)}</script><p>For IRL, it means the probability of a trajectories with high reward is exponentially more likely to be sampled from an expert than trajectory with low reward. </p><p>Then we express maximum entropy formulation as </p><script type="math/tex; mode=display">P(τ|θ)=\frac{1}{Z(θ)}e^{\sum_{s_t∈τ}θ^\top φ(s)}</script><script type="math/tex; mode=display">Z(θ)=\int e^{r(τ|θ)}dτ</script><p>We will learning the reward function for given θ by maximizing log likelihood of the observed data under the distributions </p><script type="math/tex; mode=display">θ^*=\operatorname*{argmax}\limits_{θ}L(θ)=\operatorname*{argmax}\limits_{θ}\sum_{τ_i∈D}log P(τ_i|θ)</script><p>If we use gradient descent over log-likelihood, then the gradient can be expressed as </p><script type="math/tex; mode=display">\nabla_θ L(θ)=\frac{1}{m}\sum_{τ_i∈D}μ(τ_i)-\sum_{s}d^θ_s φ(s)</script><p>where </p><script type="math/tex; mode=display">μ(τ_i)=\frac{1}{m}\sum_{s'∈τ_i}φ(s')</script><p>is the expert state feature </p><p>d<sub>s</sub> is the state occupancy measure can be calculated as</p><script type="math/tex; mode=display">d_{t+1,s'}=\sum_{a}\sum_{s}d_{d,s}π_θ(a|s)P(s'|s,a)</script><h4 id="Model-free-IRL"><a href="#Model-free-IRL" class="headerlink" title="Model-free IRL"></a>Model-free IRL</h4><p>The IRL discussed above is just to find the reward for given state dynamic. In this section, we will talk about the cases when both reward and state dynamic are not given.</p><p>One of the setting for that is to set the reward function r<sub>θ</sub>(s<sub>t</sub>, a<sub>t</sub>) as neural net.</p><p>The max entropy distribution trajectories formulation is </p><script type="math/tex; mode=display">P(τ|θ)=\frac{1}{Z(θ)}e^{r_θ(r)}</script><p>Learn θ with max likelihood. </p><script type="math/tex; mode=display">θ^*=\operatorname*{argmax}\limits_{θ}L(θ)=\operatorname*{argmax}\limits_{θ}\sum_{τ_i∈D}log P(τ_i|θ)=\operatorname*{argmax}\limits_{θ}\frac{1}{|D|}\sum_{τ_i∈D}r_θ(τ_i)-logZ(θ)</script><p>Z(θ), in this case, can be estimated by using proposal distribution q(τ) to sample trajectories D<sub>samp</sub>， which can be used for reward optimization</p><script type="math/tex; mode=display">Z(θ)≈\operatorname*{arerage}\limits_{τ∈D_{samp}}(\frac{e^{r_θ(τ)}}{q(τ)})</script><p>Except reward function optimization, policy optimization is still needed to be considered. </p><p>In the model-free optimization process, the high level overview consists of two steps for each loop:</p><ol><li>Generate sampling distributions that are a set of trajectories not far away from the ones induced by the current policy and then combine these sampled trajectories with existing export demonstration to estimate and update reward parameters</li><li>update the policy with the estimated reward function</li></ol><p><img src="/2019/08/12/imitation-learning-md/IRL2.png" alt="IRL2"></p><h5 id="Generative-adversarial-imitation-learning-GAIL"><a href="#Generative-adversarial-imitation-learning-GAIL" class="headerlink" title="Generative adversarial imitation learning (GAIL)"></a>Generative adversarial imitation learning (GAIL)</h5><p><em>Occupancy measure</em></p><p>The occupancy measure discussed before is visitation frequency of state action pair</p><script type="math/tex; mode=display">d^π_{sa}=visitation\;frequency \;of \;(s,a)|following\; π</script><script type="math/tex; mode=display">d^π_{sa}=E[\sum_{t=0}^∞ γ^t(s_t=s,a_t=a)|π]</script><p>A fact about occupancy measure is we can represent value function with the reward function and occupancy measure.</p><script type="math/tex; mode=display">V(π)=E_π[r(s,a)]=\sum_{s,a}r(s,a)d^π_{sa}</script><p>So apart from reduce imitation learning to feature matching, we can also reduce is to occupancy matching, which means if we can find the occupancy measure that match optimal occupancy matching, then for any reward function, the value will match the optimal one </p><script type="math/tex; mode=display">d^π_{sa}=d^*_{sa}=E_π[r(s,a)]=E_{π^*}[r(s,a)]</script><p>Still same as the policy chosen issue, in this case, we still need to select the one with the maximum entropy of policy, so the problems here will become minimize the occupancy measure between expert and our policy and maximize the entropy of our policy (H(π) is the casual entropy )</p><script type="math/tex; mode=display">max_π \,H(π)</script><script type="math/tex; mode=display">distance(d^π_{sa},d^*_{sa})<ε</script><p>The final formulations for it is </p><script type="math/tex; mode=display">min_π \, distance(d^π_{sa},d^*_{sa})-λH(π)</script><p>The distance here is Jensen–Shannon divergency </p><script type="math/tex; mode=display">\begin{split}distance(d^π_{sa},d^*_{sa})&=max_{D∈(0,1)^{S×A}\, }E_π[log(D(s,a))]+E_{π^*}[log(1-D(s,a))]\\&≈D_{KL}(d^π||(d^π+d^*)/2)+D_{KL}(d^*||(d^π+d^*)/2)\end{split}\tag{1.3}</script><p>Thus, the final equation of GAIL is</p><script type="math/tex; mode=display">\operatorname*{min}\limits_{π}\,\operatorname*{max}\limits_{D∈(0,1)^{S×A}}E_π[log(D(s,a))]+E_{π^*}[log(1-D(s,a))]-λH(π)</script><p>In this case, D is the discriminator, which is </p><script type="math/tex; mode=display">\operatorname*{max}\limits_{D∈(0,1)^{S×A}}E_π[log(D(s,a))]+E_{π^*}[log(1-D(s,a))]</script><p> which can be updated by the gradient of (w is the parameters of discriminator, i represent the steps )</p><script type="math/tex; mode=display">\widehat{E}_{τ_i}[\nabla_w log(D(s,a))]+\widehat{E}_{τ_E}[\nabla_w log(1-D(s,a))]</script><p>and our policy π is the generator, which has the gradient of  (θ is the parameters of our policy,, i represent the steps )</p><script type="math/tex; mode=display">\widehat{E}_{τ_i}[\nabla_θ log\,π_θ( a|s)Q(s,a)]-λ\nabla_θH(π_θ)</script><script type="math/tex; mode=display">Q(\overline{s},\overline{a})=\widehat{E}_{τ_i}[log(D_{w_{w+1}}(s,a))|s_0=\overline{s}, a_0=\overline{a}]</script><p>The discriminator aim to distinguish the action from our policy and expert policy and generator aim to generate the action that is similar to the action from expert to fool the discriminator. </p><h4 id="comparation"><a href="#comparation" class="headerlink" title="comparation"></a>comparation</h4><p>The following is the comparation between the three main imitation learning.</p><p><img src="/2019/08/12/imitation-learning-md/table.png" alt="table"></p><h3 id="Appendix"><a href="#Appendix" class="headerlink" title="Appendix"></a>Appendix</h3><p>Kullback-Leibler divergence (KL divergence) : this can measure the difference between two distribution, which is non-symmetrical, always greater than 0, and the KL divergence of the distribution it self is. </p><script type="math/tex; mode=display">KL(q||p)=\int q(x)log\frac{q(x)}{p(x)}dx</script><p>Jensen–Shannon divergence (JS divergence ) is a symmetrized and smoothed version of KL divergence. </p><script type="math/tex; mode=display">JS(q||p)=\frac{1}{2}KL(q||p)+\frac{1}{2}KL(p||q)</script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;em&gt;This note is mainly based on &lt;a href=&quot;https://www.youtube.com/watch?v=6rZTaboSY4k&amp;amp;feature=youtu.be&quot; target=&quot;_blank&quot; rel=&quot;noopener
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>gazebo and unity comparation</title>
    <link href="https://num-github.github.io/2019/07/31/gazebo-and-unity-comparation/"/>
    <id>https://num-github.github.io/2019/07/31/gazebo-and-unity-comparation/</id>
    <published>2019-07-31T15:36:28.000Z</published>
    <updated>2019-07-31T15:37:56.009Z</updated>
    
    <content type="html"><![CDATA[<h3 id="gazebo-and-unity-comparation"><a href="#gazebo-and-unity-comparation" class="headerlink" title="gazebo and unity comparation"></a>gazebo and unity comparation</h3><p>Unity have better functionality. It is very powerful, fast and easy to use compared to gazebo. You don’t need to modify some sdf or other files directly to change the hierarchy and inspector directly in order to change the parameters of an object and it support many types of files. The animation and world built by unity is more beautiful than gazebo, as well. Actually, unity is able to generate and edit some necessary file by using ROS# that is necessary for ROS and even do some simulation (even though you need to using TCP protocol to connect to ROS). Therefore, from the respect of editing sdf, URDF or do some animation, unity is more superior. Furthermore, unity can, according to ROS# Wiki, do both simulation and visualization together instead of separate tools (rviz for visualization, gazebo for simulation). Moreover, since this lab is mixed reality lab, you might have some persons that are expert in unity or you add some VR or AR stuffs to the robot, so in these applications, unity is always better than gazebo in develop these stuffs, especially multiclient support is necessary. </p><p>For gazebo, however, the main advantages are more focus on ROS itself, it has a straightforward connection, and I think it is good for beginners who just want to do some work on robot. The learning materials or tutorials that relate to ROS are far more than unity. In fact, if you have no knowledge about C# or don’t want to write some raw code to communication with ROS, the best thing that I found is ROS# for unity, which only have its wiki and few videos. Thus, for a beginner, especially the person who have no knowledge on both software, gazebo could be better. The next thing about that is plugin and support, Gazebo have many plugins for robot. According to the official tutorial, there are already some plugins like cameras, IMUs, and lasers that is written by some company or people (most of them written in C\C++). They can be easily called and used directly by following the step of tutorial. In addition, some company that produce robot (e.g. swayer robot) provide relevant simulation code for gazebo, you can just run and try it directly. In contrast, for unity, if you want to use such sensors, it would be harder to find the code to simulate it in unity and even if you are able to write a plugin or find something very similar, it might necessary to learn C# (I’m not sure other students, but for electrical and electronic engineering, only C\C++ is compulsory). </p><p>In fact, according to ROS#, unity and gazebo can be used together. There is a demo that set up a simulation world on gazebo and do visualization on unity, so I think depending what we will do, we can try both methods to achieve our goals. </p><p>In summary, unity have better and useful functionality, but gazebo is good at dealing with ROS, so in order to determine which method to choose, we would better determine what is available and what’s the whole project looks like. In next section, based on the analysis above, I will list some conditions that unity or gazebo would be certainly better. </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;gazebo-and-unity-comparation&quot;&gt;&lt;a href=&quot;#gazebo-and-unity-comparation&quot; class=&quot;headerlink&quot; title=&quot;gazebo and unity comparation&quot;&gt;&lt;/a&gt;ga
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Note for RPN</title>
    <link href="https://num-github.github.io/2019/07/25/Note%20for%20RPN/"/>
    <id>https://num-github.github.io/2019/07/25/Note for RPN/</id>
    <published>2019-07-25T15:38:15.002Z</published>
    <updated>2019-08-30T23:03:44.956Z</updated>
    
    <content type="html"><![CDATA[<h4 id="Note-for-RPN"><a href="#Note-for-RPN" class="headerlink" title="Note for RPN"></a>Note for RPN</h4><p>For bounding boxes with a scale and ratios </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">scale = <span class="number">4</span></span><br><span class="line">ratio = <span class="number">2</span></span><br><span class="line">x = scale * np.sqrt(ratio)</span><br><span class="line">y = scale / np.sqrt(ratio)</span><br></pre></td></tr></table></figure><p>Anchor generation in SiamMask:</p><p>In SiamMask, the anchors was generated in anchors.py, before we goes into the real anchor generation code, let’s firstly have a look about the parameters defined in <strong>init</strong> function</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Anchors</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.stride = <span class="number">8</span></span><br><span class="line">        self.ratios = [<span class="number">0.33</span>, <span class="number">0.5</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">        self.scales = [<span class="number">8</span>]</span><br><span class="line">        self.round_dight = <span class="number">0</span></span><br><span class="line">        self.image_center = <span class="number">0</span></span><br><span class="line">        self.size = <span class="number">0</span></span><br><span class="line">        self.anchor_density = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        self.anchor_num = len(self.scales) * len(self.ratios) * (self.anchor_density**<span class="number">2</span>)</span><br><span class="line">        self.anchors = <span class="literal">None</span>  <span class="comment"># in single position (anchor_num*4)</span></span><br><span class="line">        self.all_anchors = <span class="literal">None</span>  <span class="comment"># in all position 2*(4*anchor_num*h*w)</span></span><br><span class="line">        self.generate_anchors()</span><br></pre></td></tr></table></figure><p>The true cod from anchor generation is like below. If we put the value above into the code, we can clearly see the process of it which generate anchor with different ratio</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_anchors</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.anchors = np.zeros((self.anchor_num, <span class="number">4</span>), dtype=np.float32)</span><br><span class="line">        size = self.stride * self.stride</span><br><span class="line">        count = <span class="number">0</span></span><br><span class="line">        anchors_offset = self.stride / self.anchor_density <span class="comment">#=8</span></span><br><span class="line">        anchors_offset = np.arange(self.anchor_density)*anchors_offset</span><br><span class="line">        anchors_offset = anchors_offset - np.mean(anchors_offset)</span><br><span class="line"></span><br><span class="line">        x_offsets, y_offsets = np.meshgrid(anchors_offset, anchors_offset)</span><br><span class="line"><span class="comment">#Thus, only run outer for once </span></span><br><span class="line">        <span class="keyword">for</span> x_offset, y_offset <span class="keyword">in</span> zip(x_offsets.flatten(), y_offsets.flatten()):</span><br><span class="line">            <span class="keyword">for</span> r <span class="keyword">in</span> self.ratios:</span><br><span class="line">                <span class="keyword">if</span> self.round_dight &gt; <span class="number">0</span>:<span class="comment"># not run this if</span></span><br><span class="line">                    ws = round(math.sqrt(size*<span class="number">1.</span> / r), self.round_dight)</span><br><span class="line">                    hs = round(ws * r, self.round_dight)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="comment">#size = self.stride * self.stride</span></span><br><span class="line">                    ws = int(math.sqrt(size*<span class="number">1.</span> / r))</span><br><span class="line">                    hs = int(ws * r)</span><br><span class="line"></span><br><span class="line">                <span class="keyword">for</span> s <span class="keyword">in</span> self.scales:</span><br><span class="line">                    w = ws * s  <span class="comment">#=sqrt(stride * stride* / ratios)*scales</span></span><br><span class="line">                    <span class="comment">#=stride*scales/sqrt(ratios)</span></span><br><span class="line"></span><br><span class="line">                    h = hs * s<span class="comment">#=sqrt(stride * stride* ratios)*scales</span></span><br><span class="line">                    <span class="comment">#=stride*scales*sqrt(ratios)</span></span><br><span class="line">                    self.anchors[count][:] = [-w*<span class="number">0.5</span>+x_offset, -h*<span class="number">0.5</span>+y_offset, w*<span class="number">0.5</span>+x_offset, h*<span class="number">0.5</span>+y_offset][:]</span><br><span class="line">             <span class="comment">#array([[-52., -16.,  52.,  16.],</span></span><br><span class="line">       <span class="comment">#[-44., -20.,  44.,  20.],</span></span><br><span class="line">       <span class="comment">#[-32., -32.,  32.,  32.],</span></span><br><span class="line">       <span class="comment">#[-20., -40.,  20.,  40.],</span></span><br><span class="line">       <span class="comment">#[-16., -48.,  16.,  48.]], dtype=float32) </span></span><br><span class="line">             <span class="comment"># This is the offset for the four points of an anchor related to center points.</span></span><br><span class="line">                    count += <span class="number">1</span></span><br></pre></td></tr></table></figure><p>In data preparation stage, function generate_all_anchors has also be called</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">self.anchors.generate_all_anchors(im_c=self.search_size//<span class="number">2</span>, size=self.size)</span><br><span class="line"><span class="comment">#search_size is the length and height of search image</span></span><br><span class="line">    <span class="comment">#search_size//2 is the center of the image</span></span><br><span class="line">    <span class="comment">#size is the length and height of a feature map</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_all_anchors</span><span class="params">(self, im_c, size)</span>:</span></span><br><span class="line">    <span class="comment">#suppose search_size = 255 im_c=255//2</span></span><br><span class="line">    <span class="comment">#suppose size = 17, the parameter comes from your config.json file which represent the size of feature map  </span></span><br><span class="line">        <span class="keyword">if</span> self.image_center == im_c <span class="keyword">and</span> self.size == size:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        self.image_center = im_c</span><br><span class="line">        self.size = size</span><br><span class="line">        <span class="comment">#a0x = 63=255//2-17//2*8</span></span><br><span class="line">        a0x = im_c - size // <span class="number">2</span> * self.stride<span class="comment">#The center point of the first anchor</span></span><br><span class="line">        ori = np.array([a0x] * <span class="number">4</span>, dtype=np.float32)</span><br><span class="line">        <span class="comment">#ori = array([63., 63., 63., 63.], dtype=float32)</span></span><br><span class="line">        zero_anchors = self.anchors + ori<span class="comment">#The anchor points for the four point </span></span><br><span class="line"><span class="comment">#ori +self.anchors: array([[-52., -16.,  52.,  16.],</span></span><br><span class="line">                    <span class="comment">#        [-44., -20.,  44.,  20.],</span></span><br><span class="line">                    <span class="comment">#        [-32., -32.,  32.,  32.],</span></span><br><span class="line">                    <span class="comment">#        [-20., -40.,  20.,  40.],</span></span><br><span class="line">                    <span class="comment">#        [-16., -48.,  16.,  48.]], dtype=float32)</span></span><br><span class="line">        <span class="comment">#X1,y1,x2    ,y2  correspond to the value in the second dimension</span></span><br><span class="line">        <span class="comment">#          X1 ,  y1    ,x2    ,y2 </span></span><br><span class="line">        <span class="comment">#=array([[ 11.,  47., 115.,  79.],</span></span><br><span class="line">        <span class="comment">#       [ 19.,  43., 107.,  83.],</span></span><br><span class="line">        <span class="comment">#       [ 31.,  31.,  95.,  95.],</span></span><br><span class="line">        <span class="comment">#       [ 43.,  23.,  83., 103.],</span></span><br><span class="line">        <span class="comment">#       [ 47.,  15.,  79., 111.]], dtype=float32)</span></span><br><span class="line">        x1 = zero_anchors[:, <span class="number">0</span>]</span><br><span class="line">        y1 = zero_anchors[:, <span class="number">1</span>]</span><br><span class="line">        x2 = zero_anchors[:, <span class="number">2</span>]</span><br><span class="line">        y2 = zero_anchors[:, <span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment">#Add two more dimension to x1, y1, x2, y2 </span></span><br><span class="line">        <span class="comment">#x1=array([[[11.]],</span></span><br><span class="line">        <span class="comment">#[[19.]],</span></span><br><span class="line">        <span class="comment">#[[31.]],</span></span><br><span class="line">        <span class="comment">#[[43.]],</span></span><br><span class="line">        <span class="comment">#[[47.]]], dtype=float32)   .....</span></span><br><span class="line">        x1, y1, x2, y2 = map(<span class="keyword">lambda</span> x: x.reshape(self.anchor_num, <span class="number">1</span>, <span class="number">1</span>), [x1, y1, x2, y2])</span><br><span class="line">        cx, cy, w, h = corner2center([x1, y1, x2, y2])</span><br><span class="line">        <span class="comment">#e.g.</span></span><br><span class="line">        <span class="comment">#cx=array([[[63.]],                h=array([[[32.]],</span></span><br><span class="line">        <span class="comment">#[[63.]],    [[40.]],</span></span><br><span class="line">        <span class="comment">#[[63.]],[[64.]],</span></span><br><span class="line">        <span class="comment">#[[63.]], [[80.]],</span></span><br><span class="line">        <span class="comment">#[[63.]]], dtype=float32)[[96.]]], dtype=float32))</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#size = 17</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">#The ofset of each bounding boxes with respect to each </span></span><br><span class="line">        disp_x = np.arange(<span class="number">0</span>, size).reshape(<span class="number">1</span>, <span class="number">1</span>, <span class="number">-1</span>) * self.stride</span><br><span class="line">        disp_y = np.arange(<span class="number">0</span>, size).reshape(<span class="number">1</span>, <span class="number">-1</span>, <span class="number">1</span>) * self.stride</span><br><span class="line"><span class="comment">#disp_x=array([[[  0,   8,  16,  24,  32,  40,  48,  56,  64,  72,  80,  88,96, 104, 112, 120, 128]]])</span></span><br><span class="line"><span class="comment">#disp_y=array([[[  0], [  8], [ 16], [ 24], [ 32],  [ 40],[ 48], [ 56], [ 64],[ 72], [ 80],[ 88],[ 96],[104],[112], [120],[128]]])</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#cx.shape=(5, 1, 1)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Calculate the final location of each center points cx: x axis value</span></span><br><span class="line"><span class="comment">#cy: y axis value</span></span><br><span class="line">        cx = cx + disp_x</span><br><span class="line"> </span><br><span class="line"><span class="comment">#cx=array([[[ 63.,  71.,  79.,  87.,  95., 103., 111., 119., 127., 135.,143., 151., 159., 167., 175., 183., 191.]],</span></span><br><span class="line"><span class="comment"># [[ 63.,  71.,  79.,  87.,  95., 103., 111., 119., 127., 135., 143., 151., 159., 167., 175., 183., 191.]],</span></span><br><span class="line"><span class="comment">#[[ 63.,  71.,  79.,  87.,  95., 103., 111., 119., 127., 135.,143., 151., 159., 167., 175., 183., 191.]],</span></span><br><span class="line"><span class="comment"># [[ 63.,  71.,  79.,  87.,  95., 103., 111., 119., 127., 135.,  143., 151., 159., 167., 175., 183., 191.]],</span></span><br><span class="line"><span class="comment"># [[ 63.,  71.,  79.,  87.,  95., 103., 111., 119., 127., 135.,  143., 151., 159., 167., 175., 183., 191.]]])</span></span><br><span class="line"> </span><br><span class="line">        cy = cy + disp_y</span><br><span class="line"></span><br><span class="line">        <span class="comment"># broadcast</span></span><br><span class="line">        <span class="comment">#(5, 17, 17)</span></span><br><span class="line">        zero = np.zeros((self.anchor_num, size, size), dtype=np.float32)</span><br><span class="line">        <span class="comment">#convert cx from (17,17) to (5, 17, 17)</span></span><br><span class="line">        cx, cy, w, h = map(<span class="keyword">lambda</span> x: x + zero, [cx, cy, w, h])</span><br><span class="line">        x1, y1, x2, y2 = center2corner([cx, cy, w, h])</span><br><span class="line"><span class="comment">#(4, 5, 17, 17)</span></span><br><span class="line">        self.all_anchors = np.stack([x1, y1, x2, y2]), np.stack([cx, cy, w, h])</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;Note-for-RPN&quot;&gt;&lt;a href=&quot;#Note-for-RPN&quot; class=&quot;headerlink&quot; title=&quot;Note for RPN&quot;&gt;&lt;/a&gt;Note for RPN&lt;/h4&gt;&lt;p&gt;For bounding boxes with a scal
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Investigation for Reinforcement Learning (RL) in robotic</title>
    <link href="https://num-github.github.io/2019/07/13/Investigation-for-Reinforcement-Learning-in-robotic/"/>
    <id>https://num-github.github.io/2019/07/13/Investigation-for-Reinforcement-Learning-in-robotic/</id>
    <published>2019-07-13T12:36:39.756Z</published>
    <updated>2019-08-30T23:03:28.759Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Investigation-for-Reinforcement-Learning-RL-in-robotic"><a href="#Investigation-for-Reinforcement-Learning-RL-in-robotic" class="headerlink" title="Investigation for Reinforcement Learning (RL) in robotic"></a>Investigation for Reinforcement Learning (RL) in robotic</h1><p><em>Recently, I was asked to do some research on robot. The main task for that is to tidy up clothes. My first impression  on that task is this is very similar to pick and place tasks in has been talked about in many domes that I had watched before, so I think that might be a good point to start and investigate it from what I had know.</em></p><p>I think, this article probably useful for beginners who wants to apply reinforcement learning into their project but have a little (like me) or no knowledge about reinforcement learning. </p><p>This article also include many links that require you to set up some environment. I know that this is a very tedious and annoying work, but it would be better to do that if you hope to implement RL to your project, because I think take some practice is the best way to learn. </p><h3 id="Useful-links-for-beginners"><a href="#Useful-links-for-beginners" class="headerlink" title="Useful links for beginners:"></a>Useful links for beginners:</h3><p>The following links are some useful resources that is very useful and interesting and what I discussed here is also comes from the following links: </p><p> Tools:</p><p>​             <a href="https://stable-baselines.readthedocs.io/en/master/modules/ddpg.html" target="_blank" rel="noopener">OpenAI stable baselines</a> </p><p>​            <a href="https://github.com/openai/baselines" target="_blank" rel="noopener">OpenAI baseline</a></p><p>​            <a href="http://gym.openai.com/envs/#robotics" target="_blank" rel="noopener">OpenAI gym (robot) </a></p><p>Learning: </p><p>​            <a href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html" target="_blank" rel="noopener">Spinning up</a></p><p>​            <a href="https://www.youtube.com/watch?v=Wypc1a-1ZYA" target="_blank" rel="noopener">Matlab Walking Robot Problem</a> </p><p>​            <a href="https://github.com/sarcturus00/Tidy-Reinforcement-learning" target="_blank" rel="noopener">Tidy-reinforcement learning</a></p><p>​            <a href="https://www.bilibili.com/video/av24724071?from=search&amp;seid=14445377713604973899" target="_blank" rel="noopener">“Deep Reinforcement Learning, 2018” by Hongyi Li (Chinese version)</a></p><h5 id="Before-you-read-ensure-that-you-have-some-basic-knowledge-of-reinforcement-learning-You-can-read-though-the-tutorial-in-spinning-up-of-Key-Concepts-in-RL"><a href="#Before-you-read-ensure-that-you-have-some-basic-knowledge-of-reinforcement-learning-You-can-read-though-the-tutorial-in-spinning-up-of-Key-Concepts-in-RL" class="headerlink" title="Before you read, ensure that you have some basic knowledge of reinforcement learning. You can read though the tutorial in spinning up of Key Concepts in RL."></a>Before you read, ensure that you have some basic knowledge of reinforcement learning. You can read though the tutorial in <a href="https://spinningup.openai.com/en/latest/spinningup/rl_intro.html" target="_blank" rel="noopener">spinning up</a> of Key Concepts in RL.</h5><h3 id="Play-with-environment"><a href="#Play-with-environment" class="headerlink" title="Play with environment"></a>Play with environment</h3><p>The first algorithm that found useful is <a href="https://openai.com/blog/ingredients-for-robotics-research/" target="_blank" rel="noopener">Hindsight Experience Replay (HER) + Deep Deterministic Policy Gradient (DDPG)</a>. The ‘+’ here means it is the algorithm that apply HER to improve the performance of DDPG on this task. </p><p>Before we discussing the algorithm in detail, let’s have a look what’s that tasks that this algorithm has been solved (The following are just part of the summary of the tasks, you can find more detail on <a href="https://arxiv.org/pdf/1802.09464.pdf" target="_blank" rel="noopener">this</a>  or watch the <a href="https://sites.google.com/site/hindsightexperiencereplay/" target="_blank" rel="noopener">demo</a> that may give you some intuition on that). </p><p>Basically, the aim of the simulation is to move the gripper of a robot to achieve the certain tasks: FetchReach, FetchPush, FetchSlide  and FetchPickAndPlace. </p><p>There are 4 parameters for actions for gripper: 3 to specify how the gripper will move in Cartesian coordinates  and one to specify the  open and close of the gripper. </p><p>The reward for that is 0 if the goal is achieved (e.g. in pick and place tasks,  once the robot move the cube to desired location, the reward would be 0 or otherwise would be 1).  </p><p>The observations include the Cartesian position of the gripper, its linear velocity as well as the position and linear velocity of the robot’s gripper. If an object is present, we also include the object’s Cartesian position and rotation using Euler angles, its linear and angular velocities, as well as its position and linear velocities relative to gripper. </p><p><img src="/2019/07/13/Investigation-for-Reinforcement-Learning-in-robotic/robot.png" alt="robot"></p><p>You can run the following code and modify the array of actions to see if there are changes occur, once your had set up the environment for OpenAI gym-robot (You need to install MuJoCo).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gym</span><br><span class="line">env = gym.make(<span class="string">"FetchPickAndPlace-v1"</span>)</span><br><span class="line">observation = env.reset()</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">  env.render()</span><br><span class="line">  action = env.action_space.sample()</span><br><span class="line">  print( action )</span><br><span class="line">  observation, reward, done, info = env.step(action)</span><br><span class="line">  print(  observation, reward, done, info )</span><br><span class="line">  <span class="keyword">if</span> done:</span><br><span class="line">    observation = env.reset()</span><br><span class="line">env.close()</span><br></pre></td></tr></table></figure><p>After that you can following the guide of <a href="https://github.com/openai/baselines/tree/master/baselines/her" target="_blank" rel="noopener">baseline-HER</a> to train HER model in this environment so as to get an intuition of this algorithm or modify the code a little bit in baseline demo to apply it to your project (change from OpenAI-gym environment to your own environment (e.g. ROS) ). </p><h3 id="Link-with-openAI-with-ROS-If-you-don’t-use-ROS-in-your-project-you-can-skip-this-part"><a href="#Link-with-openAI-with-ROS-If-you-don’t-use-ROS-in-your-project-you-can-skip-this-part" class="headerlink" title="Link with openAI with ROS (If you don’t use ROS in your project, you can skip this part)"></a>Link with openAI with ROS (If you don’t use ROS in your project, you can skip this part)</h3><p>In order to do that your can following the guide of <a href="http://wiki.ros.org/openai_ros" target="_blank" rel="noopener">OpenAI_ROS</a>. In summary, what the OpenAI_ROS want you to do is to specify the following three things:</p><p><strong>Task environment</strong>:  the task that the robot has to learn (<em>e.g.</em> actions, reward functions, observations).</p><p><strong>Robot environment</strong>: the robot to use on the task (<em>e.g.</em> contain all the ROS functionalities that your robot will need in order to be controlled and checks that every ROS stuff required is up and running on the robot (topics, services …).)</p><p><strong>Training script</strong>: to set up the learning algorithm that you want to use in order to make your agent learn and select the task and robot to be used.</p><h3 id="Algorithms"><a href="#Algorithms" class="headerlink" title="Algorithms"></a>Algorithms</h3><p>In a typical robot control problems for reinforcement learning, the algorithms must be able to handle the issue of continuous actions and sparse reward problems. In the algorithm of HER+DDPG, DDPG aim to provide high performance algorithms for continuous actions and HER used to solve sparse reward problems. </p><h5 id="DDPG"><a href="#DDPG" class="headerlink" title="DDPG:"></a>DDPG:</h5><p>DDPG algorithm was developed from actor critic, which also combined policy based algorithm and value based algorithm. </p><h6 id="value-based-algorithm"><a href="#value-based-algorithm" class="headerlink" title="value  based algorithm"></a>value  based algorithm</h6><p>Let’s begin with the value based algorithm, which is <strong>Q-learning</strong> in this case. </p><p>The aim of an reinforcement learning algorithm is to maximize the reward from environment. For Q-learning, this is achieved by Q-table, which that map the reward predications relate to actions and states.  For example, if you want to train a robot that can only move forward (action 1) or backward (action 2) and  3 state (s1, s2, s3), each combinations of the states or actions will have a value (Q-value or predicted reward) for making decision, this value will be continuously updated during training based on the feedback from environment for each action and state. The optimal decision making strategy or policy is usually achieved by picking up the action that have the maximum Q value.</p><p>Thus, if you want a robot to reach a goal, go state 3 as quick as possible, for example. The robot will try an action each time based on your decision-making strategy and observe the current state and reward from environment; then, based on these to update Q table.</p><p><img src="/2019/07/13/Investigation-for-Reinforcement-Learning-in-robotic/1563039827434.png" alt="1563039827434"></p><p>The image above shows the Q-learning algorithm and how to update Q table in more detail, but I won’t talk more about it, what I want to mention is that Q table is used for mapping state and actions to Q-values, and our decision policy will make decisions based on Q-values.</p><p>The improved version for Q-learning is <strong>DQN</strong>, which replace the Q-table with a neural network. Such neural are able to handle much more state than pure Q-table. Similar to the neural network for regression problems, the neural network here regress the score of reward which is </p><script type="math/tex; mode=display">Reward+\gamma max(Q(state_{Next},action)</script><p>Where γ is discount factor. If Q is accurate enough, this</p><script type="math/tex; mode=display">max(Q(state_{Next},action)</script><p>can be viewed as  the score in the future, so the γ here is used to reduce the importance of the effect of future and the value of γ means to what extent that we hope to ignore the score of future. </p><h6 id="tips-for-train-a-neural-network-in-RL"><a href="#tips-for-train-a-neural-network-in-RL" class="headerlink" title="tips for train a neural network in RL"></a>tips for train a neural network in RL</h6><p>Another two commonly used trick which can is experience replay and target network. In my view, these two technique both trying to make the neural network training more stable by make the parameters updating process more like unsupervised learning. </p><p>Experience replay is to firstly sample reward, action, state, observation, etc, and then store them in a buffer.  And then train them together. </p><p>Target network is use two network, one for parameter updating and the other for providing Q. Once the training has processed for some times, copy the parameters of updating network to the Q network. You can read more about Q-learning and DQN in <a href="https://medium.com/@jonathan_hui/rl-dqn-deep-q-network-e207751f7ae4" target="_blank" rel="noopener">here</a></p><p>Apart from that, there is also many other tricks for improving  DQN, but the effect of these tricks can be shown on these  two  images and you can find more information on that in <a href="https://arxiv.org/pdf/1710.02298.pdf" target="_blank" rel="noopener">this paper</a>.</p><p><img src="/2019/07/13/Investigation-for-Reinforcement-Learning-in-robotic/chrome_2019-07-13_21-44-40.png" alt="chrome_2019-07-13_21-44-40"></p><p><img src="/2019/07/13/Investigation-for-Reinforcement-Learning-in-robotic/chrome_2019-07-13_21-41-24.png" alt="chrome_2019-07-13_21-41-24">{:height=”50%” width=”50%”}</p><h6 id="policy-based-algorithm"><a href="#policy-based-algorithm" class="headerlink" title="policy based algorithm"></a>policy based algorithm</h6><p>The simplest algorithm for policy based algorithm is policy gradient. Intuitively, this is the algorithm that aim to maximize the probability of the taking actions that give the maximum reward, which is achieved by the method of gradient ascent. You can see more about it in <a href="https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html" target="_blank" rel="noopener">here</a></p><p>The input of it is the observation, the output is the probability of each action. The mapping between observation and actions can be achieved by a neural network. </p><p>For deterministic policy gradient, it do not output the probability of each action, but the the exact action that it will take.</p><h6 id="DDPG-1"><a href="#DDPG-1" class="headerlink" title="DDPG"></a>DDPG</h6><p>Ok, now, you should get sense about what is Q network, policy network, and some tips for training a network in RL algorithm, in this section, let’s put them all together to form DDPG.</p><p>One of the problem for value based algorithm is it cannot handle continuous action space very in a very easy way, but policy based algorithm can achieve that goal. The problem for policy network is the efficiency of its updating is slow, so introduced Q network will speed up this process.   Actually, the basic thinking for this algorithm is to combine the benefits of the two types of algorithms, that is, the Q networks try to learn the reward of actions, the policy networks try to learn how to give actions that maximize the predicted reward (Q value). </p><p><img src="/2019/07/13/Investigation-for-Reinforcement-Learning-in-robotic/POWERPNT_2019-07-14_12-55-47.png" alt="POWERPNT_2019-07-14_12-55-47"></p><p>With such thinking and some tricks mentioned before like experience replay and target network, the final version of DDPG comes up. You may find different version of DDPG pseudo code, but they all consists of the tricks and the basic thinking. For example, the following pseudo code comes from <a href="https://github.com/sarcturus00/Tidy-Reinforcement-learning" target="_blank" rel="noopener">Tidy-Reinforcement-learning</a>, you can find other RL algorithm in here, as well.</p><p><img src="/2019/07/13/Investigation-for-Reinforcement-Learning-in-robotic/POWERPNT_2019-07-14_14-05-32.png" alt="POWERPNT_2019-07-14_14-05-32"></p><p>The blue box represents the basic thinking that Q network and policy network try to minimized. Red boxes is the tricks about reward and target network, and the yellow one is anther trick that add some noise to action to increase the ability of exploration of the algorithm. You can find more detail information and another version of DDPG at <a href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html" target="_blank" rel="noopener">here</a> which also add clip to it.  </p><h5 id="HER"><a href="#HER" class="headerlink" title="HER"></a>HER</h5><p>For now, DDPG has solved the problem of learning for continuous actions space, the next step is to deal with sparse reward problem. </p><p>In general, in my view, HER is the algorithm that make a very sparse reward problem which only give reward at certain state do not extremely sparse by directly give some reward to some of other states.</p><p>Or you can understand it in a way that <a href="https://arxiv.org/pdf/1707.01495.pdf" target="_blank" rel="noopener">HER paper</a> has mentioned: “after experiencing some episode s0, s1, . . . , sT we store in the replay buffer every transition st → st+1 not only with the original goal used for this episode but also with a subset of other goals. Notice that the goal being pursued influences the agent’s actions but not the environment dynamics and therefore we can replay each trajectory with an arbitrary goal assuming that we use an off-policy RL algorithms.” </p><p>The following is the pseudo code is the complete algorithm from HER paper and blue box is the the HER part.</p><p><img src="/2019/07/13/Investigation-for-Reinforcement-Learning-in-robotic/HER.png" alt="HER"></p><p>If you feeling that this expression is still abstract. The following code from <a href="https://github.com/sarcturus00/Tidy-Reinforcement-learning" target="_blank" rel="noopener">Tidy-Reinforcement-learning</a> clearly demonstrate what has mentioned before, where is future_k is the hyperparameter that specify how many additional state will be assign a reward (in this case, reward 0 means achieve the goal, -1 means haven’t achieve the goal). </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> HER:</span><br><span class="line">    <span class="keyword">for</span> h <span class="keyword">in</span> range(future_k):</span><br><span class="line">        future = np.random.randint(t, nstep)</span><br><span class="line">        goal_ = episode_experience[future][<span class="number">3</span>]</span><br><span class="line">        new_inputs0 = np.concatenate([obs0, goal_], axis=<span class="number">-1</span>)</span><br><span class="line">        new_inputs1 = np.concatenate([obs1, goal_], axis=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (np.array(obs1) == np.array(goal_)).all():</span><br><span class="line">            r_ = <span class="number">0.0</span> <span class="comment">#add reward to additional state.</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            r_ = <span class="number">-1.0</span></span><br><span class="line">            agent.memory.store_transition(new_inputs0, act, r_, new_inputs1,done)</span><br></pre></td></tr></table></figure><p>OK, we had finished the description for DDPG and HER algorithms</p><h3 id="Other-tips-and-information-that-maybe-helpful"><a href="#Other-tips-and-information-that-maybe-helpful" class="headerlink" title="Other tips and information that maybe helpful"></a>Other tips and information that maybe helpful</h3><p>For now, we had talked about DDPG+HER that can be applied to a robot control project and you may can test the algorithm for now, but there are also many other information that I find very interesting. </p><h5 id="Curriculum-learning-in-RL"><a href="#Curriculum-learning-in-RL" class="headerlink" title="Curriculum learning in RL"></a>Curriculum learning in RL</h5><p>This is the concept that make the robot to start learning from simple tasks and then move on to more complex tasks. For example, if you want your robot to learn how to pick up a ring and set it to a bar. You can firstly teach the robot to hold a ring in the position that is close to the bar to set it to a bar, then hold a ring in the position that is far away from the bar to set it to a bar and so on. Finally, teach the robot to learning the task in the final lesson. Such curriculum can be generated reversely, start from final steps of the task to the first step of the task. </p><h5 id="How-to-add-additional-command-to-your-robot"><a href="#How-to-add-additional-command-to-your-robot" class="headerlink" title="How to add additional command to your robot"></a>How to add additional command to your robot</h5><p>This is the <a href="https://www.youtube.com/watch?v=Wypc1a-1ZYA&amp;list=PLn8PRpmsu08qw_IwpgVNsKiJQpvvW0MmM&amp;index=4" target="_blank" rel="noopener">video</a> from Matlab. From 8:50, they start to discuss this issue. In short, it was achieved by adding additional command to robot and modify reward about the command. </p><h5 id="Some-problems-about-RL-in-robot-control-and-how-to-reduce-their-impact"><a href="#Some-problems-about-RL-in-robot-control-and-how-to-reduce-their-impact" class="headerlink" title="Some problems about RL in robot control and how to reduce their impact"></a>Some problems about RL in robot control and how to reduce their impact</h5><p>Also the <a href="https://www.youtube.com/watch?v=zHV3UcH-nr0&amp;list=PLn8PRpmsu08qw_IwpgVNsKiJQpvvW0MmM&amp;index=5" target="_blank" rel="noopener">video</a> from Matlab. In fact, these are also the problems about neural network. </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Investigation-for-Reinforcement-Learning-RL-in-robotic&quot;&gt;&lt;a href=&quot;#Investigation-for-Reinforcement-Learning-RL-in-robotic&quot; class=&quot;hea
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>SiamMask Note</title>
    <link href="https://num-github.github.io/2019/07/12/Review%20of%20SiamMask/"/>
    <id>https://num-github.github.io/2019/07/12/Review of SiamMask/</id>
    <published>2019-07-11T23:00:00.000Z</published>
    <updated>2019-08-30T13:19:57.433Z</updated>
    
    <content type="html"><![CDATA[<h3 id="SiamMask-Note"><a href="#SiamMask-Note" class="headerlink" title="SiamMask Note"></a><a href="https://arxiv.org/pdf/1812.05050.pdf" target="_blank" rel="noopener">SiamMask</a> Note</h3><p><em>SiamMask is a model that is that I found very interesting recently. It is the model that can automatically mask an instance as long as an initial bounding boxes has been given you can find the code in github at <a href="https://github.com/foolwood/SiamMask" target="_blank" rel="noopener">here</a></em></p><p>In this article I will describe the development of siamMask based on its time line: from SiamFC, then SiamRPN, finally SiamMask</p><h4 id="SiamFC"><a href="#SiamFC" class="headerlink" title="SiamFC"></a>SiamFC</h4><p>SiamFC is the network for object tracking. The basic thinking for it is to continually compare the similarity between the first image and sliding windows from the last few images so as to generate a score map or <strong>RoW</strong> for similarity.</p><p><img src="/2019/07/12/Review of SiamMask/1563725602695.png" alt="1563725602695"></p><p>The mathematical expression for this SiamFC can be expressed as </p><script type="math/tex; mode=display">g_s(z,x)=f_e(z)*f_e(x)</script><p>where z is the initial images, x is the next large images, $f_e(z) $ is embedding network and * simple cross-correlation. The training goal for this network is to maximize the score in score map to correspond to the target location is search area.</p><h5 id="What’s-improved-on-siamMask"><a href="#What’s-improved-on-siamMask" class="headerlink" title="What’s improved on siamMask"></a>What’s improved on siamMask</h5><p>For SiamMask, they improved the network architecture by output multi-channel score map rather than a single score map. Then, it replace simple cross-correlation with depth-wise cross-correlation which is a neural network. </p><h4 id="SiamMask"><a href="#SiamMask" class="headerlink" title="SiamMask"></a>SiamMask</h4><p>The architecture of SiamMask was shown blow</p><p><img src="/2019/07/12/Review of SiamMask/1563872260341.png" alt="1563872260341"></p><p>As you can see here, an mask and boxes part is added to the end of SiamMask. In this case, we just mainly talk about the mask part, the box and score is mainly proposed in SiamRPN.</p><p>More detailed architecture of it was shown in the next figure. The main <a href="https://github.com/foolwood/SiamMask" target="_blank" rel="noopener">code</a> is in custom.py. The code don’t have much comments, so I select some important code and add some comments for explanation in the following explanation.   </p><p><img src="/2019/07/12/Review of SiamMask/1563874125216.png" alt="1563874125216"></p><p>In the architecture above, the mask head was replaced by a refinement model which was inspired by <a href="https://arxiv.org/pdf/1603.08695.pdf" target="_blank" rel="noopener">this paper</a> which aim to refine the smaller mask to a bigger mask by using the features extracted from pervious convolution. </p><p>The whole architecture of it was written as shown below</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Custom</span><span class="params">(SiamMask)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, pretrain=False, **kwargs)</span>:</span></span><br><span class="line">        super(Custom, self).__init__(**kwargs)</span><br><span class="line">        <span class="comment">#tailored ResNet50+adjust layer</span></span><br><span class="line">        self.features = ResDown(pretrain=pretrain)</span><br><span class="line">        <span class="comment">#the parts for boundings boxes and classification (not discussed here, more detail are in SiamRPN)</span></span><br><span class="line">        self.rpn_model = UP(anchor_num=self.anchor_num, feature_in=<span class="number">256</span>, feature_out=<span class="number">256</span>)</span><br><span class="line">        <span class="comment">#Depth-wise correlation Maskcorr()&lt;-DepthCorr()</span></span><br><span class="line">        self.mask_model = MaskCorr()</span><br><span class="line">        <span class="comment">#Refinement part</span></span><br><span class="line">        self.refine_model = Refine()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">refine</span><span class="params">(self, f, pos=None)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.refine_model(f, pos)</span><br><span class="line">    </span><br><span class="line"><span class="comment">#Embedding for the initialization image (the smaller 127×127×3 image z) for target.</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">template</span><span class="params">(self, template)</span>:</span></span><br><span class="line">        self.zf = self.features(template)</span><br><span class="line">        </span><br><span class="line"><span class="comment">#For classification and bounding boxes</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">track</span><span class="params">(self, search)</span>:</span></span><br><span class="line">        search = self.features(search)</span><br><span class="line">        rpn_pred_cls, rpn_pred_loc = self.rpn(self.zf, search)</span><br><span class="line">        <span class="keyword">return</span> rpn_pred_cls, rpn_pred_loc</span><br><span class="line"></span><br><span class="line">    <span class="comment">#The main archtecture  </span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">track_mask</span><span class="params">(self, search)</span>:</span></span><br><span class="line">        <span class="comment"># self.feature is the output feature extracted from ResNet at each layer stage</span></span><br><span class="line">        <span class="comment">#self.search is the output of adjust layer</span></span><br><span class="line">        self.feature, self.search = self.features.forward_all(search)</span><br><span class="line">        <span class="comment">#the parts for boundings boxes and classification (not discussed here, more detail are in SiamRPN)</span></span><br><span class="line">        rpn_pred_cls, rpn_pred_loc = self.rpn(self.zf, self.search)</span><br><span class="line">        <span class="comment">#Initialized embedding and the embedding for search image feed into depth-wise correlation without calling the head for generating mask</span></span><br><span class="line">        self.corr_feature = self.mask_model.mask.forward_corr(self.zf, self.search)</span><br><span class="line">        <span class="comment"># mask output without refinement (call mathod 'head' in DepthCorr())</span></span><br><span class="line">        pred_mask = self.mask_model.mask.head(self.corr_feature)</span><br><span class="line">        <span class="keyword">return</span> rpn_pred_cls, rpn_pred_loc, pred_mask</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">track_refine</span><span class="params">(self, pos)</span>:</span></span><br><span class="line">        <span class="comment"># mask output with refinement</span></span><br><span class="line">        pred_mask = self.refine_model(self.feature, self.corr_feature, pos=pos, test=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> pred_mask</span><br></pre></td></tr></table></figure><p>The embedding network is two ResNet50 with only the first four layers and then added an adjust layer to adjust channels. The important code for these two part  is shown in below (unnecessary code was deleted).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResDown</span><span class="params">(MultiStageFeature)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, pretrain=False)</span>:</span></span><br><span class="line">        super(ResDown, self).__init__()</span><br><span class="line">        self.features = resnet50(layer3=<span class="literal">True</span>, layer4=<span class="literal">False</span>)</span><br><span class="line">        self.downsample = ResDownS(<span class="number">1024</span>, <span class="number">256</span>)</span><br><span class="line"><span class="comment">#In forward or forward_all, the tailored Resnet50 and adjust layer (downsample) was connected</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        output = self.features(x)</span><br><span class="line">        p3 = self.downsample(output[<span class="number">-1</span>])</span><br><span class="line">        <span class="keyword">return</span> p3 </span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward_all</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        output = self.features(x)</span><br><span class="line">        p3 = self.downsample(output[<span class="number">-1</span>])</span><br><span class="line">        <span class="keyword">return</span> output, p3  </span><br><span class="line"><span class="comment">#The 'output' is the features extracted by ResNet and can be used of mask refinement.</span></span><br><span class="line"><span class="comment">#p3 is the embeding that will be feed into depth-wise correlation.</span></span><br></pre></td></tr></table></figure><p>The ‘output’ is the features extracted by ResNet and can be used of mask refinement, which consists the 3 features for refinement and one features for adjustment. The forward output of its ResNet is 4 values from each stage of ResNet.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">       ........</span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">       x = self.conv1(x)</span><br><span class="line">       x = self.bn1(x)</span><br><span class="line">       p0 = self.relu(x)</span><br><span class="line">       x = self.maxpool(p0)</span><br><span class="line"></span><br><span class="line">       p1 = self.layer1(x)</span><br><span class="line">       p2 = self.layer2(p1)</span><br><span class="line">       p3 = self.layer3(p2)</span><br><span class="line"></span><br><span class="line">       <span class="keyword">return</span> p0, p1, p2, p3</span><br></pre></td></tr></table></figure><p>p3 is the embedding that will be feed into depth-wise correlation.</p><p>In adjustment layer, one 1×1 kernel convolutional layer was applied with batch normalization.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResDownS</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, inplane, outplane)</span>:</span></span><br><span class="line">        super(ResDownS, self).__init__()</span><br><span class="line">        self.downsample = nn.Sequential(</span><br><span class="line">                nn.Conv2d(inplane, outplane, kernel_size=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">                nn.BatchNorm2d(outplane))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.downsample(x)</span><br><span class="line">        <span class="keyword">if</span> x.size(<span class="number">3</span>) &lt; <span class="number">20</span>:</span><br><span class="line">            l = <span class="number">4</span></span><br><span class="line">            r = <span class="number">-4</span></span><br><span class="line">            x = x[:, :, l:r, l:r]</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p>Depth-wise correlation is still achieved with the architecture as below (the parameters of in_channels, hidden, out_channels are 256, 256, 63*63)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MaskCorr</span><span class="params">(Mask)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, oSz=<span class="number">63</span>)</span>:</span></span><br><span class="line">        super(MaskCorr, self).__init__()</span><br><span class="line">        self.oSz = oSz</span><br><span class="line">        self.mask = DepthCorr(<span class="number">256</span>, <span class="number">256</span>, self.oSz**<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, z, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.mask(z, x)</span><br></pre></td></tr></table></figure><p>DepthCorr called by  MaskCorr.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d_dw_group</span><span class="params">(x, kernel)</span>:</span></span><br><span class="line">    batch, channel = kernel.shape[:<span class="number">2</span>]</span><br><span class="line">    x = x.view(<span class="number">1</span>, batch*channel, x.size(<span class="number">2</span>), x.size(<span class="number">3</span>))  <span class="comment"># 1 * (b*c) * k * k</span></span><br><span class="line">    kernel = kernel.view(batch*channel, <span class="number">1</span>, kernel.size(<span class="number">2</span>), kernel.size(<span class="number">3</span>))  <span class="comment"># (b*c) * 1 * H * W</span></span><br><span class="line">    out = F.conv2d(x, kernel, groups=batch*channel)</span><br><span class="line">    out = out.view(batch, channel, out.size(<span class="number">2</span>), out.size(<span class="number">3</span>))</span><br><span class="line">    <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DepthCorr</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_channels, hidden, out_channels, kernel_size=<span class="number">3</span>)</span>:</span></span><br><span class="line">        super(DepthCorr, self).__init__()</span><br><span class="line">        <span class="comment"># adjust layer for asymmetrical features</span></span><br><span class="line">        self.conv_kernel = nn.Sequential(</span><br><span class="line">                nn.Conv2d(in_channels, hidden, kernel_size=kernel_size, bias=<span class="literal">False</span>),</span><br><span class="line">                nn.BatchNorm2d(hidden),</span><br><span class="line">                nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">                )</span><br><span class="line">        self.conv_search = nn.Sequential(</span><br><span class="line">                nn.Conv2d(in_channels, hidden, kernel_size=kernel_size, bias=<span class="literal">False</span>),</span><br><span class="line">                nn.BatchNorm2d(hidden),</span><br><span class="line">                nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">                )</span><br><span class="line"><span class="comment">#mask head</span></span><br><span class="line">        self.head = nn.Sequential(</span><br><span class="line">                nn.Conv2d(hidden, hidden, kernel_size=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">                nn.BatchNorm2d(hidden),</span><br><span class="line">                nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">                nn.Conv2d(hidden, out_channels, kernel_size=<span class="number">1</span>)</span><br><span class="line">                )</span><br><span class="line"><span class="comment">#output without adding mask head</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward_corr</span><span class="params">(self, kernel, input)</span>:</span></span><br><span class="line">        kernel = self.conv_kernel(kernel)</span><br><span class="line">        input = self.conv_search(input)</span><br><span class="line">        feature = conv2d_dw_group(input, kernel)</span><br><span class="line">        <span class="keyword">return</span> feature</span><br><span class="line"><span class="comment">#output with mask head</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, kernel, search)</span>:</span></span><br><span class="line">        feature = self.forward_corr(kernel, search)</span><br><span class="line">        out = self.head(feature)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure><p>In Refinement stage, you can clearly see the structure: deconvolute the output from depth-wise convolution and the three stage refinement in last few lines of forward method.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Refine</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Refine, self).__init__()</span><br><span class="line">        self.v0 = nn.Sequential(nn.Conv2d(<span class="number">64</span>, <span class="number">16</span>, <span class="number">3</span>, padding=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">                           nn.Conv2d(<span class="number">16</span>, <span class="number">4</span>, <span class="number">3</span>, padding=<span class="number">1</span>),nn.ReLU())</span><br><span class="line"></span><br><span class="line">        self.v1 = nn.Sequential(nn.Conv2d(<span class="number">256</span>, <span class="number">64</span>, <span class="number">3</span>, padding=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">                           nn.Conv2d(<span class="number">64</span>, <span class="number">16</span>, <span class="number">3</span>, padding=<span class="number">1</span>), nn.ReLU())</span><br><span class="line"></span><br><span class="line">        self.v2 = nn.Sequential(nn.Conv2d(<span class="number">512</span>, <span class="number">128</span>, <span class="number">3</span>, padding=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">                           nn.Conv2d(<span class="number">128</span>, <span class="number">32</span>, <span class="number">3</span>, padding=<span class="number">1</span>), nn.ReLU())</span><br><span class="line"></span><br><span class="line">        self.h2 = nn.Sequential(nn.Conv2d(<span class="number">32</span>, <span class="number">32</span>, <span class="number">3</span>, padding=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">                           nn.Conv2d(<span class="number">32</span>, <span class="number">32</span>, <span class="number">3</span>, padding=<span class="number">1</span>), nn.ReLU())</span><br><span class="line"></span><br><span class="line">        self.h1 = nn.Sequential(nn.Conv2d(<span class="number">16</span>, <span class="number">16</span>, <span class="number">3</span>, padding=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">                           nn.Conv2d(<span class="number">16</span>, <span class="number">16</span>, <span class="number">3</span>, padding=<span class="number">1</span>), nn.ReLU())</span><br><span class="line"></span><br><span class="line">        self.h0 = nn.Sequential(nn.Conv2d(<span class="number">4</span>, <span class="number">4</span>, <span class="number">3</span>, padding=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">                           nn.Conv2d(<span class="number">4</span>, <span class="number">4</span>, <span class="number">3</span>, padding=<span class="number">1</span>), nn.ReLU())</span><br><span class="line"></span><br><span class="line">        self.deconv = nn.ConvTranspose2d(<span class="number">256</span>, <span class="number">32</span>, <span class="number">15</span>, <span class="number">15</span>)</span><br><span class="line"></span><br><span class="line">        self.post0 = nn.Conv2d(<span class="number">32</span>, <span class="number">16</span>, <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.post1 = nn.Conv2d(<span class="number">16</span>, <span class="number">4</span>, <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.post2 = nn.Conv2d(<span class="number">4</span>, <span class="number">1</span>, <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> modules <span class="keyword">in</span> [self.v0, self.v1, self.v2, self.h2, self.h1, self.h0, self.deconv, self.post0, self.post1, self.post2,]:</span><br><span class="line">            <span class="keyword">for</span> l <span class="keyword">in</span> modules.modules():</span><br><span class="line">                <span class="keyword">if</span> isinstance(l, nn.Conv2d):</span><br><span class="line">                    nn.init.kaiming_uniform_(l.weight, a=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, f, corr_feature, pos=None, test=False)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> test:</span><br><span class="line">            p0 = torch.nn.functional.pad(f[<span class="number">0</span>], [<span class="number">16</span>, <span class="number">16</span>, <span class="number">16</span>, <span class="number">16</span>])[:, :, <span class="number">4</span>*pos[<span class="number">0</span>]:<span class="number">4</span>*pos[<span class="number">0</span>]+<span class="number">61</span>, <span class="number">4</span>*pos[<span class="number">1</span>]:<span class="number">4</span>*pos[<span class="number">1</span>]+<span class="number">61</span>]</span><br><span class="line">            p1 = torch.nn.functional.pad(f[<span class="number">1</span>], [<span class="number">8</span>, <span class="number">8</span>, <span class="number">8</span>, <span class="number">8</span>])[:, :, <span class="number">2</span> * pos[<span class="number">0</span>]:<span class="number">2</span> * pos[<span class="number">0</span>] + <span class="number">31</span>, <span class="number">2</span> * pos[<span class="number">1</span>]:<span class="number">2</span> * pos[<span class="number">1</span>] + <span class="number">31</span>]</span><br><span class="line">            p2 = torch.nn.functional.pad(f[<span class="number">2</span>], [<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>])[:, :, pos[<span class="number">0</span>]:pos[<span class="number">0</span>] + <span class="number">15</span>, pos[<span class="number">1</span>]:pos[<span class="number">1</span>] + <span class="number">15</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            p0 = F.unfold(f[<span class="number">0</span>], (<span class="number">61</span>, <span class="number">61</span>), padding=<span class="number">0</span>, stride=<span class="number">4</span>).permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>).contiguous().view(<span class="number">-1</span>, <span class="number">64</span>, <span class="number">61</span>, <span class="number">61</span>)</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> (pos <span class="keyword">is</span> <span class="literal">None</span>): p0 = torch.index_select(p0, <span class="number">0</span>, pos)</span><br><span class="line">            p1 = F.unfold(f[<span class="number">1</span>], (<span class="number">31</span>, <span class="number">31</span>), padding=<span class="number">0</span>, stride=<span class="number">2</span>).permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>).contiguous().view(<span class="number">-1</span>, <span class="number">256</span>, <span class="number">31</span>, <span class="number">31</span>)</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> (pos <span class="keyword">is</span> <span class="literal">None</span>): p1 = torch.index_select(p1, <span class="number">0</span>, pos)</span><br><span class="line">            p2 = F.unfold(f[<span class="number">2</span>], (<span class="number">15</span>, <span class="number">15</span>), padding=<span class="number">0</span>, stride=<span class="number">1</span>).permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>).contiguous().view(<span class="number">-1</span>, <span class="number">512</span>, <span class="number">15</span>, <span class="number">15</span>)</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> (pos <span class="keyword">is</span> <span class="literal">None</span>): p2 = torch.index_select(p2, <span class="number">0</span>, pos)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span>(pos <span class="keyword">is</span> <span class="literal">None</span>):</span><br><span class="line">            p3 = corr_feature[:, :, pos[<span class="number">0</span>], pos[<span class="number">1</span>]].view(<span class="number">-1</span>, <span class="number">256</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            p3 = corr_feature.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>).contiguous().view(<span class="number">-1</span>, <span class="number">256</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">##################The structure of refinement. ##################################</span></span><br><span class="line">        out = self.deconv(p3)</span><br><span class="line">        out = self.post0(F.upsample(self.h2(out) + self.v2(p2), size=(<span class="number">31</span>, <span class="number">31</span>)))</span><br><span class="line">        out = self.post1(F.upsample(self.h1(out) + self.v1(p1), size=(<span class="number">61</span>, <span class="number">61</span>)))</span><br><span class="line">        out = self.post2(F.upsample(self.h0(out) + self.v0(p0), size=(<span class="number">127</span>, <span class="number">127</span>)))</span><br><span class="line">        out = out.view(<span class="number">-1</span>, <span class="number">127</span>*<span class="number">127</span>)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure><p>Loss function</p><h4 id="SiamRPN"><a href="#SiamRPN" class="headerlink" title="SiamRPN"></a>SiamRPN</h4><p>Ok, after talking about SiamMask, let’s move on to the parts about classification and bounding box predication. The original figure of siamRPN is the figure shown below, although the later version siamRPN++ has some modified version of it, but I think the following diagram is clear enough to show the core thinking of it. </p><p><img src="/2019/07/12/Review of SiamMask/1563889858564.png" alt="1563889858564"></p><p>Basically, what SiamRPN doing in SiamMask input the two features from a network, in this case, ResNet,  and pass them to classification block and regression block. Then, in these two block, the  the depth-wise correlation will be calculated and gives a series of classification groups and regression groups to determine the bounding boxes. </p><h5 id="Some-problems-that-I-thought"><a href="#Some-problems-that-I-thought" class="headerlink" title="Some problems that I thought:"></a>Some problems that I thought:</h5><p>This model don’t have the function for memory, I think it would be better to add some technique like RNN to update the embedding. </p><p>It only initialize the image but not mask, so the quality of tracking high affected by the first image and in some cases, you cannot initialize an object that with a rectangles.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;SiamMask-Note&quot;&gt;&lt;a href=&quot;#SiamMask-Note&quot; class=&quot;headerlink&quot; title=&quot;SiamMask Note&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://arxiv.org/pdf/1812.05050.pdf&quot; 
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Problems for using gpu for Tensorflow or Keras network on linux</title>
    <link href="https://num-github.github.io/2019/06/30/Problems-for-using-gpu-for-Tensorflow-or-Keras-network-on-linux/"/>
    <id>https://num-github.github.io/2019/06/30/Problems-for-using-gpu-for-Tensorflow-or-Keras-network-on-linux/</id>
    <published>2019-06-30T12:45:06.155Z</published>
    <updated>2019-08-30T23:08:17.451Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Problems-for-using-gpu-for-Tensorflow-or-Keras-network-on-linux-tensorflow-can-only-see-CPU-Could-not-dlopen-library-‘libcuda-so-1’-failed-call-to-cuInit-UNKNOWN-ERROR-303"><a href="#Problems-for-using-gpu-for-Tensorflow-or-Keras-network-on-linux-tensorflow-can-only-see-CPU-Could-not-dlopen-library-‘libcuda-so-1’-failed-call-to-cuInit-UNKNOWN-ERROR-303" class="headerlink" title="Problems for using gpu for Tensorflow or Keras network on linux: tensorflow can only see CPU: Could not dlopen library ‘libcuda.so.1’;  failed call to cuInit: UNKNOWN ERROR (303);"></a>Problems for using gpu for Tensorflow or Keras network on linux: tensorflow can only see CPU: Could not dlopen library ‘libcuda.so.1’;  failed call to cuInit: UNKNOWN ERROR (303);</h1><p>The problem that I meet here is that when using an GPU cluster, my code ‘’with tf.device(‘/gpu:0’):’’ cannot run. If you meet the same problem, you can firstly run the following code:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">'CUDA_VISIBLE_DEVICES'</span>] = <span class="string">"0"</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.python.client <span class="keyword">import</span> device_lib</span><br><span class="line">print(device_lib.list_local_devices())</span><br></pre></td></tr></table></figure><p>If the code above cannot show any information about your GPU, then you need to check the cuda and cudnn that you installed. The version must meet your tensorflow version: <a href="https://www.tensorflow.org/install/source#tested_build_configurations" target="_blank" rel="noopener">https://www.tensorflow.org/install/source#tested_build_configurations</a></p><p>If you don’t have cude or cudnn, you can following the steps of AndrewPt in <a href="https://stackoverflow.com/questions/41402409/tensorflow-doesnt-seem-to-see-my-gpu" target="_blank" rel="noopener">https://stackoverflow.com/questions/41402409/tensorflow-doesnt-seem-to-see-my-gpu</a> to install them (but it is windows, be careful for you linux, you need to down different documents)</p><p>Then check whether you LD_LIBRARY_PATH contain the path for right cuda path, the path mush contain ‘libcuda.so.1’ ( for me it is in ‘’/home/.usr/local/cuda-10.0/lib64/stubs’), </p><p>and  ‘libcuda.so.XX.X (XX.X is your cuda version for me it is in /home/.usr/local/cuda-10.0/lib64)</p><p>Then, you must also ensure the cudnn file that contain libcudnn.so.7 is in LD_LIBRARY_PATH. For this step, you need to download cudnn. For me, I was download cudnn 7.4 for cuda 10.0 in <a href="https://developer.nvidia.com/compute/machine-learning/cudnn/secure/v7.4.1.5/prod/10.0_20181108/RHEL7_3-x64/libcudnn7-7.4.1.5-1.cuda10.0.x86_64.rpm" target="_blank" rel="noopener">cuDNN Runtime Library for RedHat/Centos 7.3 (RPM)</a> (libcudnn7-7.4.1.5-1.cuda10.0.x86_64), and there is libcudnn.so.7 in \libcudnn7-7.4.1.5-1.cuda10.0.x86_64\usr\lib64 and you can also see l ibcudnn.so.7.4.1 in it. </p><p>Extract and copy this to your linux and then, you need to delete  libcudnn.so.7, the run the following code to generate it </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ln -s libcudnn.so.7.4.1 libcudnn.so.7</span><br><span class="line">ln -s libcudnn.so.7 libcudnn.so </span><br><span class="line">ldconfig</span><br></pre></td></tr></table></figure><p>You can see both libcudnn.so.7.4.1 and libcudnn.so.7 is in the file. You can add it to  LD_LIBRARY_PATH and then run </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">'CUDA_VISIBLE_DEVICES'</span>] = <span class="string">"0"</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.python.client <span class="keyword">import</span> device_lib</span><br><span class="line">print(device_lib.list_local_devices())</span><br></pre></td></tr></table></figure><p>The tensorflow should see the gpu without an error </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">2019-06-30 14:24:17.432330: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: </span><br><span class="line">name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582</span><br><span class="line">pciBusID: 0000:db:00.0</span><br><span class="line">2019-06-30 14:24:17.432437: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0</span><br><span class="line">2019-06-30 14:24:17.432473: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0</span><br><span class="line">2019-06-30 14:24:17.432502: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0</span><br><span class="line">2019-06-30 14:24:17.432529: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0</span><br><span class="line">2019-06-30 14:24:17.432556: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0</span><br><span class="line">2019-06-30 14:24:17.432582: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0</span><br><span class="line">2019-06-30 14:24:17.432610: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7</span><br><span class="line">2019-06-30 14:24:17.478617: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0</span><br><span class="line">2019-06-30 14:24:17.478691: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:</span><br><span class="line">2019-06-30 14:24:17.478715: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 </span><br><span class="line">2019-06-30 14:24:17.478734: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N </span><br><span class="line">2019-06-30 14:24:17.498562: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11439 MB memory) -&gt; physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:db:00.0, compute capability: 6.1)</span><br><span class="line">[name: &quot;/device:CPU:0&quot;</span><br><span class="line">device_type: &quot;CPU&quot;</span><br><span class="line">memory_limit: 268435456</span><br><span class="line">locality &#123;</span><br><span class="line">&#125;</span><br><span class="line">incarnation: 7765454328739896397</span><br><span class="line">, name: &quot;/device:XLA_GPU:0&quot;</span><br><span class="line">device_type: &quot;XLA_GPU&quot;</span><br><span class="line">memory_limit: 17179869184</span><br><span class="line">locality &#123;</span><br><span class="line">&#125;</span><br><span class="line">incarnation: 14436236814517186873</span><br><span class="line">physical_device_desc: &quot;device: XLA_GPU device&quot;</span><br><span class="line">, name: &quot;/device:XLA_CPU:0&quot;</span><br><span class="line">device_type: &quot;XLA_CPU&quot;</span><br><span class="line">memory_limit: 17179869184</span><br><span class="line">locality &#123;</span><br><span class="line">&#125;</span><br><span class="line">incarnation: 10506527593758283137</span><br><span class="line">physical_device_desc: &quot;device: XLA_CPU device&quot;</span><br><span class="line">, name: &quot;/device:GPU:0&quot;</span><br><span class="line">device_type: &quot;GPU&quot;</span><br><span class="line">memory_limit: 11994670695</span><br><span class="line">locality &#123;</span><br><span class="line">  bus_id: 2</span><br><span class="line">  numa_node: 1</span><br><span class="line">  links &#123;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">incarnation: 9840900236876381126</span><br><span class="line">physical_device_desc: &quot;device: 0, name: TITAN Xp, pci bus id: 0000:db:00.0, compute capability: 6.1&quot;</span><br><span class="line">]</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Problems-for-using-gpu-for-Tensorflow-or-Keras-network-on-linux-tensorflow-can-only-see-CPU-Could-not-dlopen-library-‘libcuda-so-1’-
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Distribution related function in Seaborn</title>
    <link href="https://num-github.github.io/2019/06/10/seaborn-1/"/>
    <id>https://num-github.github.io/2019/06/10/seaborn-1/</id>
    <published>2019-06-10T00:30:07.509Z</published>
    <updated>2019-06-10T01:21:51.489Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Some-useful-distribution-related-function-in-Seaborn"><a href="#Some-useful-distribution-related-function-in-Seaborn" class="headerlink" title="Some useful distribution related function in Seaborn"></a>Some useful distribution related function in Seaborn</h1><p><em><font size="1" color="gray">I recently wrote some code for training some data with ML algorithm and I found that some functions are pretty used for data analysis</font></em></p><details open><summary><font size="4">  distplot() </font></summary>distplot() function are able to show univariate distribution. For more detailed information, click <a href="https://seaborn.pydata.org/tutorial/distributions.html#distribution-tutorial" target="_blank" rel="noopener"> here</a>. *<font size="2" color="gray">   If you want to know a distribution of a numerical feature, you can try this.</font>*<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import seaborn as sns </span><br><span class="line">sns.distplot(Input[0],label=&apos;Input&apos;,color=&apos;g&apos;)</span><br></pre></td></tr></table></figure>#### The commonly used parameters : * The input data * label: It will show on legend once you call **plt.legend()**. * Color: The color of the histogram and lines *(parameters are similar to matlab and the default color for first line is blue)*.You can draw distribution of one dimensional data by this with matplotlib.pyplot.<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import seaborn as sns </span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">plt.figure()</span><br><span class="line"># Sometimes you may need to transpose(.T) your numpy array </span><br><span class="line">sns.distplot(fail_data[0],label=&apos;fail_data&apos;,color=&apos;g&apos;)</span><br><span class="line">plt.legend();</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>fail_data[0] is the first dimension of my data. The Result looks like that.<div align="center"><img src="https://num-github.github.io/2019/06/10/seaborn-1/chrome_2019-06-10_00-12-30.png" height="300" width="400"></div>Your image could be quite different, but both **histogram** and  <a href="https://en.wikipedia.org/wiki/Kernel_density_estimation" target="_blank" rel="noopener"> kernel density estimate (KDE)</a> line .You could choose to remove histogram by setting **<font color="blue">hist=False</font>** or KDE by **<font color="blue">kde=False</font>**<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plt.figure()</span><br><span class="line">sns.distplot(hist=False,fail_data[0],label=&apos;fail_data&apos;,color=&apos;g&apos;)</span><br><span class="line">plt.legend();</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><div align="center"><img src="https://num-github.github.io/2019/06/10/seaborn-1/chrome_2019-06-10_00-37-08.png" height="300" width="400"></div>If you want to draw more than one distribution on same figure, just add anther above plt.show()<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plt.figure()</span><br><span class="line">sns.distplot(scuss_data[2],label=&apos;scuss_data&apos;)</span><br><span class="line">sns.distplot(fail_data[2],color=&apos;g&apos;,label=&apos;fail_data&apos;)</span><br><span class="line">plt.legend();</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><div align="center"><img src="https://num-github.github.io/2019/06/10/seaborn-1/chrome_2019-06-10_00-26-08.png" height="300" width="400"></div></details><details open><summary><font size="4">  violinplot() </font></summary>It is still used for showing distribution. The data I used here are look like that:<div align="center">|0       |    1         |2      |    3||:--:|:--:|:--:|:--:|| 0.000020        |0.001459    |7.992451|    1.0|| 0.000021    |0.001149    |6.903953    |0.0||    0.000029|0.001637|    6.512294    |0.0||    0.000028|0.001208|    1.936095    |1.0|。。。。。。。。。。。。。。。。 </div>The first 3 rows are values correspond to feature 0,1,2.The third row is classes (only 0 and 1)Thus, if I want to find the distribution of feature 1 in each class, we can just write <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plt.figure()</span><br><span class="line">sns.violinplot(x=3,y=1, data=pd.DataFrame(data))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><div align="center"><img src="https://num-github.github.io/2019/06/10/seaborn-1/chrome_2019-06-10_01-12-02.png" height="300" width="400"></div>* x is class row* y is feature row* Input data must be pandas data structure.* You can also input the name (string) of each row to x and y, if you have.You can also plot many figures in this way <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">f, ax1 = plt.subplots(ncols=2, nrows=2, figsize=(8, 8))</span><br><span class="line">sns.violinplot(x=3,y=0, data=pd.DataFrame(data),ax=ax1[0][0])</span><br><span class="line">ax1[0][0].set_title(&apos;x&apos;)</span><br><span class="line">sns.violinplot(x=3,y=1, data=pd.DataFrame(data),ax=ax1[0][1])</span><br><span class="line">ax1[0][1].set_title(&apos;y&apos;)</span><br><span class="line">sns.violinplot(x=3,y=2, data=pd.DataFrame(data),ax=ax1[1][0])</span><br><span class="line">ax1[1][0].set_title(&apos;z&apos;)</span><br><span class="line">f.tight_layout()</span><br></pre></td></tr></table></figure>* ax can be used to set the position of figures<div align="center"><img src="https://num-github.github.io/2019/06/10/seaborn-1/chrome_2019-06-10_01-19-24.png" height="300" width="400"></div></details>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Some-useful-distribution-related-function-in-Seaborn&quot;&gt;&lt;a href=&quot;#Some-useful-distribution-related-function-in-Seaborn&quot; class=&quot;headerl
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Get start !</title>
    <link href="https://num-github.github.io/2019/06/06/Get-start/"/>
    <id>https://num-github.github.io/2019/06/06/Get-start/</id>
    <published>2019-06-05T23:06:17.000Z</published>
    <updated>2019-06-05T23:07:50.594Z</updated>
    
    <content type="html"><![CDATA[<p>The start of my career.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;The start of my career.&lt;/p&gt;

      
    
    </summary>
    
    
  </entry>
  
</feed>
