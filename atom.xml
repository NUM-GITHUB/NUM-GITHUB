<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>FengerBlog</title>
  
  <subtitle>FengerBlog</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://num-github.github.io/"/>
  <updated>2019-07-31T15:37:56.009Z</updated>
  <id>https://num-github.github.io/</id>
  
  <author>
    <name>Fenger</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>gazebo and unity comparation</title>
    <link href="https://num-github.github.io/2019/07/31/gazebo-and-unity-comparation/"/>
    <id>https://num-github.github.io/2019/07/31/gazebo-and-unity-comparation/</id>
    <published>2019-07-31T15:36:28.000Z</published>
    <updated>2019-07-31T15:37:56.009Z</updated>
    
    <content type="html"><![CDATA[<h3 id="gazebo-and-unity-comparation"><a href="#gazebo-and-unity-comparation" class="headerlink" title="gazebo and unity comparation"></a>gazebo and unity comparation</h3><p>Unity have better functionality. It is very powerful, fast and easy to use compared to gazebo. You don’t need to modify some sdf or other files directly to change the hierarchy and inspector directly in order to change the parameters of an object and it support many types of files. The animation and world built by unity is more beautiful than gazebo, as well. Actually, unity is able to generate and edit some necessary file by using ROS# that is necessary for ROS and even do some simulation (even though you need to using TCP protocol to connect to ROS). Therefore, from the respect of editing sdf, URDF or do some animation, unity is more superior. Furthermore, unity can, according to ROS# Wiki, do both simulation and visualization together instead of separate tools (rviz for visualization, gazebo for simulation). Moreover, since this lab is mixed reality lab, you might have some persons that are expert in unity or you add some VR or AR stuffs to the robot, so in these applications, unity is always better than gazebo in develop these stuffs, especially multiclient support is necessary. </p><p>For gazebo, however, the main advantages are more focus on ROS itself, it has a straightforward connection, and I think it is good for beginners who just want to do some work on robot. The learning materials or tutorials that relate to ROS are far more than unity. In fact, if you have no knowledge about C# or don’t want to write some raw code to communication with ROS, the best thing that I found is ROS# for unity, which only have its wiki and few videos. Thus, for a beginner, especially the person who have no knowledge on both software, gazebo could be better. The next thing about that is plugin and support, Gazebo have many plugins for robot. According to the official tutorial, there are already some plugins like cameras, IMUs, and lasers that is written by some company or people (most of them written in C\C++). They can be easily called and used directly by following the step of tutorial. In addition, some company that produce robot (e.g. swayer robot) provide relevant simulation code for gazebo, you can just run and try it directly. In contrast, for unity, if you want to use such sensors, it would be harder to find the code to simulate it in unity and even if you are able to write a plugin or find something very similar, it might necessary to learn C# (I’m not sure other students, but for electrical and electronic engineering, only C\C++ is compulsory). </p><p>In fact, according to ROS#, unity and gazebo can be used together. There is a demo that set up a simulation world on gazebo and do visualization on unity, so I think depending what we will do, we can try both methods to achieve our goals. </p><p>In summary, unity have better and useful functionality, but gazebo is good at dealing with ROS, so in order to determine which method to choose, we would better determine what is available and what’s the whole project looks like. In next section, based on the analysis above, I will list some conditions that unity or gazebo would be certainly better. </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;gazebo-and-unity-comparation&quot;&gt;&lt;a href=&quot;#gazebo-and-unity-comparation&quot; class=&quot;headerlink&quot; title=&quot;gazebo and unity comparation&quot;&gt;&lt;/a&gt;ga
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://num-github.github.io/2019/07/25/Note%20for%20RPN/"/>
    <id>https://num-github.github.io/2019/07/25/Note for RPN/</id>
    <published>2019-07-25T15:38:15.002Z</published>
    <updated>2019-07-26T22:06:08.240Z</updated>
    
    <content type="html"><![CDATA[<h4 id="Note-for-RPN"><a href="#Note-for-RPN" class="headerlink" title="Note for RPN"></a>Note for RPN</h4><p>For bounding boxes with a scale and ratios </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">scale = <span class="number">4</span></span><br><span class="line">ratio = <span class="number">2</span></span><br><span class="line">x = scale * np.sqrt(ratio)</span><br><span class="line">y = scale / np.sqrt(ratio)</span><br></pre></td></tr></table></figure><p>Anchor generation in SiamMask:</p><p>In SiamMask, the anchors was generated in anchors.py, before we goes into the real anchor generation code, let’s firstly have a look about the parameters defined in <strong>init</strong> function</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Anchors</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.stride = <span class="number">8</span></span><br><span class="line">        self.ratios = [<span class="number">0.33</span>, <span class="number">0.5</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">        self.scales = [<span class="number">8</span>]</span><br><span class="line">        self.round_dight = <span class="number">0</span></span><br><span class="line">        self.image_center = <span class="number">0</span></span><br><span class="line">        self.size = <span class="number">0</span></span><br><span class="line">        self.anchor_density = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        self.anchor_num = len(self.scales) * len(self.ratios) * (self.anchor_density**<span class="number">2</span>)</span><br><span class="line">        self.anchors = <span class="literal">None</span>  <span class="comment"># in single position (anchor_num*4)</span></span><br><span class="line">        self.all_anchors = <span class="literal">None</span>  <span class="comment"># in all position 2*(4*anchor_num*h*w)</span></span><br><span class="line">        self.generate_anchors()</span><br></pre></td></tr></table></figure><p>The true cod from anchor generation is like below. If we put the value above into the code, we can clearly see the process of it which generate anchor with different ratio</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_anchors</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.anchors = np.zeros((self.anchor_num, <span class="number">4</span>), dtype=np.float32)</span><br><span class="line">        size = self.stride * self.stride</span><br><span class="line">        count = <span class="number">0</span></span><br><span class="line">        anchors_offset = self.stride / self.anchor_density <span class="comment">#=8</span></span><br><span class="line">        anchors_offset = np.arange(self.anchor_density)*anchors_offset</span><br><span class="line">        anchors_offset = anchors_offset - np.mean(anchors_offset)</span><br><span class="line"></span><br><span class="line">        x_offsets, y_offsets = np.meshgrid(anchors_offset, anchors_offset)</span><br><span class="line"><span class="comment">#Thus, only run outer for once </span></span><br><span class="line">        <span class="keyword">for</span> x_offset, y_offset <span class="keyword">in</span> zip(x_offsets.flatten(), y_offsets.flatten()):</span><br><span class="line">            <span class="keyword">for</span> r <span class="keyword">in</span> self.ratios:</span><br><span class="line">                <span class="keyword">if</span> self.round_dight &gt; <span class="number">0</span>:<span class="comment"># not run this if</span></span><br><span class="line">                    ws = round(math.sqrt(size*<span class="number">1.</span> / r), self.round_dight)</span><br><span class="line">                    hs = round(ws * r, self.round_dight)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="comment">#size = self.stride * self.stride</span></span><br><span class="line">                    ws = int(math.sqrt(size*<span class="number">1.</span> / r))</span><br><span class="line">                    hs = int(ws * r)</span><br><span class="line"></span><br><span class="line">                <span class="keyword">for</span> s <span class="keyword">in</span> self.scales:</span><br><span class="line">                    w = ws * s  <span class="comment">#=sqrt(stride * stride* / ratios)*scales</span></span><br><span class="line">                    <span class="comment">#=stride*scales/sqrt(ratios)</span></span><br><span class="line"></span><br><span class="line">                    h = hs * s<span class="comment">#=sqrt(stride * stride* ratios)*scales</span></span><br><span class="line">                    <span class="comment">#=stride*scales*sqrt(ratios)</span></span><br><span class="line">                    self.anchors[count][:] = [-w*<span class="number">0.5</span>+x_offset, -h*<span class="number">0.5</span>+y_offset, w*<span class="number">0.5</span>+x_offset, h*<span class="number">0.5</span>+y_offset][:]</span><br><span class="line">             <span class="comment">#array([[-52., -16.,  52.,  16.],</span></span><br><span class="line">       <span class="comment">#[-44., -20.,  44.,  20.],</span></span><br><span class="line">       <span class="comment">#[-32., -32.,  32.,  32.],</span></span><br><span class="line">       <span class="comment">#[-20., -40.,  20.,  40.],</span></span><br><span class="line">       <span class="comment">#[-16., -48.,  16.,  48.]], dtype=float32) </span></span><br><span class="line">             <span class="comment"># This is the offset for the four points of an anchor related to center points.</span></span><br><span class="line">                    count += <span class="number">1</span></span><br></pre></td></tr></table></figure><p>In data preparation stage, function generate_all_anchors has also be called</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">self.anchors.generate_all_anchors(im_c=self.search_size//<span class="number">2</span>, size=self.size)</span><br><span class="line"><span class="comment">#search_size is the length and height of search image</span></span><br><span class="line">    <span class="comment">#search_size//2 is the center of the image</span></span><br><span class="line">    <span class="comment">#size is the length and height of a feature map</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_all_anchors</span><span class="params">(self, im_c, size)</span>:</span></span><br><span class="line">    <span class="comment">#suppose search_size = 255 im_c=255//2</span></span><br><span class="line">    <span class="comment">#suppose size = 17, the parameter comes from your config.json file which represent the size of feature map  </span></span><br><span class="line">        <span class="keyword">if</span> self.image_center == im_c <span class="keyword">and</span> self.size == size:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        self.image_center = im_c</span><br><span class="line">        self.size = size</span><br><span class="line">        <span class="comment">#a0x = 63=255//2-17//2*8</span></span><br><span class="line">        a0x = im_c - size // <span class="number">2</span> * self.stride<span class="comment">#The center point of the first anchor</span></span><br><span class="line">        ori = np.array([a0x] * <span class="number">4</span>, dtype=np.float32)</span><br><span class="line">        <span class="comment">#ori = array([63., 63., 63., 63.], dtype=float32)</span></span><br><span class="line">        zero_anchors = self.anchors + ori<span class="comment">#The anchor points for the four point </span></span><br><span class="line"><span class="comment">#ori +self.anchors: array([[-52., -16.,  52.,  16.],</span></span><br><span class="line">                    <span class="comment">#        [-44., -20.,  44.,  20.],</span></span><br><span class="line">                    <span class="comment">#        [-32., -32.,  32.,  32.],</span></span><br><span class="line">                    <span class="comment">#        [-20., -40.,  20.,  40.],</span></span><br><span class="line">                    <span class="comment">#        [-16., -48.,  16.,  48.]], dtype=float32)</span></span><br><span class="line">        <span class="comment">#X1,y1,x2    ,y2  correspond to the value in the second dimension</span></span><br><span class="line">        <span class="comment">#          X1 ,  y1    ,x2    ,y2 </span></span><br><span class="line">        <span class="comment">#=array([[ 11.,  47., 115.,  79.],</span></span><br><span class="line">        <span class="comment">#       [ 19.,  43., 107.,  83.],</span></span><br><span class="line">        <span class="comment">#       [ 31.,  31.,  95.,  95.],</span></span><br><span class="line">        <span class="comment">#       [ 43.,  23.,  83., 103.],</span></span><br><span class="line">        <span class="comment">#       [ 47.,  15.,  79., 111.]], dtype=float32)</span></span><br><span class="line">        x1 = zero_anchors[:, <span class="number">0</span>]</span><br><span class="line">        y1 = zero_anchors[:, <span class="number">1</span>]</span><br><span class="line">        x2 = zero_anchors[:, <span class="number">2</span>]</span><br><span class="line">        y2 = zero_anchors[:, <span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment">#Add two more dimension to x1, y1, x2, y2 </span></span><br><span class="line">        <span class="comment">#x1=array([[[11.]],</span></span><br><span class="line">        <span class="comment">#[[19.]],</span></span><br><span class="line">        <span class="comment">#[[31.]],</span></span><br><span class="line">        <span class="comment">#[[43.]],</span></span><br><span class="line">        <span class="comment">#[[47.]]], dtype=float32)   .....</span></span><br><span class="line">        x1, y1, x2, y2 = map(<span class="keyword">lambda</span> x: x.reshape(self.anchor_num, <span class="number">1</span>, <span class="number">1</span>), [x1, y1, x2, y2])</span><br><span class="line">        cx, cy, w, h = corner2center([x1, y1, x2, y2])</span><br><span class="line">        <span class="comment">#e.g.</span></span><br><span class="line">        <span class="comment">#cx=array([[[63.]],                h=array([[[32.]],</span></span><br><span class="line">        <span class="comment">#[[63.]],    [[40.]],</span></span><br><span class="line">        <span class="comment">#[[63.]],[[64.]],</span></span><br><span class="line">        <span class="comment">#[[63.]], [[80.]],</span></span><br><span class="line">        <span class="comment">#[[63.]]], dtype=float32)[[96.]]], dtype=float32))</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#size = 17</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">#The ofset of each bounding boxes with respect to each </span></span><br><span class="line">        disp_x = np.arange(<span class="number">0</span>, size).reshape(<span class="number">1</span>, <span class="number">1</span>, <span class="number">-1</span>) * self.stride</span><br><span class="line">        disp_y = np.arange(<span class="number">0</span>, size).reshape(<span class="number">1</span>, <span class="number">-1</span>, <span class="number">1</span>) * self.stride</span><br><span class="line"><span class="comment">#disp_x=array([[[  0,   8,  16,  24,  32,  40,  48,  56,  64,  72,  80,  88,96, 104, 112, 120, 128]]])</span></span><br><span class="line"><span class="comment">#disp_y=array([[[  0], [  8], [ 16], [ 24], [ 32],  [ 40],[ 48], [ 56], [ 64],[ 72], [ 80],[ 88],[ 96],[104],[112], [120],[128]]])</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#cx.shape=(5, 1, 1)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Calculate the final location of each center points cx: x axis value</span></span><br><span class="line"><span class="comment">#cy: y axis value</span></span><br><span class="line">        cx = cx + disp_x</span><br><span class="line"> </span><br><span class="line"><span class="comment">#cx=array([[[ 63.,  71.,  79.,  87.,  95., 103., 111., 119., 127., 135.,143., 151., 159., 167., 175., 183., 191.]],</span></span><br><span class="line"><span class="comment"># [[ 63.,  71.,  79.,  87.,  95., 103., 111., 119., 127., 135., 143., 151., 159., 167., 175., 183., 191.]],</span></span><br><span class="line"><span class="comment">#[[ 63.,  71.,  79.,  87.,  95., 103., 111., 119., 127., 135.,143., 151., 159., 167., 175., 183., 191.]],</span></span><br><span class="line"><span class="comment"># [[ 63.,  71.,  79.,  87.,  95., 103., 111., 119., 127., 135.,  143., 151., 159., 167., 175., 183., 191.]],</span></span><br><span class="line"><span class="comment"># [[ 63.,  71.,  79.,  87.,  95., 103., 111., 119., 127., 135.,  143., 151., 159., 167., 175., 183., 191.]]])</span></span><br><span class="line"> </span><br><span class="line">        cy = cy + disp_y</span><br><span class="line"></span><br><span class="line">        <span class="comment"># broadcast</span></span><br><span class="line">        <span class="comment">#(5, 17, 17)</span></span><br><span class="line">        zero = np.zeros((self.anchor_num, size, size), dtype=np.float32)</span><br><span class="line">        <span class="comment">#convert cx from (17,17) to (5, 17, 17)</span></span><br><span class="line">        cx, cy, w, h = map(<span class="keyword">lambda</span> x: x + zero, [cx, cy, w, h])</span><br><span class="line">        x1, y1, x2, y2 = center2corner([cx, cy, w, h])</span><br><span class="line"><span class="comment">#(4, 5, 17, 17)</span></span><br><span class="line">        self.all_anchors = np.stack([x1, y1, x2, y2]), np.stack([cx, cy, w, h])</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;Note-for-RPN&quot;&gt;&lt;a href=&quot;#Note-for-RPN&quot; class=&quot;headerlink&quot; title=&quot;Note for RPN&quot;&gt;&lt;/a&gt;Note for RPN&lt;/h4&gt;&lt;p&gt;For bounding boxes with a scal
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://num-github.github.io/2019/07/21/Review%20of%20SiamMask/"/>
    <id>https://num-github.github.io/2019/07/21/Review of SiamMask/</id>
    <published>2019-07-21T16:52:35.170Z</published>
    <updated>2019-07-25T15:21:47.989Z</updated>
    
    <content type="html"><![CDATA[<h3 id="SiamMask-Note"><a href="#SiamMask-Note" class="headerlink" title="SiamMask Note"></a><a href="https://arxiv.org/pdf/1812.05050.pdf" target="_blank" rel="noopener">SiamMask</a> Note</h3><p><em>SiamMask is a model that is that I found very interesting recently. It is the model that can automatically mask an instance as long as an initial bounding boxes has been given you can find the code in github at <a href="https://github.com/foolwood/SiamMask" target="_blank" rel="noopener">here</a></em></p><p>In this article I will describe the development of siamMask based on its time line: from SiamFC, then SiamRPN, finally SiamMask</p><h4 id="SiamFC"><a href="#SiamFC" class="headerlink" title="SiamFC"></a>SiamFC</h4><p>SiamFC is the network for object tracking. The basic thinking for it is to continually compare the similarity between the first image and sliding windows from the last few images so as to generate a score map or <strong>RoW</strong> for similarity.</p><p><img src="/.io//C:%5CUsers%5Ca%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1563725602695.png" alt="1563725602695"></p><p>The mathematical expression for this SiamFC can be expressed as<br>$$<br>g_s(z,x)=f_e(z)*f_e(x)<br>$$<br>where z is the initial images, x is the next large images, $f_e(z) $ is embedding network and * simple cross-correlation. The training goal for this network is to maximize the score in score map to correspond to the target location is search area.</p><h5 id="What’s-improved-on-siamMask"><a href="#What’s-improved-on-siamMask" class="headerlink" title="What’s improved on siamMask"></a>What’s improved on siamMask</h5><p>For SiamMask, they improved the network architecture by output multi-channel score map rather than a single score map. Then, it replace simple cross-correlation with depth-wise cross-correlation which is a neural network. </p><h4 id="SiamMask"><a href="#SiamMask" class="headerlink" title="SiamMask"></a>SiamMask</h4><p>The architecture of SiamMask was shown blow</p><p><img src="/.io//C:%5CUsers%5Ca%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1563872260341.png" alt="1563872260341"></p><p>As you can see here, an mask and boxes part is added to the end of SiamMask. In this case, we just mainly talk about the mask part, the box and score is mainly proposed in SiamRPN.</p><p>More detailed architecture of it was shown in the next figure. The main <a href="https://github.com/foolwood/SiamMask" target="_blank" rel="noopener">code</a> is in custom.py. The code don’t have much comments, so I select some important code and add some comments for explanation in the following explanation.   </p><p><img src="/.io//C:%5CUsers%5Ca%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1563874125216.png" alt="1563874125216"></p><p>In the architecture above, the mask head was replaced by a refinement model which was inspired by <a href="https://arxiv.org/pdf/1603.08695.pdf" target="_blank" rel="noopener">this paper</a> which aim to refine the smaller mask to a bigger mask by using the features extracted from pervious convolution. </p><p>The whole architecture of it was written as shown below</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Custom</span><span class="params">(SiamMask)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, pretrain=False, **kwargs)</span>:</span></span><br><span class="line">        super(Custom, self).__init__(**kwargs)</span><br><span class="line">        <span class="comment">#tailored ResNet50+adjust layer</span></span><br><span class="line">        self.features = ResDown(pretrain=pretrain)</span><br><span class="line">        <span class="comment">#the parts for boundings boxes and classification (not discussed here, more detail are in SiamRPN)</span></span><br><span class="line">        self.rpn_model = UP(anchor_num=self.anchor_num, feature_in=<span class="number">256</span>, feature_out=<span class="number">256</span>)</span><br><span class="line">        <span class="comment">#Depth-wise correlation Maskcorr()&lt;-DepthCorr()</span></span><br><span class="line">        self.mask_model = MaskCorr()</span><br><span class="line">        <span class="comment">#Refinement part</span></span><br><span class="line">        self.refine_model = Refine()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">refine</span><span class="params">(self, f, pos=None)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.refine_model(f, pos)</span><br><span class="line">    </span><br><span class="line"><span class="comment">#Embedding for the initialization image (the smaller 127×127×3 image z) for target.</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">template</span><span class="params">(self, template)</span>:</span></span><br><span class="line">        self.zf = self.features(template)</span><br><span class="line">        </span><br><span class="line"><span class="comment">#For classification and bounding boxes</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">track</span><span class="params">(self, search)</span>:</span></span><br><span class="line">        search = self.features(search)</span><br><span class="line">        rpn_pred_cls, rpn_pred_loc = self.rpn(self.zf, search)</span><br><span class="line">        <span class="keyword">return</span> rpn_pred_cls, rpn_pred_loc</span><br><span class="line"></span><br><span class="line">    <span class="comment">#The main archtecture  </span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">track_mask</span><span class="params">(self, search)</span>:</span></span><br><span class="line">        <span class="comment"># self.feature is the output feature extracted from ResNet at each layer stage</span></span><br><span class="line">        <span class="comment">#self.search is the output of adjust layer</span></span><br><span class="line">        self.feature, self.search = self.features.forward_all(search)</span><br><span class="line">        <span class="comment">#the parts for boundings boxes and classification (not discussed here, more detail are in SiamRPN)</span></span><br><span class="line">        rpn_pred_cls, rpn_pred_loc = self.rpn(self.zf, self.search)</span><br><span class="line">        <span class="comment">#Initialized embedding and the embedding for search image feed into depth-wise correlation without calling the head for generating mask</span></span><br><span class="line">        self.corr_feature = self.mask_model.mask.forward_corr(self.zf, self.search)</span><br><span class="line">        <span class="comment"># mask output without refinement (call mathod 'head' in DepthCorr())</span></span><br><span class="line">        pred_mask = self.mask_model.mask.head(self.corr_feature)</span><br><span class="line">        <span class="keyword">return</span> rpn_pred_cls, rpn_pred_loc, pred_mask</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">track_refine</span><span class="params">(self, pos)</span>:</span></span><br><span class="line">        <span class="comment"># mask output with refinement</span></span><br><span class="line">        pred_mask = self.refine_model(self.feature, self.corr_feature, pos=pos, test=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> pred_mask</span><br></pre></td></tr></table></figure><p>The embedding network is two ResNet50 with only the first four layers and then added an adjust layer to adjust channels. The important code for these two part  is shown in below (unnecessary code was deleted).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResDown</span><span class="params">(MultiStageFeature)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, pretrain=False)</span>:</span></span><br><span class="line">        super(ResDown, self).__init__()</span><br><span class="line">        self.features = resnet50(layer3=<span class="literal">True</span>, layer4=<span class="literal">False</span>)</span><br><span class="line">        self.downsample = ResDownS(<span class="number">1024</span>, <span class="number">256</span>)</span><br><span class="line"><span class="comment">#In forward or forward_all, the tailored Resnet50 and adjust layer (downsample) was connected</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        output = self.features(x)</span><br><span class="line">        p3 = self.downsample(output[<span class="number">-1</span>])</span><br><span class="line">        <span class="keyword">return</span> p3 </span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward_all</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        output = self.features(x)</span><br><span class="line">        p3 = self.downsample(output[<span class="number">-1</span>])</span><br><span class="line">        <span class="keyword">return</span> output, p3  </span><br><span class="line"><span class="comment">#The 'output' is the features extracted by ResNet and can be used of mask refinement.</span></span><br><span class="line"><span class="comment">#p3 is the embeding that will be feed into depth-wise correlation.</span></span><br></pre></td></tr></table></figure><p>The ‘output’ is the features extracted by ResNet and can be used of mask refinement, which consists the 3 features for refinement and one features for adjustment. The forward output of its ResNet is 4 values from each stage of ResNet.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">       ........</span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">       x = self.conv1(x)</span><br><span class="line">       x = self.bn1(x)</span><br><span class="line">       p0 = self.relu(x)</span><br><span class="line">       x = self.maxpool(p0)</span><br><span class="line"></span><br><span class="line">       p1 = self.layer1(x)</span><br><span class="line">       p2 = self.layer2(p1)</span><br><span class="line">       p3 = self.layer3(p2)</span><br><span class="line"></span><br><span class="line">       <span class="keyword">return</span> p0, p1, p2, p3</span><br></pre></td></tr></table></figure><p>p3 is the embedding that will be feed into depth-wise correlation.</p><p>In adjustment layer, one 1×1 kernel convolutional layer was applied with batch normalization.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResDownS</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, inplane, outplane)</span>:</span></span><br><span class="line">        super(ResDownS, self).__init__()</span><br><span class="line">        self.downsample = nn.Sequential(</span><br><span class="line">                nn.Conv2d(inplane, outplane, kernel_size=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">                nn.BatchNorm2d(outplane))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.downsample(x)</span><br><span class="line">        <span class="keyword">if</span> x.size(<span class="number">3</span>) &lt; <span class="number">20</span>:</span><br><span class="line">            l = <span class="number">4</span></span><br><span class="line">            r = <span class="number">-4</span></span><br><span class="line">            x = x[:, :, l:r, l:r]</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p>Depth-wise correlation is still achieved with the architecture as below (the parameters of in_channels, hidden, out_channels are 256, 256, 63*63)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MaskCorr</span><span class="params">(Mask)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, oSz=<span class="number">63</span>)</span>:</span></span><br><span class="line">        super(MaskCorr, self).__init__()</span><br><span class="line">        self.oSz = oSz</span><br><span class="line">        self.mask = DepthCorr(<span class="number">256</span>, <span class="number">256</span>, self.oSz**<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, z, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.mask(z, x)</span><br></pre></td></tr></table></figure><p>DepthCorr called by  MaskCorr.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d_dw_group</span><span class="params">(x, kernel)</span>:</span></span><br><span class="line">    batch, channel = kernel.shape[:<span class="number">2</span>]</span><br><span class="line">    x = x.view(<span class="number">1</span>, batch*channel, x.size(<span class="number">2</span>), x.size(<span class="number">3</span>))  <span class="comment"># 1 * (b*c) * k * k</span></span><br><span class="line">    kernel = kernel.view(batch*channel, <span class="number">1</span>, kernel.size(<span class="number">2</span>), kernel.size(<span class="number">3</span>))  <span class="comment"># (b*c) * 1 * H * W</span></span><br><span class="line">    out = F.conv2d(x, kernel, groups=batch*channel)</span><br><span class="line">    out = out.view(batch, channel, out.size(<span class="number">2</span>), out.size(<span class="number">3</span>))</span><br><span class="line">    <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DepthCorr</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_channels, hidden, out_channels, kernel_size=<span class="number">3</span>)</span>:</span></span><br><span class="line">        super(DepthCorr, self).__init__()</span><br><span class="line">        <span class="comment"># adjust layer for asymmetrical features</span></span><br><span class="line">        self.conv_kernel = nn.Sequential(</span><br><span class="line">                nn.Conv2d(in_channels, hidden, kernel_size=kernel_size, bias=<span class="literal">False</span>),</span><br><span class="line">                nn.BatchNorm2d(hidden),</span><br><span class="line">                nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">                )</span><br><span class="line">        self.conv_search = nn.Sequential(</span><br><span class="line">                nn.Conv2d(in_channels, hidden, kernel_size=kernel_size, bias=<span class="literal">False</span>),</span><br><span class="line">                nn.BatchNorm2d(hidden),</span><br><span class="line">                nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">                )</span><br><span class="line"><span class="comment">#mask head</span></span><br><span class="line">        self.head = nn.Sequential(</span><br><span class="line">                nn.Conv2d(hidden, hidden, kernel_size=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">                nn.BatchNorm2d(hidden),</span><br><span class="line">                nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">                nn.Conv2d(hidden, out_channels, kernel_size=<span class="number">1</span>)</span><br><span class="line">                )</span><br><span class="line"><span class="comment">#output without adding mask head</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward_corr</span><span class="params">(self, kernel, input)</span>:</span></span><br><span class="line">        kernel = self.conv_kernel(kernel)</span><br><span class="line">        input = self.conv_search(input)</span><br><span class="line">        feature = conv2d_dw_group(input, kernel)</span><br><span class="line">        <span class="keyword">return</span> feature</span><br><span class="line"><span class="comment">#output with mask head</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, kernel, search)</span>:</span></span><br><span class="line">        feature = self.forward_corr(kernel, search)</span><br><span class="line">        out = self.head(feature)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure><p>In Refinement stage, you can clearly see the structure: deconvolute the output from depth-wise convolution and the three stage refinement in last few lines of forward method.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Refine</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Refine, self).__init__()</span><br><span class="line">        self.v0 = nn.Sequential(nn.Conv2d(<span class="number">64</span>, <span class="number">16</span>, <span class="number">3</span>, padding=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">                           nn.Conv2d(<span class="number">16</span>, <span class="number">4</span>, <span class="number">3</span>, padding=<span class="number">1</span>),nn.ReLU())</span><br><span class="line"></span><br><span class="line">        self.v1 = nn.Sequential(nn.Conv2d(<span class="number">256</span>, <span class="number">64</span>, <span class="number">3</span>, padding=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">                           nn.Conv2d(<span class="number">64</span>, <span class="number">16</span>, <span class="number">3</span>, padding=<span class="number">1</span>), nn.ReLU())</span><br><span class="line"></span><br><span class="line">        self.v2 = nn.Sequential(nn.Conv2d(<span class="number">512</span>, <span class="number">128</span>, <span class="number">3</span>, padding=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">                           nn.Conv2d(<span class="number">128</span>, <span class="number">32</span>, <span class="number">3</span>, padding=<span class="number">1</span>), nn.ReLU())</span><br><span class="line"></span><br><span class="line">        self.h2 = nn.Sequential(nn.Conv2d(<span class="number">32</span>, <span class="number">32</span>, <span class="number">3</span>, padding=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">                           nn.Conv2d(<span class="number">32</span>, <span class="number">32</span>, <span class="number">3</span>, padding=<span class="number">1</span>), nn.ReLU())</span><br><span class="line"></span><br><span class="line">        self.h1 = nn.Sequential(nn.Conv2d(<span class="number">16</span>, <span class="number">16</span>, <span class="number">3</span>, padding=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">                           nn.Conv2d(<span class="number">16</span>, <span class="number">16</span>, <span class="number">3</span>, padding=<span class="number">1</span>), nn.ReLU())</span><br><span class="line"></span><br><span class="line">        self.h0 = nn.Sequential(nn.Conv2d(<span class="number">4</span>, <span class="number">4</span>, <span class="number">3</span>, padding=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">                           nn.Conv2d(<span class="number">4</span>, <span class="number">4</span>, <span class="number">3</span>, padding=<span class="number">1</span>), nn.ReLU())</span><br><span class="line"></span><br><span class="line">        self.deconv = nn.ConvTranspose2d(<span class="number">256</span>, <span class="number">32</span>, <span class="number">15</span>, <span class="number">15</span>)</span><br><span class="line"></span><br><span class="line">        self.post0 = nn.Conv2d(<span class="number">32</span>, <span class="number">16</span>, <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.post1 = nn.Conv2d(<span class="number">16</span>, <span class="number">4</span>, <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.post2 = nn.Conv2d(<span class="number">4</span>, <span class="number">1</span>, <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> modules <span class="keyword">in</span> [self.v0, self.v1, self.v2, self.h2, self.h1, self.h0, self.deconv, self.post0, self.post1, self.post2,]:</span><br><span class="line">            <span class="keyword">for</span> l <span class="keyword">in</span> modules.modules():</span><br><span class="line">                <span class="keyword">if</span> isinstance(l, nn.Conv2d):</span><br><span class="line">                    nn.init.kaiming_uniform_(l.weight, a=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, f, corr_feature, pos=None, test=False)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> test:</span><br><span class="line">            p0 = torch.nn.functional.pad(f[<span class="number">0</span>], [<span class="number">16</span>, <span class="number">16</span>, <span class="number">16</span>, <span class="number">16</span>])[:, :, <span class="number">4</span>*pos[<span class="number">0</span>]:<span class="number">4</span>*pos[<span class="number">0</span>]+<span class="number">61</span>, <span class="number">4</span>*pos[<span class="number">1</span>]:<span class="number">4</span>*pos[<span class="number">1</span>]+<span class="number">61</span>]</span><br><span class="line">            p1 = torch.nn.functional.pad(f[<span class="number">1</span>], [<span class="number">8</span>, <span class="number">8</span>, <span class="number">8</span>, <span class="number">8</span>])[:, :, <span class="number">2</span> * pos[<span class="number">0</span>]:<span class="number">2</span> * pos[<span class="number">0</span>] + <span class="number">31</span>, <span class="number">2</span> * pos[<span class="number">1</span>]:<span class="number">2</span> * pos[<span class="number">1</span>] + <span class="number">31</span>]</span><br><span class="line">            p2 = torch.nn.functional.pad(f[<span class="number">2</span>], [<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>])[:, :, pos[<span class="number">0</span>]:pos[<span class="number">0</span>] + <span class="number">15</span>, pos[<span class="number">1</span>]:pos[<span class="number">1</span>] + <span class="number">15</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            p0 = F.unfold(f[<span class="number">0</span>], (<span class="number">61</span>, <span class="number">61</span>), padding=<span class="number">0</span>, stride=<span class="number">4</span>).permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>).contiguous().view(<span class="number">-1</span>, <span class="number">64</span>, <span class="number">61</span>, <span class="number">61</span>)</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> (pos <span class="keyword">is</span> <span class="literal">None</span>): p0 = torch.index_select(p0, <span class="number">0</span>, pos)</span><br><span class="line">            p1 = F.unfold(f[<span class="number">1</span>], (<span class="number">31</span>, <span class="number">31</span>), padding=<span class="number">0</span>, stride=<span class="number">2</span>).permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>).contiguous().view(<span class="number">-1</span>, <span class="number">256</span>, <span class="number">31</span>, <span class="number">31</span>)</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> (pos <span class="keyword">is</span> <span class="literal">None</span>): p1 = torch.index_select(p1, <span class="number">0</span>, pos)</span><br><span class="line">            p2 = F.unfold(f[<span class="number">2</span>], (<span class="number">15</span>, <span class="number">15</span>), padding=<span class="number">0</span>, stride=<span class="number">1</span>).permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>).contiguous().view(<span class="number">-1</span>, <span class="number">512</span>, <span class="number">15</span>, <span class="number">15</span>)</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> (pos <span class="keyword">is</span> <span class="literal">None</span>): p2 = torch.index_select(p2, <span class="number">0</span>, pos)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span>(pos <span class="keyword">is</span> <span class="literal">None</span>):</span><br><span class="line">            p3 = corr_feature[:, :, pos[<span class="number">0</span>], pos[<span class="number">1</span>]].view(<span class="number">-1</span>, <span class="number">256</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            p3 = corr_feature.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>).contiguous().view(<span class="number">-1</span>, <span class="number">256</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">##################The structure of refinement. ##################################</span></span><br><span class="line">        out = self.deconv(p3)</span><br><span class="line">        out = self.post0(F.upsample(self.h2(out) + self.v2(p2), size=(<span class="number">31</span>, <span class="number">31</span>)))</span><br><span class="line">        out = self.post1(F.upsample(self.h1(out) + self.v1(p1), size=(<span class="number">61</span>, <span class="number">61</span>)))</span><br><span class="line">        out = self.post2(F.upsample(self.h0(out) + self.v0(p0), size=(<span class="number">127</span>, <span class="number">127</span>)))</span><br><span class="line">        out = out.view(<span class="number">-1</span>, <span class="number">127</span>*<span class="number">127</span>)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure><p>Loss function</p><h4 id="SiamRPN"><a href="#SiamRPN" class="headerlink" title="SiamRPN"></a>SiamRPN</h4><p>Ok, after talking about SiamMask, let’s move on to the parts about classification and bounding box predication. The original figure of siamRPN is the figure shown below, although the later version siamRPN++ has some modified version of it, but I think the following diagram is clear enough to show the core thinking of it. </p><p><img src="/.io//C:%5CUsers%5Ca%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1563889858564.png" alt="1563889858564"></p><p>Basically, what SiamRPN doing in SiamMask input the two features from a network, in this case, ResNet,  and pass them to classification block and regression block. Then, in these two block, the  the depth-wise correlation will be calculated and gives a series of classification groups and regression groups to determine the bounding boxes. </p><h5 id="Some-problems-that-I-thought"><a href="#Some-problems-that-I-thought" class="headerlink" title="Some problems that I thought:"></a>Some problems that I thought:</h5><p>This model don’t have the function for memory, I think it would be better to add some technique like RNN to update the embedding. </p><p>It only initialize the image but not mask, so the quality of tracking high affected by the first image and in some cases, you cannot initialize an object that with a rectangles.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;SiamMask-Note&quot;&gt;&lt;a href=&quot;#SiamMask-Note&quot; class=&quot;headerlink&quot; title=&quot;SiamMask Note&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://arxiv.org/pdf/1812.05050.pdf&quot; 
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://num-github.github.io/2019/07/13/Investigation-for-Reinforcement-Learning-in-robotic/"/>
    <id>https://num-github.github.io/2019/07/13/Investigation-for-Reinforcement-Learning-in-robotic/</id>
    <published>2019-07-13T12:36:39.756Z</published>
    <updated>2019-07-31T15:51:52.133Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Investigation-for-Reinforcement-Learning-RL-in-robotic"><a href="#Investigation-for-Reinforcement-Learning-RL-in-robotic" class="headerlink" title="Investigation for Reinforcement Learning (RL) in robotic"></a>Investigation for Reinforcement Learning (RL) in robotic</h1><p><em>Recently, I was asked to do some research on robot. The main task for that is to tidy up clothes. My first impression  on that task is this is very similar to pick and place tasks in has been talked about in many domes that I had watched before, so I think that might be a good point to start and investigate it from what I had know.</em></p><p>I think, this article probably useful for beginners who wants to apply reinforcement learning into their project but have a little (like me) or no knowledge about reinforcement learning. </p><p>This article also include many links that require you to set up some environment. I know that this is a very tedious and annoying work, but it would be better to do that if you hope to implement RL to your project, because I think take some practice is the best way to learn. </p><h3 id="Useful-links-for-beginners"><a href="#Useful-links-for-beginners" class="headerlink" title="Useful links for beginners:"></a>Useful links for beginners:</h3><p>The following links are some useful resources that is very useful and interesting and what I discussed here is also comes from the following links: </p><p> Tools:</p><p>​             <a href="https://stable-baselines.readthedocs.io/en/master/modules/ddpg.html" target="_blank" rel="noopener">OpenAI stable baselines</a> </p><p>​            <a href="https://github.com/openai/baselines" target="_blank" rel="noopener">OpenAI baseline</a></p><p>​            <a href="http://gym.openai.com/envs/#robotics" target="_blank" rel="noopener">OpenAI gym (robot) </a></p><p>Learning: </p><p>​            <a href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html" target="_blank" rel="noopener">Spinning up</a></p><p>​            <a href="https://www.youtube.com/watch?v=Wypc1a-1ZYA" target="_blank" rel="noopener">Matlab Walking Robot Problem</a> </p><p>​            <a href="https://github.com/sarcturus00/Tidy-Reinforcement-learning" target="_blank" rel="noopener">Tidy-reinforcement learning</a></p><p>​            <a href="https://www.bilibili.com/video/av24724071?from=search&seid=14445377713604973899" target="_blank" rel="noopener">“Deep Reinforcement Learning, 2018” by Hongyi Li (Chinese version)</a></p><h5 id="Before-you-read-ensure-that-you-have-some-basic-knowledge-of-reinforcement-learning-You-can-read-though-the-tutorial-in-spinning-up-of-Key-Concepts-in-RL"><a href="#Before-you-read-ensure-that-you-have-some-basic-knowledge-of-reinforcement-learning-You-can-read-though-the-tutorial-in-spinning-up-of-Key-Concepts-in-RL" class="headerlink" title="Before you read, ensure that you have some basic knowledge of reinforcement learning. You can read though the tutorial in spinning up of Key Concepts in RL."></a>Before you read, ensure that you have some basic knowledge of reinforcement learning. You can read though the tutorial in <a href="https://spinningup.openai.com/en/latest/spinningup/rl_intro.html" target="_blank" rel="noopener">spinning up</a> of Key Concepts in RL.</h5><h3 id="Play-with-environment"><a href="#Play-with-environment" class="headerlink" title="Play with environment"></a>Play with environment</h3><p>The first algorithm that found useful is <a href="https://openai.com/blog/ingredients-for-robotics-research/" target="_blank" rel="noopener">Hindsight Experience Replay (HER) + Deep Deterministic Policy Gradient (DDPG)</a>. The ‘+’ here means it is the algorithm that apply HER to improve the performance of DDPG on this task. </p><p>Before we discussing the algorithm in detail, let’s have a look what’s that tasks that this algorithm has been solved (The following are just part of the summary of the tasks, you can find more detail on <a href="https://arxiv.org/pdf/1802.09464.pdf" target="_blank" rel="noopener">this</a>  or watch the <a href="https://sites.google.com/site/hindsightexperiencereplay/" target="_blank" rel="noopener">demo</a> that may give you some intuition on that). </p><p>Basically, the aim of the simulation is to move the gripper of a robot to achieve the certain tasks: FetchReach, FetchPush, FetchSlide  and FetchPickAndPlace. </p><p>There are 4 parameters for actions for gripper: 3 to specify how the gripper will move in Cartesian coordinates  and one to specify the  open and close of the gripper. </p><p>The reward for that is 0 if the goal is achieved (e.g. in pick and place tasks,  once the robot move the cube to desired location, the reward would be 0 or otherwise would be 1).  </p><p>The observations include the Cartesian position of the gripper, its linear velocity as well as the position and linear velocity of the robot’s gripper. If an object is present, we also include the object’s Cartesian position and rotation using Euler angles, its linear and angular velocities, as well as its position and linear velocities relative to gripper. </p><p><img src="/.io//robot.png" alt="robot"></p><p>You can run the following code and modify the array of actions to see if there are changes occur, once your had set up the environment for OpenAI gym-robot (You need to install MuJoCo).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gym</span><br><span class="line">env = gym.make(<span class="string">"FetchPickAndPlace-v1"</span>)</span><br><span class="line">observation = env.reset()</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">  env.render()</span><br><span class="line">  action = env.action_space.sample()</span><br><span class="line">  print( action )</span><br><span class="line">  observation, reward, done, info = env.step(action)</span><br><span class="line">  print(  observation, reward, done, info )</span><br><span class="line">  <span class="keyword">if</span> done:</span><br><span class="line">    observation = env.reset()</span><br><span class="line">env.close()</span><br></pre></td></tr></table></figure><p>After that you can following the guide of <a href="https://github.com/openai/baselines/tree/master/baselines/her" target="_blank" rel="noopener">baseline-HER</a> to train HER model in this environment so as to get an intuition of this algorithm or modify the code a little bit in baseline demo to apply it to your project (change from OpenAI-gym environment to your own environment (e.g. ROS) ). </p><h3 id="Link-with-openAI-with-ROS-If-you-don’t-use-ROS-in-your-project-you-can-skip-this-part"><a href="#Link-with-openAI-with-ROS-If-you-don’t-use-ROS-in-your-project-you-can-skip-this-part" class="headerlink" title="Link with openAI with ROS (If you don’t use ROS in your project, you can skip this part)"></a>Link with openAI with ROS (If you don’t use ROS in your project, you can skip this part)</h3><p>In order to do that your can following the guide of <a href="http://wiki.ros.org/openai_ros" target="_blank" rel="noopener">OpenAI_ROS</a>. In summary, what the OpenAI_ROS want you to do is to specify the following three things:</p><p><strong>Task environment</strong>:  the task that the robot has to learn (<em>e.g.</em> actions, reward functions, observations).</p><p><strong>Robot environment</strong>: the robot to use on the task (<em>e.g.</em> contain all the ROS functionalities that your robot will need in order to be controlled and checks that every ROS stuff required is up and running on the robot (topics, services …).)</p><p><strong>Training script</strong>: to set up the learning algorithm that you want to use in order to make your agent learn and select the task and robot to be used.</p><h3 id="Algorithms"><a href="#Algorithms" class="headerlink" title="Algorithms"></a>Algorithms</h3><p>In a typical robot control problems for reinforcement learning, the algorithms must be able to handle the issue of continuous actions and sparse reward problems. In the algorithm of HER+DDPG, DDPG aim to provide high performance algorithms for continuous actions and HER used to solve sparse reward problems. </p><h5 id="DDPG"><a href="#DDPG" class="headerlink" title="DDPG:"></a>DDPG:</h5><p>DDPG algorithm was developed from actor critic, which also combined policy based algorithm and value based algorithm. </p><h6 id="value-based-algorithm"><a href="#value-based-algorithm" class="headerlink" title="value  based algorithm"></a>value  based algorithm</h6><p>Let’s begin with the value based algorithm, which is <strong>Q-learning</strong> in this case. </p><p>The aim of an reinforcement learning algorithm is to maximize the reward from environment. For Q-learning, this is achieved by Q-table, which that map the reward predications relate to actions and states.  For example, if you want to train a robot that can only move forward (action 1) or backward (action 2) and  3 state (s1, s2, s3), each combinations of the states or actions will have a value (Q-value or predicted reward) for making decision, this value will be continuously updated during training based on the feedback from environment for each action and state. The optimal decision making strategy or policy is usually achieved by picking up the action that have the maximum Q value.</p><p>Thus, if you want a robot to reach a goal, go state 3 as quick as possible, for example. The robot will try an action each time based on your decision-making strategy and observe the current state and reward from environment; then, based on these to update Q table.</p><p><img src="/.io//1563039827434.png" alt="1563039827434"></p><p>The image above shows the Q-learning algorithm and how to update Q table in more detail, but I won’t talk more about it, what I want to mention is that Q table is used for mapping state and actions to Q-values, and our decision policy will make decisions based on Q-values.</p><p>The improved version for Q-learning is <strong>DQN</strong>, which replace the Q-table with a neural network. Such neural are able to handle much more state than pure Q-table. Similar to the neural network for regression problems, the neural network here regress the score of reward which is<br>$$<br>Reward+\gamma max(Q(state_{Next},action)<br>$$<br>Where γ is discount factor. If Q is accurate enough, this<br>$$<br>max(Q(state_{Next},action)<br>$$<br>can be viewed as  the score in the future, so the γ here is used to reduce the importance of the effect of future and the value of γ means to what extent that we hope to ignore the score of future. </p><h6 id="tips-for-train-a-neural-network-in-RL"><a href="#tips-for-train-a-neural-network-in-RL" class="headerlink" title="tips for train a neural network in RL"></a>tips for train a neural network in RL</h6><p>Another two commonly used trick which can is experience replay and target network. In my view, these two technique both trying to make the neural network training more stable by make the parameters updating process more like unsupervised learning. </p><p>Experience replay is to firstly sample reward, action, state, observation, etc, and then store them in a buffer.  And then train them together. </p><p>Target network is use two network, one for parameter updating and the other for providing Q. Once the training has processed for some times, copy the parameters of updating network to the Q network. You can read more about Q-learning and DQN in <a href="https://medium.com/@jonathan_hui/rl-dqn-deep-q-network-e207751f7ae4" target="_blank" rel="noopener">here</a></p><p>Apart from that, there is also many other tricks for improving  DQN, but the effect of these tricks can be shown on these  two  images and you can find more information on that in <a href="https://arxiv.org/pdf/1710.02298.pdf" target="_blank" rel="noopener">this paper</a>.</p><p><img src="/.io//D:%5Cblog%5Csource_posts%5Cchrome_2019-07-13_21-44-40.png" alt="chrome_2019-07-13_21-44-40"></p><p><img src="/.io//chrome_2019-07-13_21-41-24.png" alt="chrome_2019-07-13_21-41-24">{:height=”50%” width=”50%”}</p><h6 id="policy-based-algorithm"><a href="#policy-based-algorithm" class="headerlink" title="policy based algorithm"></a>policy based algorithm</h6><p>The simplest algorithm for policy based algorithm is policy gradient. Intuitively, this is the algorithm that aim to maximize the probability of the taking actions that give the maximum reward, which is achieved by the method of gradient ascent. You can see more about it in <a href="https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html" target="_blank" rel="noopener">here</a></p><p>The input of it is the observation, the output is the probability of each action. The mapping between observation and actions can be achieved by a neural network. </p><p>For deterministic policy gradient, it do not output the probability of each action, but the the exact action that it will take.</p><h6 id="DDPG-1"><a href="#DDPG-1" class="headerlink" title="DDPG"></a>DDPG</h6><p>Ok, now, you should get sense about what is Q network, policy network, and some tips for training a network in RL algorithm, in this section, let’s put them all together to form DDPG.</p><p>One of the problem for value based algorithm is it cannot handle continuous action space very in a very easy way, but policy based algorithm can achieve that goal. The problem for policy network is the efficiency of its updating is slow, so introduced Q network will speed up this process.   Actually, the basic thinking for this algorithm is to combine the benefits of the two types of algorithms, that is, the Q networks try to learn the reward of actions, the policy networks try to learn how to give actions that maximize the predicted reward (Q value). </p><p><img src="/.io//POWERPNT_2019-07-14_12-55-47.png" alt="POWERPNT_2019-07-14_12-55-47"></p><p>With such thinking and some tricks mentioned before like experience replay and target network, the final version of DDPG comes up. You may find different version of DDPG pseudo code, but they all consists of the tricks and the basic thinking. For example, the following pseudo code comes from <a href="https://github.com/sarcturus00/Tidy-Reinforcement-learning" target="_blank" rel="noopener">Tidy-Reinforcement-learning</a>, you can find other RL algorithm in here, as well.</p><p><img src="/.io//POWERPNT_2019-07-14_14-05-32.png" alt="POWERPNT_2019-07-14_14-05-32"></p><p>The blue box represents the basic thinking that Q network and policy network try to minimized. Red boxes is the tricks about reward and target network, and the yellow one is anther trick that add some noise to action to increase the ability of exploration of the algorithm. You can find more detail information and another version of DDPG at <a href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html" target="_blank" rel="noopener">here</a> which also add clip to it.  </p><h5 id="HER"><a href="#HER" class="headerlink" title="HER"></a>HER</h5><p>For now, DDPG has solved the problem of learning for continuous actions space, the next step is to deal with sparse reward problem. </p><p>In general, in my view, HER is the algorithm that make a very sparse reward problem which only give reward at certain state do not extremely sparse by directly give some reward to some of other states.</p><p>Or you can understand it in a way that <a href="https://arxiv.org/pdf/1707.01495.pdf" target="_blank" rel="noopener">HER paper</a> has mentioned: “after experiencing some episode s0, s1, . . . , sT we store in the replay buffer every transition st → st+1 not only with the original goal used for this episode but also with a subset of other goals. Notice that the goal being pursued influences the agent’s actions but not the environment dynamics and therefore we can replay each trajectory with an arbitrary goal assuming that we use an off-policy RL algorithms.” </p><p>The following is the pseudo code is the complete algorithm from HER paper and blue box is the the HER part.</p><p><img src="/.io//HER.png" alt="HER"></p><p>If you feeling that this expression is still abstract. The following code from <a href="https://github.com/sarcturus00/Tidy-Reinforcement-learning" target="_blank" rel="noopener">Tidy-Reinforcement-learning</a> clearly demonstrate what has mentioned before, where is future_k is the hyperparameter that specify how many additional state will be assign a reward (in this case, reward 0 means achieve the goal, -1 means haven’t achieve the goal). </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> HER:</span><br><span class="line">    <span class="keyword">for</span> h <span class="keyword">in</span> range(future_k):</span><br><span class="line">        future = np.random.randint(t, nstep)</span><br><span class="line">        goal_ = episode_experience[future][<span class="number">3</span>]</span><br><span class="line">        new_inputs0 = np.concatenate([obs0, goal_], axis=<span class="number">-1</span>)</span><br><span class="line">        new_inputs1 = np.concatenate([obs1, goal_], axis=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (np.array(obs1) == np.array(goal_)).all():</span><br><span class="line">            r_ = <span class="number">0.0</span> <span class="comment">#add reward to additional state.</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            r_ = <span class="number">-1.0</span></span><br><span class="line">            agent.memory.store_transition(new_inputs0, act, r_, new_inputs1,done)</span><br></pre></td></tr></table></figure><p>OK, we had finished the description for DDPG and HER algorithms</p><h3 id="Other-tips-and-information-that-maybe-helpful"><a href="#Other-tips-and-information-that-maybe-helpful" class="headerlink" title="Other tips and information that maybe helpful"></a>Other tips and information that maybe helpful</h3><p>For now, we had talked about DDPG+HER that can be applied to a robot control project and you may can test the algorithm for now, but there are also many other information that I find very interesting. </p><h5 id="Curriculum-learning-in-RL"><a href="#Curriculum-learning-in-RL" class="headerlink" title="Curriculum learning in RL"></a>Curriculum learning in RL</h5><p>This is the concept that make the robot to start learning from simple tasks and then move on to more complex tasks. For example, if you want your robot to learn how to pick up a ring and set it to a bar. You can firstly teach the robot to hold a ring in the position that is close to the bar to set it to a bar, then hold a ring in the position that is far away from the bar to set it to a bar and so on. Finally, teach the robot to learning the task in the final lesson. Such curriculum can be generated reversely, start from final steps of the task to the first step of the task. </p><h5 id="How-to-add-additional-command-to-your-robot"><a href="#How-to-add-additional-command-to-your-robot" class="headerlink" title="How to add additional command to your robot"></a>How to add additional command to your robot</h5><p>This is the <a href="https://www.youtube.com/watch?v=Wypc1a-1ZYA&list=PLn8PRpmsu08qw_IwpgVNsKiJQpvvW0MmM&index=4" target="_blank" rel="noopener">video</a> from Matlab. From 8:50, they start to discuss this issue. In short, it was achieved by adding additional command to robot and modify reward about the command. </p><h5 id="Some-problems-about-RL-in-robot-control-and-how-to-reduce-their-impact"><a href="#Some-problems-about-RL-in-robot-control-and-how-to-reduce-their-impact" class="headerlink" title="Some problems about RL in robot control and how to reduce their impact"></a>Some problems about RL in robot control and how to reduce their impact</h5><p>Also the <a href="https://www.youtube.com/watch?v=zHV3UcH-nr0&list=PLn8PRpmsu08qw_IwpgVNsKiJQpvvW0MmM&index=5" target="_blank" rel="noopener">video</a> from Matlab. In fact, these are also the problems about neural network. </p><h3 id="Imitation-learning"><a href="#Imitation-learning" class="headerlink" title="Imitation learning"></a>Imitation learning</h3>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Investigation-for-Reinforcement-Learning-RL-in-robotic&quot;&gt;&lt;a href=&quot;#Investigation-for-Reinforcement-Learning-RL-in-robotic&quot; class=&quot;hea
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://num-github.github.io/2019/06/30/Problems-for-using-gpu-for-Tensorflow-or-Keras-network-on-linux/"/>
    <id>https://num-github.github.io/2019/06/30/Problems-for-using-gpu-for-Tensorflow-or-Keras-network-on-linux/</id>
    <published>2019-06-30T12:45:06.155Z</published>
    <updated>2019-06-30T20:22:41.809Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Problems-for-using-gpu-for-Tensorflow-or-Keras-network-on-linux-tensorflow-can-only-see-CPU-Could-not-dlopen-library-‘libcuda-so-1’-failed-call-to-cuInit-UNKNOWN-ERROR-303"><a href="#Problems-for-using-gpu-for-Tensorflow-or-Keras-network-on-linux-tensorflow-can-only-see-CPU-Could-not-dlopen-library-‘libcuda-so-1’-failed-call-to-cuInit-UNKNOWN-ERROR-303" class="headerlink" title="Problems for using gpu for Tensorflow or Keras network on linux: tensorflow can only see CPU: Could not dlopen library ‘libcuda.so.1’;  failed call to cuInit: UNKNOWN ERROR (303);"></a>Problems for using gpu for Tensorflow or Keras network on linux: tensorflow can only see CPU: Could not dlopen library ‘libcuda.so.1’;  failed call to cuInit: UNKNOWN ERROR (303);</h1><p>The problem that I meet here is that when using an GPU cluster, my code ‘’with tf.device(‘/gpu:0’):’’ cannot run. If you meet the same problem, you can firstly run the following code:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">'CUDA_VISIBLE_DEVICES'</span>] = <span class="string">"0"</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.python.client <span class="keyword">import</span> device_lib</span><br><span class="line">print(device_lib.list_local_devices())</span><br></pre></td></tr></table></figure><p>If the code above cannot show any information about your GPU, then you need to check the cuda and cudnn that you installed. The version must meet your tensorflow version: <a href="https://www.tensorflow.org/install/source#tested_build_configurations" target="_blank" rel="noopener">https://www.tensorflow.org/install/source#tested_build_configurations</a></p><p>If you don’t have cude or cudnn, you can following the steps of AndrewPt in <a href="https://stackoverflow.com/questions/41402409/tensorflow-doesnt-seem-to-see-my-gpu" target="_blank" rel="noopener">https://stackoverflow.com/questions/41402409/tensorflow-doesnt-seem-to-see-my-gpu</a> to install them (but it is windows, be careful for you linux, you need to down different documents)</p><p>Then check whether you LD_LIBRARY_PATH contain the path for right cuda path, the path mush contain ‘libcuda.so.1’ ( for me it is in ‘’/home/.usr/local/cuda-10.0/lib64/stubs’), </p><p>and  ‘libcuda.so.XX.X (XX.X is your cuda version for me it is in /home/.usr/local/cuda-10.0/lib64)</p><p>Then, you must also ensure the cudnn file that contain libcudnn.so.7 is in LD_LIBRARY_PATH. For this step, you need to download cudnn. For me, I was download cudnn 7.4 for cuda 10.0 in <a href="https://developer.nvidia.com/compute/machine-learning/cudnn/secure/v7.4.1.5/prod/10.0_20181108/RHEL7_3-x64/libcudnn7-7.4.1.5-1.cuda10.0.x86_64.rpm" target="_blank" rel="noopener">cuDNN Runtime Library for RedHat/Centos 7.3 (RPM)</a> (libcudnn7-7.4.1.5-1.cuda10.0.x86_64), and there is libcudnn.so.7 in \libcudnn7-7.4.1.5-1.cuda10.0.x86_64\usr\lib64 and you can also see l ibcudnn.so.7.4.1 in it. </p><p>Extract and copy this to your linux and then, you need to delete  libcudnn.so.7, the run the following code to generate it </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ln -s libcudnn.so.7.4.1 libcudnn.so.7</span><br><span class="line">ln -s libcudnn.so.7 libcudnn.so </span><br><span class="line">ldconfig</span><br></pre></td></tr></table></figure><p>You can see both libcudnn.so.7.4.1 and libcudnn.so.7 is in the file. You can add it to  LD_LIBRARY_PATH and then run </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">'CUDA_VISIBLE_DEVICES'</span>] = <span class="string">"0"</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.python.client <span class="keyword">import</span> device_lib</span><br><span class="line">print(device_lib.list_local_devices())</span><br></pre></td></tr></table></figure><p>The tensorflow should see the gpu without an error </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">2019-06-30 14:24:17.432330: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: </span><br><span class="line">name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582</span><br><span class="line">pciBusID: 0000:db:00.0</span><br><span class="line">2019-06-30 14:24:17.432437: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0</span><br><span class="line">2019-06-30 14:24:17.432473: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0</span><br><span class="line">2019-06-30 14:24:17.432502: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0</span><br><span class="line">2019-06-30 14:24:17.432529: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0</span><br><span class="line">2019-06-30 14:24:17.432556: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0</span><br><span class="line">2019-06-30 14:24:17.432582: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0</span><br><span class="line">2019-06-30 14:24:17.432610: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7</span><br><span class="line">2019-06-30 14:24:17.478617: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0</span><br><span class="line">2019-06-30 14:24:17.478691: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:</span><br><span class="line">2019-06-30 14:24:17.478715: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 </span><br><span class="line">2019-06-30 14:24:17.478734: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N </span><br><span class="line">2019-06-30 14:24:17.498562: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11439 MB memory) -&gt; physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:db:00.0, compute capability: 6.1)</span><br><span class="line">[name: &quot;/device:CPU:0&quot;</span><br><span class="line">device_type: &quot;CPU&quot;</span><br><span class="line">memory_limit: 268435456</span><br><span class="line">locality &#123;</span><br><span class="line">&#125;</span><br><span class="line">incarnation: 7765454328739896397</span><br><span class="line">, name: &quot;/device:XLA_GPU:0&quot;</span><br><span class="line">device_type: &quot;XLA_GPU&quot;</span><br><span class="line">memory_limit: 17179869184</span><br><span class="line">locality &#123;</span><br><span class="line">&#125;</span><br><span class="line">incarnation: 14436236814517186873</span><br><span class="line">physical_device_desc: &quot;device: XLA_GPU device&quot;</span><br><span class="line">, name: &quot;/device:XLA_CPU:0&quot;</span><br><span class="line">device_type: &quot;XLA_CPU&quot;</span><br><span class="line">memory_limit: 17179869184</span><br><span class="line">locality &#123;</span><br><span class="line">&#125;</span><br><span class="line">incarnation: 10506527593758283137</span><br><span class="line">physical_device_desc: &quot;device: XLA_CPU device&quot;</span><br><span class="line">, name: &quot;/device:GPU:0&quot;</span><br><span class="line">device_type: &quot;GPU&quot;</span><br><span class="line">memory_limit: 11994670695</span><br><span class="line">locality &#123;</span><br><span class="line">  bus_id: 2</span><br><span class="line">  numa_node: 1</span><br><span class="line">  links &#123;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">incarnation: 9840900236876381126</span><br><span class="line">physical_device_desc: &quot;device: 0, name: TITAN Xp, pci bus id: 0000:db:00.0, compute capability: 6.1&quot;</span><br><span class="line">]</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Problems-for-using-gpu-for-Tensorflow-or-Keras-network-on-linux-tensorflow-can-only-see-CPU-Could-not-dlopen-library-‘libcuda-so-1’-
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Distribution related function in Seaborn</title>
    <link href="https://num-github.github.io/2019/06/10/seaborn-1/"/>
    <id>https://num-github.github.io/2019/06/10/seaborn-1/</id>
    <published>2019-06-10T00:30:07.509Z</published>
    <updated>2019-06-10T01:21:51.489Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Some-useful-distribution-related-function-in-Seaborn"><a href="#Some-useful-distribution-related-function-in-Seaborn" class="headerlink" title="Some useful distribution related function in Seaborn"></a>Some useful distribution related function in Seaborn</h1><p>*<font size="1" color="gray">I recently wrote some code for training some data with ML algorithm and I found that some functions are pretty used for data analysis</font>*</p><details open><summary><font size="4">  distplot() </font></summary><p>distplot() function are able to show univariate distribution. For more detailed information, click <a href="https://seaborn.pydata.org/tutorial/distributions.html#distribution-tutorial" target="_blank" rel="noopener"> here</a>. </p><p>*<font size="2" color="gray">   If you want to know a distribution of a numerical feature, you can try this.</font>*</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import seaborn as sns </span><br><span class="line">sns.distplot(Input[0],label=&apos;Input&apos;,color=&apos;g&apos;)</span><br></pre></td></tr></table></figure><h4 id="The-commonly-used-parameters"><a href="#The-commonly-used-parameters" class="headerlink" title="The commonly used parameters :"></a>The commonly used parameters :</h4><ul><li>The input data </li><li>label: It will show on legend once you call <strong>plt.legend()</strong>. </li><li>Color: The color of the histogram and lines <em>(parameters are similar to matlab and the default color for first line is blue)</em>.</li></ul><p>You can draw distribution of one dimensional data by this with matplotlib.pyplot.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import seaborn as sns </span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">plt.figure()</span><br><span class="line"># Sometimes you may need to transpose(.T) your numpy array </span><br><span class="line">sns.distplot(fail_data[0],label=&apos;fail_data&apos;,color=&apos;g&apos;)</span><br><span class="line">plt.legend();</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>fail_data[0] is the first dimension of my data. The Result looks like that.</p><div align="center"><img src="https://num-github.github.io/2019/06/10/seaborn-1/chrome_2019-06-10_00-12-30.png" height="300" width="400"></div><p>Your image could be quite different, but both <strong>histogram</strong> and  <a href="https://en.wikipedia.org/wiki/Kernel_density_estimation" target="_blank" rel="noopener"> kernel density estimate (KDE)</a> line .</p><p>You could choose to remove histogram by setting <strong><font color="blue">hist=False</font></strong> or KDE by <strong><font color="blue">kde=False</font></strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plt.figure()</span><br><span class="line">sns.distplot(hist=False,fail_data[0],label=&apos;fail_data&apos;,color=&apos;g&apos;)</span><br><span class="line">plt.legend();</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><div align="center"><img src="https://num-github.github.io/2019/06/10/seaborn-1/chrome_2019-06-10_00-37-08.png" height="300" width="400"></div><p>If you want to draw more than one distribution on same figure, just add anther above plt.show()</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plt.figure()</span><br><span class="line">sns.distplot(scuss_data[2],label=&apos;scuss_data&apos;)</span><br><span class="line">sns.distplot(fail_data[2],color=&apos;g&apos;,label=&apos;fail_data&apos;)</span><br><span class="line">plt.legend();</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><div align="center"><img src="https://num-github.github.io/2019/06/10/seaborn-1/chrome_2019-06-10_00-26-08.png" height="300" width="400"></div></details><details open><summary><font size="4">  violinplot() </font></summary>It is still used for showing distribution. The data I used here are look like that:<div align="center"><table><thead><tr><th align="center">0</th><th align="center">1</th><th align="center">2</th><th align="center">3</th></tr></thead><tbody><tr><td align="center">0.000020</td><td align="center">0.001459</td><td align="center">7.992451</td><td align="center">1.0</td></tr><tr><td align="center">0.000021</td><td align="center">0.001149</td><td align="center">6.903953</td><td align="center">0.0</td></tr><tr><td align="center">0.000029</td><td align="center">0.001637</td><td align="center">6.512294</td><td align="center">0.0</td></tr><tr><td align="center">0.000028</td><td align="center">0.001208</td><td align="center">1.936095</td><td align="center">1.0</td></tr><tr><td align="center">。。。。。。。。。。。。。。。。</td><td align="center"></td><td align="center"></td><td align="center"></td></tr></tbody></table></div>The first 3 rows are values correspond to feature 0,1,2.The third row is classes (only 0 and 1)<p>Thus, if I want to find the distribution of feature 1 in each class, we can just write </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plt.figure()</span><br><span class="line">sns.violinplot(x=3,y=1, data=pd.DataFrame(data))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><div align="center"><img src="https://num-github.github.io/2019/06/10/seaborn-1/chrome_2019-06-10_01-12-02.png" height="300" width="400"></div><ul><li>x is class row</li><li>y is feature row</li><li>Input data must be pandas data structure.</li><li>You can also input the name (string) of each row to x and y, if you have.</li></ul><p>You can also plot many figures in this way </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">f, ax1 = plt.subplots(ncols=2, nrows=2, figsize=(8, 8))</span><br><span class="line">sns.violinplot(x=3,y=0, data=pd.DataFrame(data),ax=ax1[0][0])</span><br><span class="line">ax1[0][0].set_title(&apos;x&apos;)</span><br><span class="line">sns.violinplot(x=3,y=1, data=pd.DataFrame(data),ax=ax1[0][1])</span><br><span class="line">ax1[0][1].set_title(&apos;y&apos;)</span><br><span class="line">sns.violinplot(x=3,y=2, data=pd.DataFrame(data),ax=ax1[1][0])</span><br><span class="line">ax1[1][0].set_title(&apos;z&apos;)</span><br><span class="line">f.tight_layout()</span><br></pre></td></tr></table></figure><ul><li>ax can be used to set the position of figures</li></ul><div align="center"><img src="https://num-github.github.io/2019/06/10/seaborn-1/chrome_2019-06-10_01-19-24.png" height="300" width="400"></div></details>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Some-useful-distribution-related-function-in-Seaborn&quot;&gt;&lt;a href=&quot;#Some-useful-distribution-related-function-in-Seaborn&quot; class=&quot;headerl
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Get start !</title>
    <link href="https://num-github.github.io/2019/06/06/Get-start/"/>
    <id>https://num-github.github.io/2019/06/06/Get-start/</id>
    <published>2019-06-05T23:06:17.000Z</published>
    <updated>2019-06-05T23:07:50.594Z</updated>
    
    <content type="html"><![CDATA[<p>The start of my career.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;The start of my career.&lt;/p&gt;

      
    
    </summary>
    
    
  </entry>
  
</feed>
