<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  

  
  <title>FengerBlog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Investigation for Reinforcement Learning (RL) in roboticRecently, I was asked to do some research on robot. The main task for that is to tidy up clothes. My first impression  on that task is this is v">
<meta property="og:type" content="article">
<meta property="og:title" content="FengerBlog">
<meta property="og:url" content="https://num-github.github.io/2019/07/13/Investigation-for-Reinforcement-Learning-in-robotic/index.html">
<meta property="og:site_name" content="FengerBlog">
<meta property="og:description" content="Investigation for Reinforcement Learning (RL) in roboticRecently, I was asked to do some research on robot. The main task for that is to tidy up clothes. My first impression  on that task is this is v">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://num-github.github.io/.io//robot.png">
<meta property="og:image" content="https://num-github.github.io/.io//1563039827434.png">
<meta property="og:image" content="https://num-github.github.io/.io//D:%5Cblog%5Csource_posts%5Cchrome_2019-07-13_21-44-40.png">
<meta property="og:image" content="https://num-github.github.io/.io//chrome_2019-07-13_21-41-24.png">
<meta property="og:image" content="https://num-github.github.io/.io//POWERPNT_2019-07-14_12-55-47.png">
<meta property="og:image" content="https://num-github.github.io/.io//POWERPNT_2019-07-14_14-05-32.png">
<meta property="og:image" content="https://num-github.github.io/.io//HER.png">
<meta property="og:updated_time" content="2019-07-31T15:51:52.133Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="FengerBlog">
<meta name="twitter:description" content="Investigation for Reinforcement Learning (RL) in roboticRecently, I was asked to do some research on robot. The main task for that is to tidy up clothes. My first impression  on that task is this is v">
<meta name="twitter:image" content="https://num-github.github.io/.io//robot.png">
  
    <link rel="alternate" href="/atom.xml" title="FengerBlog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">FengerBlog</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">FengerBlog</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://num-github.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-Investigation-for-Reinforcement-Learning-in-robotic" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/07/13/Investigation-for-Reinforcement-Learning-in-robotic/" class="article-date">
  <time datetime="2019-07-13T12:36:39.756Z" itemprop="datePublished">2019-07-13</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Investigation-for-Reinforcement-Learning-RL-in-robotic"><a href="#Investigation-for-Reinforcement-Learning-RL-in-robotic" class="headerlink" title="Investigation for Reinforcement Learning (RL) in robotic"></a>Investigation for Reinforcement Learning (RL) in robotic</h1><p><em>Recently, I was asked to do some research on robot. The main task for that is to tidy up clothes. My first impression  on that task is this is very similar to pick and place tasks in has been talked about in many domes that I had watched before, so I think that might be a good point to start and investigate it from what I had know.</em></p>
<p>I think, this article probably useful for beginners who wants to apply reinforcement learning into their project but have a little (like me) or no knowledge about reinforcement learning. </p>
<p>This article also include many links that require you to set up some environment. I know that this is a very tedious and annoying work, but it would be better to do that if you hope to implement RL to your project, because I think take some practice is the best way to learn. </p>
<h3 id="Useful-links-for-beginners"><a href="#Useful-links-for-beginners" class="headerlink" title="Useful links for beginners:"></a>Useful links for beginners:</h3><p>The following links are some useful resources that is very useful and interesting and what I discussed here is also comes from the following links: </p>
<p> Tools:</p>
<p>​             <a href="https://stable-baselines.readthedocs.io/en/master/modules/ddpg.html" target="_blank" rel="noopener">OpenAI stable baselines</a> </p>
<p>​            <a href="https://github.com/openai/baselines" target="_blank" rel="noopener">OpenAI baseline</a></p>
<p>​            <a href="http://gym.openai.com/envs/#robotics" target="_blank" rel="noopener">OpenAI gym (robot) </a></p>
<p>Learning: </p>
<p>​            <a href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html" target="_blank" rel="noopener">Spinning up</a></p>
<p>​            <a href="https://www.youtube.com/watch?v=Wypc1a-1ZYA" target="_blank" rel="noopener">Matlab Walking Robot Problem</a> </p>
<p>​            <a href="https://github.com/sarcturus00/Tidy-Reinforcement-learning" target="_blank" rel="noopener">Tidy-reinforcement learning</a></p>
<p>​            <a href="https://www.bilibili.com/video/av24724071?from=search&seid=14445377713604973899" target="_blank" rel="noopener">“Deep Reinforcement Learning, 2018” by Hongyi Li (Chinese version)</a></p>
<h5 id="Before-you-read-ensure-that-you-have-some-basic-knowledge-of-reinforcement-learning-You-can-read-though-the-tutorial-in-spinning-up-of-Key-Concepts-in-RL"><a href="#Before-you-read-ensure-that-you-have-some-basic-knowledge-of-reinforcement-learning-You-can-read-though-the-tutorial-in-spinning-up-of-Key-Concepts-in-RL" class="headerlink" title="Before you read, ensure that you have some basic knowledge of reinforcement learning. You can read though the tutorial in spinning up of Key Concepts in RL."></a>Before you read, ensure that you have some basic knowledge of reinforcement learning. You can read though the tutorial in <a href="https://spinningup.openai.com/en/latest/spinningup/rl_intro.html" target="_blank" rel="noopener">spinning up</a> of Key Concepts in RL.</h5><h3 id="Play-with-environment"><a href="#Play-with-environment" class="headerlink" title="Play with environment"></a>Play with environment</h3><p>The first algorithm that found useful is <a href="https://openai.com/blog/ingredients-for-robotics-research/" target="_blank" rel="noopener">Hindsight Experience Replay (HER) + Deep Deterministic Policy Gradient (DDPG)</a>. The ‘+’ here means it is the algorithm that apply HER to improve the performance of DDPG on this task. </p>
<p>Before we discussing the algorithm in detail, let’s have a look what’s that tasks that this algorithm has been solved (The following are just part of the summary of the tasks, you can find more detail on <a href="https://arxiv.org/pdf/1802.09464.pdf" target="_blank" rel="noopener">this</a>  or watch the <a href="https://sites.google.com/site/hindsightexperiencereplay/" target="_blank" rel="noopener">demo</a> that may give you some intuition on that). </p>
<p>Basically, the aim of the simulation is to move the gripper of a robot to achieve the certain tasks: FetchReach, FetchPush, FetchSlide  and FetchPickAndPlace. </p>
<p>There are 4 parameters for actions for gripper: 3 to specify how the gripper will move in Cartesian coordinates  and one to specify the  open and close of the gripper. </p>
<p>The reward for that is 0 if the goal is achieved (e.g. in pick and place tasks,  once the robot move the cube to desired location, the reward would be 0 or otherwise would be 1).  </p>
<p>The observations include the Cartesian position of the gripper, its linear velocity as well as the position and linear velocity of the robot’s gripper. If an object is present, we also include the object’s Cartesian position and rotation using Euler angles, its linear and angular velocities, as well as its position and linear velocities relative to gripper. </p>
<p><img src="/.io//robot.png" alt="robot"></p>
<p>You can run the following code and modify the array of actions to see if there are changes occur, once your had set up the environment for OpenAI gym-robot (You need to install MuJoCo).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gym</span><br><span class="line">env = gym.make(<span class="string">"FetchPickAndPlace-v1"</span>)</span><br><span class="line">observation = env.reset()</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">  env.render()</span><br><span class="line">  action = env.action_space.sample()</span><br><span class="line">  print( action )</span><br><span class="line">  observation, reward, done, info = env.step(action)</span><br><span class="line">  print(  observation, reward, done, info )</span><br><span class="line">  <span class="keyword">if</span> done:</span><br><span class="line">    observation = env.reset()</span><br><span class="line">env.close()</span><br></pre></td></tr></table></figure>

<p>After that you can following the guide of <a href="https://github.com/openai/baselines/tree/master/baselines/her" target="_blank" rel="noopener">baseline-HER</a> to train HER model in this environment so as to get an intuition of this algorithm or modify the code a little bit in baseline demo to apply it to your project (change from OpenAI-gym environment to your own environment (e.g. ROS) ). </p>
<h3 id="Link-with-openAI-with-ROS-If-you-don’t-use-ROS-in-your-project-you-can-skip-this-part"><a href="#Link-with-openAI-with-ROS-If-you-don’t-use-ROS-in-your-project-you-can-skip-this-part" class="headerlink" title="Link with openAI with ROS (If you don’t use ROS in your project, you can skip this part)"></a>Link with openAI with ROS (If you don’t use ROS in your project, you can skip this part)</h3><p>In order to do that your can following the guide of <a href="http://wiki.ros.org/openai_ros" target="_blank" rel="noopener">OpenAI_ROS</a>. In summary, what the OpenAI_ROS want you to do is to specify the following three things:</p>
<p><strong>Task environment</strong>:  the task that the robot has to learn (<em>e.g.</em> actions, reward functions, observations).</p>
<p><strong>Robot environment</strong>: the robot to use on the task (<em>e.g.</em> contain all the ROS functionalities that your robot will need in order to be controlled and checks that every ROS stuff required is up and running on the robot (topics, services …).)</p>
<p><strong>Training script</strong>: to set up the learning algorithm that you want to use in order to make your agent learn and select the task and robot to be used.</p>
<h3 id="Algorithms"><a href="#Algorithms" class="headerlink" title="Algorithms"></a>Algorithms</h3><p>In a typical robot control problems for reinforcement learning, the algorithms must be able to handle the issue of continuous actions and sparse reward problems. In the algorithm of HER+DDPG, DDPG aim to provide high performance algorithms for continuous actions and HER used to solve sparse reward problems. </p>
<h5 id="DDPG"><a href="#DDPG" class="headerlink" title="DDPG:"></a>DDPG:</h5><p>DDPG algorithm was developed from actor critic, which also combined policy based algorithm and value based algorithm. </p>
<h6 id="value-based-algorithm"><a href="#value-based-algorithm" class="headerlink" title="value  based algorithm"></a>value  based algorithm</h6><p>Let’s begin with the value based algorithm, which is <strong>Q-learning</strong> in this case. </p>
<p>The aim of an reinforcement learning algorithm is to maximize the reward from environment. For Q-learning, this is achieved by Q-table, which that map the reward predications relate to actions and states.  For example, if you want to train a robot that can only move forward (action 1) or backward (action 2) and  3 state (s1, s2, s3), each combinations of the states or actions will have a value (Q-value or predicted reward) for making decision, this value will be continuously updated during training based on the feedback from environment for each action and state. The optimal decision making strategy or policy is usually achieved by picking up the action that have the maximum Q value.</p>
<p>Thus, if you want a robot to reach a goal, go state 3 as quick as possible, for example. The robot will try an action each time based on your decision-making strategy and observe the current state and reward from environment; then, based on these to update Q table.</p>
<p><img src="/.io//1563039827434.png" alt="1563039827434"></p>
<p>The image above shows the Q-learning algorithm and how to update Q table in more detail, but I won’t talk more about it, what I want to mention is that Q table is used for mapping state and actions to Q-values, and our decision policy will make decisions based on Q-values.</p>
<p>The improved version for Q-learning is <strong>DQN</strong>, which replace the Q-table with a neural network. Such neural are able to handle much more state than pure Q-table. Similar to the neural network for regression problems, the neural network here regress the score of reward which is<br>$$<br>Reward+\gamma max(Q(state_{Next},action)<br>$$<br>Where γ is discount factor. If Q is accurate enough, this<br>$$<br>max(Q(state_{Next},action)<br>$$<br>can be viewed as  the score in the future, so the γ here is used to reduce the importance of the effect of future and the value of γ means to what extent that we hope to ignore the score of future. </p>
<h6 id="tips-for-train-a-neural-network-in-RL"><a href="#tips-for-train-a-neural-network-in-RL" class="headerlink" title="tips for train a neural network in RL"></a>tips for train a neural network in RL</h6><p>Another two commonly used trick which can is experience replay and target network. In my view, these two technique both trying to make the neural network training more stable by make the parameters updating process more like unsupervised learning. </p>
<p>Experience replay is to firstly sample reward, action, state, observation, etc, and then store them in a buffer.  And then train them together. </p>
<p>Target network is use two network, one for parameter updating and the other for providing Q. Once the training has processed for some times, copy the parameters of updating network to the Q network. You can read more about Q-learning and DQN in <a href="https://medium.com/@jonathan_hui/rl-dqn-deep-q-network-e207751f7ae4" target="_blank" rel="noopener">here</a></p>
<p>Apart from that, there is also many other tricks for improving  DQN, but the effect of these tricks can be shown on these  two  images and you can find more information on that in <a href="https://arxiv.org/pdf/1710.02298.pdf" target="_blank" rel="noopener">this paper</a>.</p>
<p><img src="/.io//D:%5Cblog%5Csource_posts%5Cchrome_2019-07-13_21-44-40.png" alt="chrome_2019-07-13_21-44-40"></p>
<p><img src="/.io//chrome_2019-07-13_21-41-24.png" alt="chrome_2019-07-13_21-41-24">{:height=”50%” width=”50%”}</p>
<h6 id="policy-based-algorithm"><a href="#policy-based-algorithm" class="headerlink" title="policy based algorithm"></a>policy based algorithm</h6><p>The simplest algorithm for policy based algorithm is policy gradient. Intuitively, this is the algorithm that aim to maximize the probability of the taking actions that give the maximum reward, which is achieved by the method of gradient ascent. You can see more about it in <a href="https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html" target="_blank" rel="noopener">here</a></p>
<p>The input of it is the observation, the output is the probability of each action. The mapping between observation and actions can be achieved by a neural network. </p>
<p>For deterministic policy gradient, it do not output the probability of each action, but the the exact action that it will take.</p>
<h6 id="DDPG-1"><a href="#DDPG-1" class="headerlink" title="DDPG"></a>DDPG</h6><p>Ok, now, you should get sense about what is Q network, policy network, and some tips for training a network in RL algorithm, in this section, let’s put them all together to form DDPG.</p>
<p>One of the problem for value based algorithm is it cannot handle continuous action space very in a very easy way, but policy based algorithm can achieve that goal. The problem for policy network is the efficiency of its updating is slow, so introduced Q network will speed up this process.   Actually, the basic thinking for this algorithm is to combine the benefits of the two types of algorithms, that is, the Q networks try to learn the reward of actions, the policy networks try to learn how to give actions that maximize the predicted reward (Q value). </p>
<p><img src="/.io//POWERPNT_2019-07-14_12-55-47.png" alt="POWERPNT_2019-07-14_12-55-47"></p>
<p>With such thinking and some tricks mentioned before like experience replay and target network, the final version of DDPG comes up. You may find different version of DDPG pseudo code, but they all consists of the tricks and the basic thinking. For example, the following pseudo code comes from <a href="https://github.com/sarcturus00/Tidy-Reinforcement-learning" target="_blank" rel="noopener">Tidy-Reinforcement-learning</a>, you can find other RL algorithm in here, as well.</p>
<p><img src="/.io//POWERPNT_2019-07-14_14-05-32.png" alt="POWERPNT_2019-07-14_14-05-32"></p>
<p>The blue box represents the basic thinking that Q network and policy network try to minimized. Red boxes is the tricks about reward and target network, and the yellow one is anther trick that add some noise to action to increase the ability of exploration of the algorithm. You can find more detail information and another version of DDPG at <a href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html" target="_blank" rel="noopener">here</a> which also add clip to it.  </p>
<h5 id="HER"><a href="#HER" class="headerlink" title="HER"></a>HER</h5><p>For now, DDPG has solved the problem of learning for continuous actions space, the next step is to deal with sparse reward problem. </p>
<p>In general, in my view, HER is the algorithm that make a very sparse reward problem which only give reward at certain state do not extremely sparse by directly give some reward to some of other states.</p>
<p>Or you can understand it in a way that <a href="https://arxiv.org/pdf/1707.01495.pdf" target="_blank" rel="noopener">HER paper</a> has mentioned: “after experiencing some episode s0, s1, . . . , sT we store in the replay buffer every transition st → st+1 not only with the original goal used for this episode but also with a subset of other goals. Notice that the goal being pursued influences the agent’s actions but not the environment dynamics and therefore we can replay each trajectory with an arbitrary goal assuming that we use an off-policy RL algorithms.” </p>
<p>The following is the pseudo code is the complete algorithm from HER paper and blue box is the the HER part.</p>
<p><img src="/.io//HER.png" alt="HER"></p>
<p>If you feeling that this expression is still abstract. The following code from <a href="https://github.com/sarcturus00/Tidy-Reinforcement-learning" target="_blank" rel="noopener">Tidy-Reinforcement-learning</a> clearly demonstrate what has mentioned before, where is future_k is the hyperparameter that specify how many additional state will be assign a reward (in this case, reward 0 means achieve the goal, -1 means haven’t achieve the goal). </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> HER:</span><br><span class="line">    <span class="keyword">for</span> h <span class="keyword">in</span> range(future_k):</span><br><span class="line">        future = np.random.randint(t, nstep)</span><br><span class="line">        goal_ = episode_experience[future][<span class="number">3</span>]</span><br><span class="line">        new_inputs0 = np.concatenate([obs0, goal_], axis=<span class="number">-1</span>)</span><br><span class="line">        new_inputs1 = np.concatenate([obs1, goal_], axis=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (np.array(obs1) == np.array(goal_)).all():</span><br><span class="line">            r_ = <span class="number">0.0</span> <span class="comment">#add reward to additional state.</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            r_ = <span class="number">-1.0</span></span><br><span class="line">            agent.memory.store_transition(new_inputs0, act, r_, new_inputs1,done)</span><br></pre></td></tr></table></figure>

<p>OK, we had finished the description for DDPG and HER algorithms</p>
<h3 id="Other-tips-and-information-that-maybe-helpful"><a href="#Other-tips-and-information-that-maybe-helpful" class="headerlink" title="Other tips and information that maybe helpful"></a>Other tips and information that maybe helpful</h3><p>For now, we had talked about DDPG+HER that can be applied to a robot control project and you may can test the algorithm for now, but there are also many other information that I find very interesting. </p>
<h5 id="Curriculum-learning-in-RL"><a href="#Curriculum-learning-in-RL" class="headerlink" title="Curriculum learning in RL"></a>Curriculum learning in RL</h5><p>This is the concept that make the robot to start learning from simple tasks and then move on to more complex tasks. For example, if you want your robot to learn how to pick up a ring and set it to a bar. You can firstly teach the robot to hold a ring in the position that is close to the bar to set it to a bar, then hold a ring in the position that is far away from the bar to set it to a bar and so on. Finally, teach the robot to learning the task in the final lesson. Such curriculum can be generated reversely, start from final steps of the task to the first step of the task. </p>
<h5 id="How-to-add-additional-command-to-your-robot"><a href="#How-to-add-additional-command-to-your-robot" class="headerlink" title="How to add additional command to your robot"></a>How to add additional command to your robot</h5><p>This is the <a href="https://www.youtube.com/watch?v=Wypc1a-1ZYA&list=PLn8PRpmsu08qw_IwpgVNsKiJQpvvW0MmM&index=4" target="_blank" rel="noopener">video</a> from Matlab. From 8:50, they start to discuss this issue. In short, it was achieved by adding additional command to robot and modify reward about the command. </p>
<h5 id="Some-problems-about-RL-in-robot-control-and-how-to-reduce-their-impact"><a href="#Some-problems-about-RL-in-robot-control-and-how-to-reduce-their-impact" class="headerlink" title="Some problems about RL in robot control and how to reduce their impact"></a>Some problems about RL in robot control and how to reduce their impact</h5><p>Also the <a href="https://www.youtube.com/watch?v=zHV3UcH-nr0&list=PLn8PRpmsu08qw_IwpgVNsKiJQpvvW0MmM&index=5" target="_blank" rel="noopener">video</a> from Matlab. In fact, these are also the problems about neural network. </p>
<h3 id="Imitation-learning"><a href="#Imitation-learning" class="headerlink" title="Imitation learning"></a>Imitation learning</h3>
      
    </div>
    <footer class="article-footer">
      <a data-url="https://num-github.github.io/2019/07/13/Investigation-for-Reinforcement-Learning-in-robotic/" data-id="cjyrfjmoc00051gup5qnt00px" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2019/07/21/Review of SiamMask/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          (no title)
        
      </div>
    </a>
  
  
    <a href="/2019/06/30/Problems-for-using-gpu-for-Tensorflow-or-Keras-network-on-linux/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title"></div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">July 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/06/">June 2019</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2019/07/31/gazebo-and-unity-comparation/">gazebo and unity comparation</a>
          </li>
        
          <li>
            <a href="/2019/07/25/Note for RPN/">(no title)</a>
          </li>
        
          <li>
            <a href="/2019/07/21/Review of SiamMask/">(no title)</a>
          </li>
        
          <li>
            <a href="/2019/07/13/Investigation-for-Reinforcement-Learning-in-robotic/">(no title)</a>
          </li>
        
          <li>
            <a href="/2019/06/30/Problems-for-using-gpu-for-Tensorflow-or-Keras-network-on-linux/">(no title)</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2019 Fenger<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>