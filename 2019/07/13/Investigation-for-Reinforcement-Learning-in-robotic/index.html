<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang>
<head><meta name="generator" content="Hexo 3.9.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">





  <link rel="alternate" href="/atom.xml" title="FengerBlog" type="application/atom+xml">






<meta name="description" content="Investigation for Reinforcement Learning (RL) in roboticRecently, I was asked to do some research on robot. The main task for that is to tidy up clothes. My first impression  on that task is this is v">
<meta property="og:type" content="article">
<meta property="og:title" content="Investigation for Reinforcement Learning (RL) in robotic">
<meta property="og:url" content="https://num-github.github.io/2019/07/13/Investigation-for-Reinforcement-Learning-in-robotic/index.html">
<meta property="og:site_name" content="FengerBlog">
<meta property="og:description" content="Investigation for Reinforcement Learning (RL) in roboticRecently, I was asked to do some research on robot. The main task for that is to tidy up clothes. My first impression  on that task is this is v">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://num-github.github.io/2019/07/13/Investigation-for-Reinforcement-Learning-in-robotic/robot.png">
<meta property="og:image" content="https://num-github.github.io/2019/07/13/Investigation-for-Reinforcement-Learning-in-robotic/1563039827434.png">
<meta property="og:image" content="https://num-github.github.io/2019/07/13/Investigation-for-Reinforcement-Learning-in-robotic/chrome_2019-07-13_21-44-40.png">
<meta property="og:image" content="https://num-github.github.io/2019/07/13/Investigation-for-Reinforcement-Learning-in-robotic/chrome_2019-07-13_21-41-24.png">
<meta property="og:image" content="https://num-github.github.io/2019/07/13/Investigation-for-Reinforcement-Learning-in-robotic/POWERPNT_2019-07-14_12-55-47.png">
<meta property="og:image" content="https://num-github.github.io/2019/07/13/Investigation-for-Reinforcement-Learning-in-robotic/POWERPNT_2019-07-14_14-05-32.png">
<meta property="og:image" content="https://num-github.github.io/2019/07/13/Investigation-for-Reinforcement-Learning-in-robotic/HER.png">
<meta property="og:updated_time" content="2019-08-30T23:03:28.759Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Investigation for Reinforcement Learning (RL) in robotic">
<meta name="twitter:description" content="Investigation for Reinforcement Learning (RL) in roboticRecently, I was asked to do some research on robot. The main task for that is to tidy up clothes. My first impression  on that task is this is v">
<meta name="twitter:image" content="https://num-github.github.io/2019/07/13/Investigation-for-Reinforcement-Learning-in-robotic/robot.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://num-github.github.io/2019/07/13/Investigation-for-Reinforcement-Learning-in-robotic/">





  <title>Investigation for Reinforcement Learning (RL) in robotic | FengerBlog</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">FengerBlog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">FengerBlog</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://num-github.github.io/2019/07/13/Investigation-for-Reinforcement-Learning-in-robotic/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Fenger">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="FengerBlog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Investigation for Reinforcement Learning (RL) in robotic</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-07-13T13:36:39+01:00">
                2019-07-13
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="Investigation-for-Reinforcement-Learning-RL-in-robotic"><a href="#Investigation-for-Reinforcement-Learning-RL-in-robotic" class="headerlink" title="Investigation for Reinforcement Learning (RL) in robotic"></a>Investigation for Reinforcement Learning (RL) in robotic</h1><p><em>Recently, I was asked to do some research on robot. The main task for that is to tidy up clothes. My first impression  on that task is this is very similar to pick and place tasks in has been talked about in many domes that I had watched before, so I think that might be a good point to start and investigate it from what I had know.</em></p>
<p>I think, this article probably useful for beginners who wants to apply reinforcement learning into their project but have a little (like me) or no knowledge about reinforcement learning. </p>
<p>This article also include many links that require you to set up some environment. I know that this is a very tedious and annoying work, but it would be better to do that if you hope to implement RL to your project, because I think take some practice is the best way to learn. </p>
<h3 id="Useful-links-for-beginners"><a href="#Useful-links-for-beginners" class="headerlink" title="Useful links for beginners:"></a>Useful links for beginners:</h3><p>The following links are some useful resources that is very useful and interesting and what I discussed here is also comes from the following links: </p>
<p> Tools:</p>
<p>​             <a href="https://stable-baselines.readthedocs.io/en/master/modules/ddpg.html" target="_blank" rel="noopener">OpenAI stable baselines</a> </p>
<p>​            <a href="https://github.com/openai/baselines" target="_blank" rel="noopener">OpenAI baseline</a></p>
<p>​            <a href="http://gym.openai.com/envs/#robotics" target="_blank" rel="noopener">OpenAI gym (robot) </a></p>
<p>Learning: </p>
<p>​            <a href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html" target="_blank" rel="noopener">Spinning up</a></p>
<p>​            <a href="https://www.youtube.com/watch?v=Wypc1a-1ZYA" target="_blank" rel="noopener">Matlab Walking Robot Problem</a> </p>
<p>​            <a href="https://github.com/sarcturus00/Tidy-Reinforcement-learning" target="_blank" rel="noopener">Tidy-reinforcement learning</a></p>
<p>​            <a href="https://www.bilibili.com/video/av24724071?from=search&amp;seid=14445377713604973899" target="_blank" rel="noopener">“Deep Reinforcement Learning, 2018” by Hongyi Li (Chinese version)</a></p>
<h5 id="Before-you-read-ensure-that-you-have-some-basic-knowledge-of-reinforcement-learning-You-can-read-though-the-tutorial-in-spinning-up-of-Key-Concepts-in-RL"><a href="#Before-you-read-ensure-that-you-have-some-basic-knowledge-of-reinforcement-learning-You-can-read-though-the-tutorial-in-spinning-up-of-Key-Concepts-in-RL" class="headerlink" title="Before you read, ensure that you have some basic knowledge of reinforcement learning. You can read though the tutorial in spinning up of Key Concepts in RL."></a>Before you read, ensure that you have some basic knowledge of reinforcement learning. You can read though the tutorial in <a href="https://spinningup.openai.com/en/latest/spinningup/rl_intro.html" target="_blank" rel="noopener">spinning up</a> of Key Concepts in RL.</h5><h3 id="Play-with-environment"><a href="#Play-with-environment" class="headerlink" title="Play with environment"></a>Play with environment</h3><p>The first algorithm that found useful is <a href="https://openai.com/blog/ingredients-for-robotics-research/" target="_blank" rel="noopener">Hindsight Experience Replay (HER) + Deep Deterministic Policy Gradient (DDPG)</a>. The ‘+’ here means it is the algorithm that apply HER to improve the performance of DDPG on this task. </p>
<p>Before we discussing the algorithm in detail, let’s have a look what’s that tasks that this algorithm has been solved (The following are just part of the summary of the tasks, you can find more detail on <a href="https://arxiv.org/pdf/1802.09464.pdf" target="_blank" rel="noopener">this</a>  or watch the <a href="https://sites.google.com/site/hindsightexperiencereplay/" target="_blank" rel="noopener">demo</a> that may give you some intuition on that). </p>
<p>Basically, the aim of the simulation is to move the gripper of a robot to achieve the certain tasks: FetchReach, FetchPush, FetchSlide  and FetchPickAndPlace. </p>
<p>There are 4 parameters for actions for gripper: 3 to specify how the gripper will move in Cartesian coordinates  and one to specify the  open and close of the gripper. </p>
<p>The reward for that is 0 if the goal is achieved (e.g. in pick and place tasks,  once the robot move the cube to desired location, the reward would be 0 or otherwise would be 1).  </p>
<p>The observations include the Cartesian position of the gripper, its linear velocity as well as the position and linear velocity of the robot’s gripper. If an object is present, we also include the object’s Cartesian position and rotation using Euler angles, its linear and angular velocities, as well as its position and linear velocities relative to gripper. </p>
<p><img src="/2019/07/13/Investigation-for-Reinforcement-Learning-in-robotic/robot.png" alt="robot"></p>
<p>You can run the following code and modify the array of actions to see if there are changes occur, once your had set up the environment for OpenAI gym-robot (You need to install MuJoCo).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gym</span><br><span class="line">env = gym.make(<span class="string">"FetchPickAndPlace-v1"</span>)</span><br><span class="line">observation = env.reset()</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">  env.render()</span><br><span class="line">  action = env.action_space.sample()</span><br><span class="line">  print( action )</span><br><span class="line">  observation, reward, done, info = env.step(action)</span><br><span class="line">  print(  observation, reward, done, info )</span><br><span class="line">  <span class="keyword">if</span> done:</span><br><span class="line">    observation = env.reset()</span><br><span class="line">env.close()</span><br></pre></td></tr></table></figure>
<p>After that you can following the guide of <a href="https://github.com/openai/baselines/tree/master/baselines/her" target="_blank" rel="noopener">baseline-HER</a> to train HER model in this environment so as to get an intuition of this algorithm or modify the code a little bit in baseline demo to apply it to your project (change from OpenAI-gym environment to your own environment (e.g. ROS) ). </p>
<h3 id="Link-with-openAI-with-ROS-If-you-don’t-use-ROS-in-your-project-you-can-skip-this-part"><a href="#Link-with-openAI-with-ROS-If-you-don’t-use-ROS-in-your-project-you-can-skip-this-part" class="headerlink" title="Link with openAI with ROS (If you don’t use ROS in your project, you can skip this part)"></a>Link with openAI with ROS (If you don’t use ROS in your project, you can skip this part)</h3><p>In order to do that your can following the guide of <a href="http://wiki.ros.org/openai_ros" target="_blank" rel="noopener">OpenAI_ROS</a>. In summary, what the OpenAI_ROS want you to do is to specify the following three things:</p>
<p><strong>Task environment</strong>:  the task that the robot has to learn (<em>e.g.</em> actions, reward functions, observations).</p>
<p><strong>Robot environment</strong>: the robot to use on the task (<em>e.g.</em> contain all the ROS functionalities that your robot will need in order to be controlled and checks that every ROS stuff required is up and running on the robot (topics, services …).)</p>
<p><strong>Training script</strong>: to set up the learning algorithm that you want to use in order to make your agent learn and select the task and robot to be used.</p>
<h3 id="Algorithms"><a href="#Algorithms" class="headerlink" title="Algorithms"></a>Algorithms</h3><p>In a typical robot control problems for reinforcement learning, the algorithms must be able to handle the issue of continuous actions and sparse reward problems. In the algorithm of HER+DDPG, DDPG aim to provide high performance algorithms for continuous actions and HER used to solve sparse reward problems. </p>
<h5 id="DDPG"><a href="#DDPG" class="headerlink" title="DDPG:"></a>DDPG:</h5><p>DDPG algorithm was developed from actor critic, which also combined policy based algorithm and value based algorithm. </p>
<h6 id="value-based-algorithm"><a href="#value-based-algorithm" class="headerlink" title="value  based algorithm"></a>value  based algorithm</h6><p>Let’s begin with the value based algorithm, which is <strong>Q-learning</strong> in this case. </p>
<p>The aim of an reinforcement learning algorithm is to maximize the reward from environment. For Q-learning, this is achieved by Q-table, which that map the reward predications relate to actions and states.  For example, if you want to train a robot that can only move forward (action 1) or backward (action 2) and  3 state (s1, s2, s3), each combinations of the states or actions will have a value (Q-value or predicted reward) for making decision, this value will be continuously updated during training based on the feedback from environment for each action and state. The optimal decision making strategy or policy is usually achieved by picking up the action that have the maximum Q value.</p>
<p>Thus, if you want a robot to reach a goal, go state 3 as quick as possible, for example. The robot will try an action each time based on your decision-making strategy and observe the current state and reward from environment; then, based on these to update Q table.</p>
<p><img src="/2019/07/13/Investigation-for-Reinforcement-Learning-in-robotic/1563039827434.png" alt="1563039827434"></p>
<p>The image above shows the Q-learning algorithm and how to update Q table in more detail, but I won’t talk more about it, what I want to mention is that Q table is used for mapping state and actions to Q-values, and our decision policy will make decisions based on Q-values.</p>
<p>The improved version for Q-learning is <strong>DQN</strong>, which replace the Q-table with a neural network. Such neural are able to handle much more state than pure Q-table. Similar to the neural network for regression problems, the neural network here regress the score of reward which is </p>
<script type="math/tex; mode=display">
Reward+\gamma max(Q(state_{Next},action)</script><p>Where γ is discount factor. If Q is accurate enough, this</p>
<script type="math/tex; mode=display">
max(Q(state_{Next},action)</script><p>can be viewed as  the score in the future, so the γ here is used to reduce the importance of the effect of future and the value of γ means to what extent that we hope to ignore the score of future. </p>
<h6 id="tips-for-train-a-neural-network-in-RL"><a href="#tips-for-train-a-neural-network-in-RL" class="headerlink" title="tips for train a neural network in RL"></a>tips for train a neural network in RL</h6><p>Another two commonly used trick which can is experience replay and target network. In my view, these two technique both trying to make the neural network training more stable by make the parameters updating process more like unsupervised learning. </p>
<p>Experience replay is to firstly sample reward, action, state, observation, etc, and then store them in a buffer.  And then train them together. </p>
<p>Target network is use two network, one for parameter updating and the other for providing Q. Once the training has processed for some times, copy the parameters of updating network to the Q network. You can read more about Q-learning and DQN in <a href="https://medium.com/@jonathan_hui/rl-dqn-deep-q-network-e207751f7ae4" target="_blank" rel="noopener">here</a></p>
<p>Apart from that, there is also many other tricks for improving  DQN, but the effect of these tricks can be shown on these  two  images and you can find more information on that in <a href="https://arxiv.org/pdf/1710.02298.pdf" target="_blank" rel="noopener">this paper</a>.</p>
<p><img src="/2019/07/13/Investigation-for-Reinforcement-Learning-in-robotic/chrome_2019-07-13_21-44-40.png" alt="chrome_2019-07-13_21-44-40"></p>
<p><img src="/2019/07/13/Investigation-for-Reinforcement-Learning-in-robotic/chrome_2019-07-13_21-41-24.png" alt="chrome_2019-07-13_21-41-24">{:height=”50%” width=”50%”}</p>
<h6 id="policy-based-algorithm"><a href="#policy-based-algorithm" class="headerlink" title="policy based algorithm"></a>policy based algorithm</h6><p>The simplest algorithm for policy based algorithm is policy gradient. Intuitively, this is the algorithm that aim to maximize the probability of the taking actions that give the maximum reward, which is achieved by the method of gradient ascent. You can see more about it in <a href="https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html" target="_blank" rel="noopener">here</a></p>
<p>The input of it is the observation, the output is the probability of each action. The mapping between observation and actions can be achieved by a neural network. </p>
<p>For deterministic policy gradient, it do not output the probability of each action, but the the exact action that it will take.</p>
<h6 id="DDPG-1"><a href="#DDPG-1" class="headerlink" title="DDPG"></a>DDPG</h6><p>Ok, now, you should get sense about what is Q network, policy network, and some tips for training a network in RL algorithm, in this section, let’s put them all together to form DDPG.</p>
<p>One of the problem for value based algorithm is it cannot handle continuous action space very in a very easy way, but policy based algorithm can achieve that goal. The problem for policy network is the efficiency of its updating is slow, so introduced Q network will speed up this process.   Actually, the basic thinking for this algorithm is to combine the benefits of the two types of algorithms, that is, the Q networks try to learn the reward of actions, the policy networks try to learn how to give actions that maximize the predicted reward (Q value). </p>
<p><img src="/2019/07/13/Investigation-for-Reinforcement-Learning-in-robotic/POWERPNT_2019-07-14_12-55-47.png" alt="POWERPNT_2019-07-14_12-55-47"></p>
<p>With such thinking and some tricks mentioned before like experience replay and target network, the final version of DDPG comes up. You may find different version of DDPG pseudo code, but they all consists of the tricks and the basic thinking. For example, the following pseudo code comes from <a href="https://github.com/sarcturus00/Tidy-Reinforcement-learning" target="_blank" rel="noopener">Tidy-Reinforcement-learning</a>, you can find other RL algorithm in here, as well.</p>
<p><img src="/2019/07/13/Investigation-for-Reinforcement-Learning-in-robotic/POWERPNT_2019-07-14_14-05-32.png" alt="POWERPNT_2019-07-14_14-05-32"></p>
<p>The blue box represents the basic thinking that Q network and policy network try to minimized. Red boxes is the tricks about reward and target network, and the yellow one is anther trick that add some noise to action to increase the ability of exploration of the algorithm. You can find more detail information and another version of DDPG at <a href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html" target="_blank" rel="noopener">here</a> which also add clip to it.  </p>
<h5 id="HER"><a href="#HER" class="headerlink" title="HER"></a>HER</h5><p>For now, DDPG has solved the problem of learning for continuous actions space, the next step is to deal with sparse reward problem. </p>
<p>In general, in my view, HER is the algorithm that make a very sparse reward problem which only give reward at certain state do not extremely sparse by directly give some reward to some of other states.</p>
<p>Or you can understand it in a way that <a href="https://arxiv.org/pdf/1707.01495.pdf" target="_blank" rel="noopener">HER paper</a> has mentioned: “after experiencing some episode s0, s1, . . . , sT we store in the replay buffer every transition st → st+1 not only with the original goal used for this episode but also with a subset of other goals. Notice that the goal being pursued influences the agent’s actions but not the environment dynamics and therefore we can replay each trajectory with an arbitrary goal assuming that we use an off-policy RL algorithms.” </p>
<p>The following is the pseudo code is the complete algorithm from HER paper and blue box is the the HER part.</p>
<p><img src="/2019/07/13/Investigation-for-Reinforcement-Learning-in-robotic/HER.png" alt="HER"></p>
<p>If you feeling that this expression is still abstract. The following code from <a href="https://github.com/sarcturus00/Tidy-Reinforcement-learning" target="_blank" rel="noopener">Tidy-Reinforcement-learning</a> clearly demonstrate what has mentioned before, where is future_k is the hyperparameter that specify how many additional state will be assign a reward (in this case, reward 0 means achieve the goal, -1 means haven’t achieve the goal). </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> HER:</span><br><span class="line">    <span class="keyword">for</span> h <span class="keyword">in</span> range(future_k):</span><br><span class="line">        future = np.random.randint(t, nstep)</span><br><span class="line">        goal_ = episode_experience[future][<span class="number">3</span>]</span><br><span class="line">        new_inputs0 = np.concatenate([obs0, goal_], axis=<span class="number">-1</span>)</span><br><span class="line">        new_inputs1 = np.concatenate([obs1, goal_], axis=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (np.array(obs1) == np.array(goal_)).all():</span><br><span class="line">            r_ = <span class="number">0.0</span> <span class="comment">#add reward to additional state.</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            r_ = <span class="number">-1.0</span></span><br><span class="line">            agent.memory.store_transition(new_inputs0, act, r_, new_inputs1,done)</span><br></pre></td></tr></table></figure>
<p>OK, we had finished the description for DDPG and HER algorithms</p>
<h3 id="Other-tips-and-information-that-maybe-helpful"><a href="#Other-tips-and-information-that-maybe-helpful" class="headerlink" title="Other tips and information that maybe helpful"></a>Other tips and information that maybe helpful</h3><p>For now, we had talked about DDPG+HER that can be applied to a robot control project and you may can test the algorithm for now, but there are also many other information that I find very interesting. </p>
<h5 id="Curriculum-learning-in-RL"><a href="#Curriculum-learning-in-RL" class="headerlink" title="Curriculum learning in RL"></a>Curriculum learning in RL</h5><p>This is the concept that make the robot to start learning from simple tasks and then move on to more complex tasks. For example, if you want your robot to learn how to pick up a ring and set it to a bar. You can firstly teach the robot to hold a ring in the position that is close to the bar to set it to a bar, then hold a ring in the position that is far away from the bar to set it to a bar and so on. Finally, teach the robot to learning the task in the final lesson. Such curriculum can be generated reversely, start from final steps of the task to the first step of the task. </p>
<h5 id="How-to-add-additional-command-to-your-robot"><a href="#How-to-add-additional-command-to-your-robot" class="headerlink" title="How to add additional command to your robot"></a>How to add additional command to your robot</h5><p>This is the <a href="https://www.youtube.com/watch?v=Wypc1a-1ZYA&amp;list=PLn8PRpmsu08qw_IwpgVNsKiJQpvvW0MmM&amp;index=4" target="_blank" rel="noopener">video</a> from Matlab. From 8:50, they start to discuss this issue. In short, it was achieved by adding additional command to robot and modify reward about the command. </p>
<h5 id="Some-problems-about-RL-in-robot-control-and-how-to-reduce-their-impact"><a href="#Some-problems-about-RL-in-robot-control-and-how-to-reduce-their-impact" class="headerlink" title="Some problems about RL in robot control and how to reduce their impact"></a>Some problems about RL in robot control and how to reduce their impact</h5><p>Also the <a href="https://www.youtube.com/watch?v=zHV3UcH-nr0&amp;list=PLn8PRpmsu08qw_IwpgVNsKiJQpvvW0MmM&amp;index=5" target="_blank" rel="noopener">video</a> from Matlab. In fact, these are also the problems about neural network. </p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/07/12/Review of SiamMask/" rel="next" title="SiamMask Note">
                <i class="fa fa-chevron-left"></i> SiamMask Note
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/07/25/Note for RPN/" rel="prev" title="Note for RPN">
                Note for RPN <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Fenger</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">8</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Investigation-for-Reinforcement-Learning-RL-in-robotic"><span class="nav-number">1.</span> <span class="nav-text">Investigation for Reinforcement Learning (RL) in robotic</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Useful-links-for-beginners"><span class="nav-number">1.0.1.</span> <span class="nav-text">Useful links for beginners:</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Before-you-read-ensure-that-you-have-some-basic-knowledge-of-reinforcement-learning-You-can-read-though-the-tutorial-in-spinning-up-of-Key-Concepts-in-RL"><span class="nav-number">1.0.1.0.1.</span> <span class="nav-text">Before you read, ensure that you have some basic knowledge of reinforcement learning. You can read though the tutorial in spinning up of Key Concepts in RL.</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Play-with-environment"><span class="nav-number">1.0.2.</span> <span class="nav-text">Play with environment</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Link-with-openAI-with-ROS-If-you-don’t-use-ROS-in-your-project-you-can-skip-this-part"><span class="nav-number">1.0.3.</span> <span class="nav-text">Link with openAI with ROS (If you don’t use ROS in your project, you can skip this part)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Algorithms"><span class="nav-number">1.0.4.</span> <span class="nav-text">Algorithms</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#DDPG"><span class="nav-number">1.0.4.0.1.</span> <span class="nav-text">DDPG:</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#value-based-algorithm"><span class="nav-number">1.0.4.0.1.1.</span> <span class="nav-text">value  based algorithm</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#tips-for-train-a-neural-network-in-RL"><span class="nav-number">1.0.4.0.1.2.</span> <span class="nav-text">tips for train a neural network in RL</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#policy-based-algorithm"><span class="nav-number">1.0.4.0.1.3.</span> <span class="nav-text">policy based algorithm</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#DDPG-1"><span class="nav-number">1.0.4.0.1.4.</span> <span class="nav-text">DDPG</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#HER"><span class="nav-number">1.0.4.0.2.</span> <span class="nav-text">HER</span></a></li></ol></li></ol><li class="nav-item nav-level-3"><a class="nav-link" href="#Other-tips-and-information-that-maybe-helpful"><span class="nav-number">1.0.5.</span> <span class="nav-text">Other tips and information that maybe helpful</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Curriculum-learning-in-RL"><span class="nav-number">1.0.5.0.1.</span> <span class="nav-text">Curriculum learning in RL</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#How-to-add-additional-command-to-your-robot"><span class="nav-number">1.0.5.0.2.</span> <span class="nav-text">How to add additional command to your robot</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Some-problems-about-RL-in-robot-control-and-how-to-reduce-their-impact"><span class="nav-number">1.0.5.0.3.</span> <span class="nav-text">Some problems about RL in robot control and how to reduce their impact</span></a></li></ol></li></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Fenger</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

</body>
</html>
