<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang>
<head><meta name="generator" content="Hexo 3.9.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">




  


  <link rel="alternate" href="/atom.xml" title="FengerBlog" type="application/atom+xml">






<meta name="description" content="This note is mainly based on Artificial Intelligence Imitation Learning - Tutorial - 2018 ICML Notation &amp;amp; SetupSame as reinforcement learning:  State: s        Action: a       Policy:  πθ    State">
<meta property="og:type" content="article">
<meta property="og:title" content="imitation learning">
<meta property="og:url" content="https://num-github.github.io/2019/08/12/imitation-learning-md/index.html">
<meta property="og:site_name" content="FengerBlog">
<meta property="og:description" content="This note is mainly based on Artificial Intelligence Imitation Learning - Tutorial - 2018 ICML Notation &amp;amp; SetupSame as reinforcement learning:  State: s        Action: a       Policy:  πθ    State">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://num-github.github.io/2019/08/12/imitation-learning-md/dpL.png">
<meta property="og:image" content="https://num-github.github.io/2019/08/12/imitation-learning-md/IRL.png">
<meta property="og:image" content="https://num-github.github.io/2019/08/12/imitation-learning-md/IRL2.png">
<meta property="og:image" content="https://num-github.github.io/2019/08/12/imitation-learning-md/table.png">
<meta property="og:updated_time" content="2019-08-30T22:57:36.130Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="imitation learning">
<meta name="twitter:description" content="This note is mainly based on Artificial Intelligence Imitation Learning - Tutorial - 2018 ICML Notation &amp;amp; SetupSame as reinforcement learning:  State: s        Action: a       Policy:  πθ    State">
<meta name="twitter:image" content="https://num-github.github.io/2019/08/12/imitation-learning-md/dpL.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://num-github.github.io/2019/08/12/imitation-learning-md/">





  <title>imitation learning | FengerBlog</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">FengerBlog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">FengerBlog</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://num-github.github.io/2019/08/12/imitation-learning-md/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Fenger">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="FengerBlog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">imitation learning</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-08-12T20:25:12+01:00">
                2019-08-12
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p><em>This note is mainly based on <a href="https://www.youtube.com/watch?v=6rZTaboSY4k&amp;feature=youtu.be" target="_blank" rel="noopener">Artificial Intelligence Imitation Learning - Tutorial - 2018 ICML</a></em></p>
<h4 id="Notation-amp-Setup"><a href="#Notation-amp-Setup" class="headerlink" title="Notation &amp; Setup"></a>Notation &amp; Setup</h4><p>Same as reinforcement learning: </p>
<p>State: <strong>s</strong>       </p>
<p>Action: <strong>a</strong>      </p>
<p>Policy:  <strong>π<sub>θ</sub></strong>   </p>
<p>State Dynamics (or environment): <strong>P(s’|s,a)</strong> —— given current state &amp; actions, what is the next state.</p>
<p>Rollout: sequential execute <strong>π(s<sub>0</sub>)</strong> on an initial state —-produce trajectory <strong>τ=(s<sub>0</sub>, a<sub>0</sub>, s<sub>1</sub>, a<sub>1</sub> ……)</strong></p>
<p><strong>P(τ|π)</strong>: distribution of trajectories induced by a policy—- depend on the randomness of policy and randomness of state dynamics</p>
<p>​        1. Sample s<sub>0</sub> from P<sub>0</sub> (distribution over initial state), initialize t=1.</p>
<p>​        2. Sample action a<sub>i</sub> from π(s<sub>t-1</sub>)</p>
<p>​        3. Sample next state s<sub>t</sub> from applying a<sub>t</sub> to s<sub> t-1</sub> (require access to environment)</p>
<p>​        4. Repeat from Step 2 with t=t+1</p>
<p><strong>P(s|π)</strong>: distribution of states induced by a policy</p>
<p>​        Let P<sub>t</sub> (s|π) denote distribution over t-th state</p>
<p>​        P(s|π)=(1/T)∑P<sub>t</sub>(s|π) </p>
<h5 id="Behavioral-Cloning-Reduction-to-supervised-learning"><a href="#Behavioral-Cloning-Reduction-to-supervised-learning" class="headerlink" title="Behavioral Cloning (Reduction to supervised learning ):"></a>Behavioral Cloning (Reduction to supervised learning ):</h5><p>Define P<em> = P(s|π</em>) (distribution of states visited by expert ) ( the * notation here means expert)</p>
<h5 id="The-learning-objective"><a href="#The-learning-objective" class="headerlink" title="The learning objective:"></a>The learning objective:</h5><p> apply supervised learning to minimize the error between the action provided by experts and the action provided by our policy, where L is the loss function.</p>
<script type="math/tex; mode=display">
arg min_θ E_{(s,a^*)\sim P^*} L(a^*,π_θ(s))</script><p>Note: the training data was collected by exogenously queuing from experts to provide state and action pairs. </p>
<h5 id="Two-interpretations-of-what-behavioral-cloning-is-doing"><a href="#Two-interpretations-of-what-behavioral-cloning-is-doing" class="headerlink" title="Two interpretations of what behavioral cloning is doing:"></a>Two interpretations of what behavioral cloning is doing:</h5><p>​        Assuming perfect imitation so far, learning to continue imitating perfectly </p>
<p>​        Minimize 1-step deviation error along the expert trajectories</p>
<h5 id="Compared-with-general-imitation-learning"><a href="#Compared-with-general-imitation-learning" class="headerlink" title="Compared with general imitation learning"></a>Compared with general imitation learning</h5><p>The learning objective for general imitation learning</p>
<script type="math/tex; mode=display">
arg min_θ E_{s\sim P(s|θ)} L(π^*(s),π_θ(s))</script><p>The key difference here is the distribution of states visited by expert depends on our policy or rollout instead of directly sample data exogenously, which means in behavioral cloning presume all states was sampled, but general imitation learning is not. This assumption make behavioral cloning hard to handle unseen state.</p>
<p>P(s|θ) = state distribution of π<sub>θ</sub>  </p>
<h5 id="When-to-use-behavioral-cloning"><a href="#When-to-use-behavioral-cloning" class="headerlink" title="When to use behavioral cloning"></a>When to use behavioral cloning</h5><p>Advantages: Simple and efficient</p>
<p>Use When: </p>
<p>​        1-step deviation not too bad</p>
<p>​        Learning reactive behaviors</p>
<p>​        Expert trajectories “cover” state space</p>
<p>Disadvantages </p>
<p>​        Distribution mismatch between training and testing </p>
<p>​        No long term planning</p>
<p>Don’t Use When </p>
<p>​        1-step deviations can lead to catastrophic error</p>
<p>​        Optimizing long-term objective </p>
<h5 id="Direct-policy-learning"><a href="#Direct-policy-learning" class="headerlink" title="Direct policy learning"></a>Direct policy learning</h5><p>Basic idea: construct a sequence of supervise learning problems that ideally converge the best policy  for the general imitation problem, which typically start from imitation learning.</p>
<p><img src="/2019/08/12/imitation-learning-md/dpL.png" alt="dpL"></p>
<p>If apply train a policy from the first stage and then move on the next stage and so on, the policy will probably forget what has learned before. To solve this problem, data aggregation and policy aggregation was applied, which will be discussed in sequential learning reductions. </p>
<h5 id="Interactive-Expert"><a href="#Interactive-Expert" class="headerlink" title="Interactive Expert"></a>Interactive Expert</h5><p>The human or computation, etc that is available at training time to provide feedback on state distribution induced by the policy that we are training. The expert that we can query an at state, which can be use to construct loss function to get the mismatch between our policy and expert for any state (give feedback). </p>
<h5 id="Sequential-Learning-Reductions"><a href="#Sequential-Learning-Reductions" class="headerlink" title="Sequential Learning Reductions"></a>Sequential Learning Reductions</h5><p>Start from a initial predictor: π<sub>0</sub>, which is initial expert demonstrations.</p>
<p>For m=1 (m is an index)</p>
<p>​        Collect trajectories τ via previous policy π<sub>m-1</sub> and rolling out mutiple times</p>
<p>​        Estimate state distribution P<sub>m</sub> using these trajectories (s∈τ)</p>
<p>​        For each state in the rollout, we collection interactive feedback from experts {π*(s)|s∈τ}</p>
<p>​        Do data aggregation, which means train the policy on all data or distributions that we have </p>
<p>​                        e.g. do online learning on our data to train the policy</p>
<p>​        Or do policy aggregation, train policy on current data or distribution and then blend them.</p>
<p>​                        e.g. train π’<sub>m</sub> on the data or distribution at current stage P<sub>m</sub> and blend them with                         previous trained policy, which can be expressed as below.</p>
<script type="math/tex; mode=display">
π_m=β π_m'+(1-β)π_{m-1}</script><p>​                                        and finally (M mean the final index,π<sub>0</sub> is the expert)</p>
<script type="math/tex; mode=display">
π_m=(1-β)^M π_0+β∑_{m'} (1-β)^{(M-m')}π_{m'}</script><p>​                            The ways to analyse policy aggregation :</p>
<p>​                                                    π<sub>m</sub> not much worse than π<sub>m-1</sub> </p>
<p>​                                                    The final policy π<sub>M</sub> not much worse than expert π<sub>0</sub> </p>
<h4 id="Inverse-reinforcement-learning-IRL-—-do-training-to-learn-a-good-reward-function"><a href="#Inverse-reinforcement-learning-IRL-—-do-training-to-learn-a-good-reward-function" class="headerlink" title="Inverse reinforcement learning (IRL) —- do training to learn a good reward function"></a>Inverse reinforcement learning (IRL) —- do training to learn a good reward function</h4><p>For given trajectories (or data) from expert π*</p>
<script type="math/tex; mode=display">
D=\left\{τ_1,τ_2,...\right\}=\left\{ (s_0^i,s_0^i,s_1^i,a_1^i,......)\right\} \sim π^*</script><p>The goal is to learn a reward function r* so as to maximize the reward of expert policy</p>
<script type="math/tex; mode=display">
π^*=argmax( E_π[r^*(s,a)])</script><h5 id="General-recipe-for-IRL"><a href="#General-recipe-for-IRL" class="headerlink" title="General recipe for IRL"></a>General recipe for IRL</h5><ol>
<li>Get a set of expert demonstration </li>
</ol>
<script type="math/tex; mode=display">
D=\left\{τ_1,...,τ_m\right\}</script><ol>
<li><p>Learn reward function: r<sub>θ</sub>(s<sub>t</sub>,a<sub>t</sub>)</p>
</li>
<li><p>Learn policy for given reward function with reinforcement learning</p>
</li>
<li><p>Compare the learned policy with expert to see whether the policy is good enough. If not, then repeat the same process from the second step. </p>
<p><img src="/2019/08/12/imitation-learning-md/IRL.png" alt="IRL"></p>
</li>
</ol>
<p>Relaxation of IRL</p>
<p>The first method: Instead of trying to find the optimal reward function r<em> and policy P</em>, as long as a reward function was found so that the performance of the optimal function policy with respect to that reward is not much worse than the performance of the true expert performance then “I’m happy”. </p>
<script type="math/tex; mode=display">
\operatorname*{max}\limits_{π∈Π}(E_π[r(s,a)])≥E_{π^*}[r*(s,a)]-ε</script><p>Then second method: The goal is to find policy π performing better than expert π* over a restricted class of rewards function. In this case, IRL is formulated as two player game: one player tries to find the best reward function, another player tries to maximize the difference between the trained policy and the experts.</p>
<script type="math/tex; mode=display">
\operatorname*{max}\limits_{π∈Π}(\operatorname*{min}\limits_{r∈R}(E_π[r(s,a)])-E_{π*}[r(s,a)])</script><p>Assumptions behand these two method:</p>
<ol>
<li>the model of the environment is given</li>
<li>the oracle of the reinforcement learning is given</li>
<li>The reward is linear is known features of state in actions (r(s|θ)=θ<sup>T</sup> φ(s), where ‖θ‖<sub>1</sub>=1, ‖θ‖<sub>2</sub>≤1, φ(s) is features of state in actions)</li>
</ol>
<p>How the third assumption linear reward function can help us:</p>
<p>Rewrite the reward function </p>
<script type="math/tex; mode=display">
r(s)=θ·φ(s)</script><p>The value function of a policy would be </p>
<script type="math/tex; mode=display">
V(π|s_0)=E[\sum_{π∈Π}^∞ γ^tθ·φ(s_t)|π]=θ·E[\sum_{π∈Π}^∞ γ^tφ(s_t)|π]</script><p>From the equation above, we can see that the parameter θ can be pulled out and leave a term that only depend on feature φ(s). The remain term  is also called feature expectations pf the policy π</p>
<script type="math/tex; mode=display">
μ(π)=E[\sum_{π∈Π}^∞ γ^tφ(s_t)|π]</script><p>Another way of interpreting feature expectation μ(π) is that is is the average of visited state features given a policy and if the reward is linear then finding a policy that matches the expert features imply that the policy is optimal </p>
<script type="math/tex; mode=display">
μ(π)=E[visited\, state\, feature|π]</script><script type="math/tex; mode=display">
μ(π)=μ(π^*)→V(π)=V(π^*)</script><p>so, ideally,  the reinforcement learning problem can be reduced to a feature matching problem.</p>
<p>However, since we only have limited demonstration, we cannot estimate μ(π*) exactly. To solve this problem, some solution was proposed. </p>
<h6 id="Solution-1-Find-a-policy-that-have-feature-expectations-difference-between-expert-policy-by-no-more-than-some-ε"><a href="#Solution-1-Find-a-policy-that-have-feature-expectations-difference-between-expert-policy-by-no-more-than-some-ε" class="headerlink" title="Solution 1: Find a policy that have feature expectations difference between expert policy by no more than some ε"></a>Solution 1: Find a policy that have feature expectations difference between expert policy by no more than some ε</h6><script type="math/tex; mode=display">
‖μ(π)-μ(π^*)‖_2≤ε</script><p>​    Algorithm for update reward for this ( learn reward function step in ‘General recipe for IRL section’): solve max-margin problem in each loop which maximize the margin of the expert feature from all the features that the policy we have found so far. At every time during this optimization, we can get a new reward estimated. ——I think this can view as to train a reward function that can distinguish the performance different between expert policy and our policy.</p>
<p>After some iteration with this reward-updating method, the train will stop until the error ε is less than certain value.</p>
<p>Theory has proved that it requires at most </p>
<script type="math/tex; mode=display">
O(\frac{k}{(1-γ)^2ε^2}log\frac{k}{(1-γ)ε})</script><p>iteration.</p>
<h6 id="Maximum-entropy"><a href="#Maximum-entropy" class="headerlink" title="Maximum entropy"></a>Maximum entropy</h6><p> If we want to find a policy that can explain the policy behaviour of expert, then we create any policy that match the expert’s behavior so that we can create any stochastic combination of them and the resulting policy will also satisfy the feature matching requirement. Thus, in this case we still don’t know which one is the right policy, which can be resolved by maximum entropy.</p>
<p>Suppose policy π induces distribution over trajectories P(τ), which satisfy </p>
<script type="math/tex; mode=display">
\sum_{τ}P(τ)μ(τ)=μ(π^*)</script><script type="math/tex; mode=display">
\sum_{τ}P(τ)=1</script><p>Then we should choose the one with the largest entropy (pick one the explained the data).</p>
<p><em>Maximum entropy principle: The probability distribution which best represents the current state of knowledge is the one with the largest entropy</em></p>
<p>After some mathematical derivation if the reward is linear the max entropy distribution trajectories given the reward parameters θ will depend exponentially on the inner product of  θ in the feature of trajectory τ</p>
<script type="math/tex; mode=display">
r_θ(s)=θ^\topφ(s)→P(τ|θ)∝e^{θ^\top φ(s)}</script><p>For IRL, it means the probability of a trajectories with high reward is exponentially more likely to be sampled from an expert than trajectory with low reward. </p>
<p>Then we express maximum entropy formulation as </p>
<script type="math/tex; mode=display">
P(τ|θ)=\frac{1}{Z(θ)}e^{\sum_{s_t∈τ}θ^\top φ(s)}</script><script type="math/tex; mode=display">
Z(θ)=\int e^{r(τ|θ)}dτ</script><p>We will learning the reward function for given θ by maximizing log likelihood of the observed data under the distributions </p>
<script type="math/tex; mode=display">
θ^*=\operatorname*{argmax}\limits_{θ}L(θ)=\operatorname*{argmax}\limits_{θ}\sum_{τ_i∈D}log P(τ_i|θ)</script><p>If we use gradient descent over log-likelihood, then the gradient can be expressed as </p>
<script type="math/tex; mode=display">
\nabla_θ L(θ)=\frac{1}{m}\sum_{τ_i∈D}μ(τ_i)-\sum_{s}d^θ_s φ(s)</script><p>where </p>
<script type="math/tex; mode=display">
μ(τ_i)=\frac{1}{m}\sum_{s'∈τ_i}φ(s')</script><p>is the expert state feature </p>
<p>d<sub>s</sub> is the state occupancy measure can be calculated as</p>
<script type="math/tex; mode=display">
d_{t+1,s'}=\sum_{a}\sum_{s}d_{d,s}π_θ(a|s)P(s'|s,a)</script><h4 id="Model-free-IRL"><a href="#Model-free-IRL" class="headerlink" title="Model-free IRL"></a>Model-free IRL</h4><p>The IRL discussed above is just to find the reward for given state dynamic. In this section, we will talk about the cases when both reward and state dynamic are not given.</p>
<p>One of the setting for that is to set the reward function r<sub>θ</sub>(s<sub>t</sub>, a<sub>t</sub>) as neural net.</p>
<p>The max entropy distribution trajectories formulation is </p>
<script type="math/tex; mode=display">
P(τ|θ)=\frac{1}{Z(θ)}e^{r_θ(r)}</script><p>Learn θ with max likelihood. </p>
<script type="math/tex; mode=display">
θ^*=\operatorname*{argmax}\limits_{θ}L(θ)=\operatorname*{argmax}\limits_{θ}\sum_{τ_i∈D}log P(τ_i|θ)=\operatorname*{argmax}\limits_{θ}\frac{1}{|D|}\sum_{τ_i∈D}r_θ(τ_i)-logZ(θ)</script><p>Z(θ), in this case, can be estimated by using proposal distribution q(τ) to sample trajectories D<sub>samp</sub>， which can be used for reward optimization</p>
<script type="math/tex; mode=display">
Z(θ)≈\operatorname*{arerage}\limits_{τ∈D_{samp}}(\frac{e^{r_θ(τ)}}{q(τ)})</script><p>Except reward function optimization, policy optimization is still needed to be considered. </p>
<p>In the model-free optimization process, the high level overview consists of two steps for each loop:</p>
<ol>
<li>Generate sampling distributions that are a set of trajectories not far away from the ones induced by the current policy and then combine these sampled trajectories with existing export demonstration to estimate and update reward parameters</li>
<li>update the policy with the estimated reward function</li>
</ol>
<p><img src="/2019/08/12/imitation-learning-md/IRL2.png" alt="IRL2"></p>
<h5 id="Generative-adversarial-imitation-learning-GAIL"><a href="#Generative-adversarial-imitation-learning-GAIL" class="headerlink" title="Generative adversarial imitation learning (GAIL)"></a>Generative adversarial imitation learning (GAIL)</h5><p><em>Occupancy measure</em></p>
<p>The occupancy measure discussed before is visitation frequency of state action pair</p>
<script type="math/tex; mode=display">
d^π_{sa}=visitation\;frequency \;of \;(s,a)|following\; π</script><script type="math/tex; mode=display">
d^π_{sa}=E[\sum_{t=0}^∞ γ^t(s_t=s,a_t=a)|π]</script><p>A fact about occupancy measure is we can represent value function with the reward function and occupancy measure.</p>
<script type="math/tex; mode=display">
V(π)=E_π[r(s,a)]=\sum_{s,a}r(s,a)d^π_{sa}</script><p>So apart from reduce imitation learning to feature matching, we can also reduce is to occupancy matching, which means if we can find the occupancy measure that match optimal occupancy matching, then for any reward function, the value will match the optimal one </p>
<script type="math/tex; mode=display">
d^π_{sa}=d^*_{sa}=E_π[r(s,a)]=E_{π^*}[r(s,a)]</script><p>Still same as the policy chosen issue, in this case, we still need to select the one with the maximum entropy of policy, so the problems here will become minimize the occupancy measure between expert and our policy and maximize the entropy of our policy (H(π) is the casual entropy )</p>
<script type="math/tex; mode=display">
max_π \,H(π)</script><script type="math/tex; mode=display">
distance(d^π_{sa},d^*_{sa})<ε</script><p>The final formulations for it is </p>
<script type="math/tex; mode=display">
min_π \, distance(d^π_{sa},d^*_{sa})-λH(π)</script><p>The distance here is Jensen–Shannon divergency </p>
<script type="math/tex; mode=display">
\begin{split}
distance(d^π_{sa},d^*_{sa})&=max_{D∈(0,1)^{S×A}\, }E_π[log(D(s,a))]+E_{π^*}[log(1-D(s,a))]\\
&≈D_{KL}(d^π||(d^π+d^*)/2)+D_{KL}(d^*||(d^π+d^*)/2)
\end{split}\tag{1.3}</script><p>Thus, the final equation of GAIL is</p>
<script type="math/tex; mode=display">
\operatorname*{min}\limits_{π}\,\operatorname*{max}\limits_{D∈(0,1)^{S×A}}E_π[log(D(s,a))]+E_{π^*}[log(1-D(s,a))]-λH(π)</script><p>In this case, D is the discriminator, which is </p>
<script type="math/tex; mode=display">
\operatorname*{max}\limits_{D∈(0,1)^{S×A}}E_π[log(D(s,a))]+E_{π^*}[log(1-D(s,a))]</script><p> which can be updated by the gradient of (w is the parameters of discriminator, i represent the steps )</p>
<script type="math/tex; mode=display">
\widehat{E}_{τ_i}[\nabla_w log(D(s,a))]+\widehat{E}_{τ_E}[\nabla_w log(1-D(s,a))]</script><p>and our policy π is the generator, which has the gradient of  (θ is the parameters of our policy,, i represent the steps )</p>
<script type="math/tex; mode=display">
\widehat{E}_{τ_i}[\nabla_θ log\,π_θ( a|s)Q(s,a)]-λ\nabla_θH(π_θ)</script><script type="math/tex; mode=display">
Q(\overline{s},\overline{a})=\widehat{E}_{τ_i}[log(D_{w_{w+1}}(s,a))|s_0=\overline{s}, a_0=\overline{a}]</script><p>The discriminator aim to distinguish the action from our policy and expert policy and generator aim to generate the action that is similar to the action from expert to fool the discriminator. </p>
<h4 id="comparation"><a href="#comparation" class="headerlink" title="comparation"></a>comparation</h4><p>The following is the comparation between the three main imitation learning.</p>
<p><img src="/2019/08/12/imitation-learning-md/table.png" alt="table"></p>
<h3 id="Appendix"><a href="#Appendix" class="headerlink" title="Appendix"></a>Appendix</h3><p>Kullback-Leibler divergence (KL divergence) : this can measure the difference between two distribution, which is non-symmetrical, always greater than 0, and the KL divergence of the distribution it self is. </p>
<script type="math/tex; mode=display">
KL(q||p)=\int q(x)log\frac{q(x)}{p(x)}dx</script><p>Jensen–Shannon divergence (JS divergence ) is a symmetrized and smoothed version of KL divergence. </p>
<script type="math/tex; mode=display">
JS(q||p)=\frac{1}{2}KL(q||p)+\frac{1}{2}KL(p||q)</script>
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/07/31/gazebo-and-unity-comparation/" rel="next" title="gazebo and unity comparation">
                <i class="fa fa-chevron-left"></i> gazebo and unity comparation
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Fenger</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">8</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-4"><a class="nav-link" href="#Notation-amp-Setup"><span class="nav-number">1.</span> <span class="nav-text">Notation &amp; Setup</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Behavioral-Cloning-Reduction-to-supervised-learning"><span class="nav-number">1.1.</span> <span class="nav-text">Behavioral Cloning (Reduction to supervised learning ):</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#The-learning-objective"><span class="nav-number">1.2.</span> <span class="nav-text">The learning objective:</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Two-interpretations-of-what-behavioral-cloning-is-doing"><span class="nav-number">1.3.</span> <span class="nav-text">Two interpretations of what behavioral cloning is doing:</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Compared-with-general-imitation-learning"><span class="nav-number">1.4.</span> <span class="nav-text">Compared with general imitation learning</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#When-to-use-behavioral-cloning"><span class="nav-number">1.5.</span> <span class="nav-text">When to use behavioral cloning</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Direct-policy-learning"><span class="nav-number">1.6.</span> <span class="nav-text">Direct policy learning</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Interactive-Expert"><span class="nav-number">1.7.</span> <span class="nav-text">Interactive Expert</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Sequential-Learning-Reductions"><span class="nav-number">1.8.</span> <span class="nav-text">Sequential Learning Reductions</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Inverse-reinforcement-learning-IRL-—-do-training-to-learn-a-good-reward-function"><span class="nav-number">2.</span> <span class="nav-text">Inverse reinforcement learning (IRL) —- do training to learn a good reward function</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#General-recipe-for-IRL"><span class="nav-number">2.1.</span> <span class="nav-text">General recipe for IRL</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#Solution-1-Find-a-policy-that-have-feature-expectations-difference-between-expert-policy-by-no-more-than-some-ε"><span class="nav-number">2.1.1.</span> <span class="nav-text">Solution 1: Find a policy that have feature expectations difference between expert policy by no more than some ε</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Maximum-entropy"><span class="nav-number">2.1.2.</span> <span class="nav-text">Maximum entropy</span></a></li></ol></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Model-free-IRL"><span class="nav-number">3.</span> <span class="nav-text">Model-free IRL</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Generative-adversarial-imitation-learning-GAIL"><span class="nav-number">3.1.</span> <span class="nav-text">Generative adversarial imitation learning (GAIL)</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#comparation"><span class="nav-number">4.</span> <span class="nav-text">comparation</span></a></li></ol><li class="nav-item nav-level-3"><a class="nav-link" href="#Appendix"><span class="nav-number"></span> <span class="nav-text">Appendix</span></a></li></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Fenger</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

</body>
</html>
