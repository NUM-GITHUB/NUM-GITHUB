<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="description" content="This note is mainly based on Artificial Intelligence Imitation Learning - Tutorial - 2018 ICML Notation &amp;amp; SetupSame as reinforcement learning:  State: s        Action: a       Policy:  πθ     Stat">
<meta property="og:type" content="article">
<meta property="og:title" content="imitation-learning.md">
<meta property="og:url" content="https://num-github.github.io/2019/08/12/imitation-learning-md/index.html">
<meta property="og:site_name" content="FengerBlog">
<meta property="og:description" content="This note is mainly based on Artificial Intelligence Imitation Learning - Tutorial - 2018 ICML Notation &amp;amp; SetupSame as reinforcement learning:  State: s        Action: a       Policy:  πθ     Stat">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://num-github.github.io/.io//dpL.png">
<meta property="og:image" content="https://num-github.github.io/.io//D:%5Cblog%5Csource_posts%5CIRL.png">
<meta property="og:image" content="https://num-github.github.io/.io//D:%5Cblog%5Csource_posts%5CIRL2.png">
<meta property="og:image" content="https://num-github.github.io/.io//D:%5Cblog%5Csource_posts%5Ctable.png">
<meta property="og:updated_time" content="2019-08-22T21:15:38.646Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="imitation-learning.md">
<meta name="twitter:description" content="This note is mainly based on Artificial Intelligence Imitation Learning - Tutorial - 2018 ICML Notation &amp;amp; SetupSame as reinforcement learning:  State: s        Action: a       Policy:  πθ     Stat">
<meta name="twitter:image" content="https://num-github.github.io/.io//dpL.png">
    
    
        
          
              <link rel="shortcut icon" href="/images/favicon.ico">
          
        
        
          
            <link rel="icon" type="image/png" href="/images/favicon-192x192.png" sizes="192x192">
          
        
        
          
            <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
          
        
    
    <!-- title -->
    <title>imitation-learning.md</title>
    <!-- styles -->
    <link rel="stylesheet" href="/css/style.css">
    <!-- persian styles -->
    
      <link rel="stylesheet" href="/css/rtl.css">
    
    <!-- rss -->
    
    
</head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
         
          <li><a href="/">Inici</a></li>
         
          <li><a href="/about/">Qui som</a></li>
         
          <li><a href="/archives/">Articles</a></li>
         
          <li><a href="/projects_url">Projectes</a></li>
        
      </ul>
    </span>
    <br>
    <span id="actions">
      <ul>
        
        
        <li><a class="icon" href="/2019/07/31/gazebo-and-unity-comparation/"><i class="fas fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" href="#"><i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Post Anterior</span>
      <span id="i-next" class="info" style="display:none;">Post Següent</span>
      <span id="i-top" class="info" style="display:none;">Adalt</span>
      <span id="i-share" class="info" style="display:none;">Compartir Post</span>
    </span>
    <br>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" href="http://www.facebook.com/sharer.php?u=https://num-github.github.io/2019/08/12/imitation-learning-md/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://twitter.com/share?url=https://num-github.github.io/2019/08/12/imitation-learning-md/&text=imitation-learning.md"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.linkedin.com/shareArticle?url=https://num-github.github.io/2019/08/12/imitation-learning-md/&title=imitation-learning.md"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=https://num-github.github.io/2019/08/12/imitation-learning-md/&is_video=false&description=imitation-learning.md"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=imitation-learning.md&body=Check out this article: https://num-github.github.io/2019/08/12/imitation-learning-md/"><i class="fas fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://getpocket.com/save?url=https://num-github.github.io/2019/08/12/imitation-learning-md/&title=imitation-learning.md"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://reddit.com/submit?url=https://num-github.github.io/2019/08/12/imitation-learning-md/&title=imitation-learning.md"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.stumbleupon.com/submit?url=https://num-github.github.io/2019/08/12/imitation-learning-md/&title=imitation-learning.md"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://digg.com/submit?url=https://num-github.github.io/2019/08/12/imitation-learning-md/&title=imitation-learning.md"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.tumblr.com/share/link?url=https://num-github.github.io/2019/08/12/imitation-learning-md/&name=imitation-learning.md&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-4"><a class="toc-link" href="#Notation-amp-Setup"><span class="toc-number">1.</span> <span class="toc-text">Notation &amp; Setup</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Behavioral-Cloning-Reduction-to-supervised-learning"><span class="toc-number">1.1.</span> <span class="toc-text">Behavioral Cloning (Reduction to supervised learning ):</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#The-learning-objective"><span class="toc-number">1.2.</span> <span class="toc-text">The learning objective:</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Two-interpretations-of-what-behavioral-cloning-is-doing"><span class="toc-number">1.3.</span> <span class="toc-text">Two interpretations of what behavioral cloning is doing:</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Compared-with-general-imitation-learning"><span class="toc-number">1.4.</span> <span class="toc-text">Compared with general imitation learning</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#When-to-use-behavioral-cloning"><span class="toc-number">1.5.</span> <span class="toc-text">When to use behavioral cloning</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Direct-policy-learning"><span class="toc-number">1.6.</span> <span class="toc-text">Direct policy learning</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Interactive-Expert"><span class="toc-number">1.7.</span> <span class="toc-text">Interactive Expert</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Sequential-Learning-Reductions"><span class="toc-number">1.8.</span> <span class="toc-text">Sequential Learning Reductions</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Inverse-reinforcement-learning-IRL-—-do-training-to-learn-a-good-reward-function"><span class="toc-number">2.</span> <span class="toc-text">Inverse reinforcement learning (IRL) — do training to learn a good reward function</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#General-recipe-for-IRL"><span class="toc-number">2.1.</span> <span class="toc-text">General recipe for IRL</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#Solution-1-Find-a-policy-that-have-feature-expectations-difference-between-expert-policy-by-no-more-than-some-ε"><span class="toc-number">2.1.1.</span> <span class="toc-text">Solution 1: Find a policy that have feature expectations difference between expert policy by no more than some ε</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#Maximum-entropy"><span class="toc-number">2.1.2.</span> <span class="toc-text">Maximum entropy</span></a></li></ol></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Model-free-IRL"><span class="toc-number">3.</span> <span class="toc-text">Model-free IRL</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Generative-adversarial-imitation-learning-GAIL"><span class="toc-number">3.1.</span> <span class="toc-text">Generative adversarial imitation learning (GAIL)</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#comparation"><span class="toc-number">4.</span> <span class="toc-text">comparation</span></a></li></ol><li class="toc-item toc-level-3"><a class="toc-link" href="#Appendix"><span class="toc-number"></span> <span class="toc-text">Appendix</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Basic-Results-for-Behavioral-Cloning"><span class="toc-number">0.1.</span> <span class="toc-text">Basic Results for Behavioral Cloning</span></a></li></ol></li>
    </div>
  </span>
</div>

    
    <div class="content index py4">
        
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        imitation-learning.md
    </h1>



    <div class="meta">
      <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">FengerBlog</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2019-08-12T19:25:12.000Z" itemprop="datePublished">2019-08-12</time>
        
      
    </div>


      

      

    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <p><em>This note is mainly based on <a href="https://www.youtube.com/watch?v=6rZTaboSY4k&feature=youtu.be" target="_blank" rel="noopener">Artificial Intelligence Imitation Learning - Tutorial - 2018 ICML</a></em></p>
<h4 id="Notation-amp-Setup"><a href="#Notation-amp-Setup" class="headerlink" title="Notation &amp; Setup"></a>Notation &amp; Setup</h4><p>Same as reinforcement learning: </p>
<p>State: s       </p>
<p>Action: a      </p>
<p>Policy:  π<sub>θ</sub>    </p>
<p>State Dynamics (or environment): P(s’|s,a) —- given current state &amp; actions, what is the next state.</p>
<p>Rollout: sequential execute π(s<sub>0</sub>) on an initial state —produce trajectory τ=(s<sub>0</sub>, a<sub>0</sub>, s<sub>1</sub>, a<sub>1</sub> ……)</p>
<p>P(τ|π): distribution of trajectories induced by a policy— depend on the randomness of policy and randomness of state dynamics</p>
<p>​        1. Sample s<sub>0</sub> from P<sub>0</sub> (distribution over initial state), initialize t=1.</p>
<p>​        2. Sample action a<sub>i</sub> from π(s<sub>t-1</sub>)</p>
<p>​        3. Sample next state s<sub>t</sub> from applying a<sub>t</sub> to s<sub> t-1</sub> (require access to environment)</p>
<p>​        4. Repeat from Step 2 with t=t+1</p>
<p>P(s|π): distribution of states induced by a policy</p>
<p>​        Let P<sub>t</sub> (s|π) denote distribution over t-th state</p>
<p>​        P(s|π)=(1/T)∑P<sub>t</sub>(s|π) </p>
<h5 id="Behavioral-Cloning-Reduction-to-supervised-learning"><a href="#Behavioral-Cloning-Reduction-to-supervised-learning" class="headerlink" title="Behavioral Cloning (Reduction to supervised learning ):"></a>Behavioral Cloning (Reduction to supervised learning ):</h5><p>Define P* = P(s|π*) (distribution of states visited by expert ) ( the * notation here means expert)</p>
<h5 id="The-learning-objective"><a href="#The-learning-objective" class="headerlink" title="The learning objective:"></a>The learning objective:</h5><p> apply supervised learning to minimize the error between the action provided by experts and the action provided by our policy, where L is the loss function.<br>$$<br>arg min_θ E_{(s,a^<em>)\sim P^</em>} L(a^*,π_θ(s))<br>$$<br>Note: the training data was collected by exogenously queuing from experts to provide state and action pairs. </p>
<h5 id="Two-interpretations-of-what-behavioral-cloning-is-doing"><a href="#Two-interpretations-of-what-behavioral-cloning-is-doing" class="headerlink" title="Two interpretations of what behavioral cloning is doing:"></a>Two interpretations of what behavioral cloning is doing:</h5><p>​        Assuming perfect imitation so far, learning to continue imitating perfectly </p>
<p>​        Minimize 1-step deviation error along the expert trajectories</p>
<h5 id="Compared-with-general-imitation-learning"><a href="#Compared-with-general-imitation-learning" class="headerlink" title="Compared with general imitation learning"></a>Compared with general imitation learning</h5><p>The learning objective for general imitation learning<br>$$<br>arg min_θ E_{s\sim P(s|θ)} L(π^*(s),π_θ(s))<br>$$<br>The key difference here is the distribution of states visited by expert depends on our policy or rollout instead of directly sample data exogenously, which means in behavioral cloning presume all states was sampled, but general imitation learning is not. This assumption make behavioral cloning hard to handle unseen state.</p>
<p>P(s|θ) = state distribution of π<sub>θ</sub>  </p>
<h5 id="When-to-use-behavioral-cloning"><a href="#When-to-use-behavioral-cloning" class="headerlink" title="When to use behavioral cloning"></a>When to use behavioral cloning</h5><p>Advantages: Simple and efficient</p>
<p>Use When: </p>
<p>​        1-step deviation not too bad</p>
<p>​        Learning reactive behaviors</p>
<p>​        Expert trajectories “cover” state space</p>
<p>Disadvantages </p>
<p>​        Distribution mismatch between training and testing </p>
<p>​        No long term planning</p>
<p>Don’t Use When </p>
<p>​        1-step deviations can lead to catastrophic error</p>
<p>​        Optimizing long-term objective </p>
<h5 id="Direct-policy-learning"><a href="#Direct-policy-learning" class="headerlink" title="Direct policy learning"></a>Direct policy learning</h5><p>Basic idea: construct a sequence of supervise learning problems that ideally converge the best policy  for the general imitation problem, which typically start from imitation learning.</p>
<p><img src="/.io//dpL.png" alt="dpL"></p>
<p>If apply train a policy from the first stage and then move on the next stage and so on, the policy will probably forget what has learned before. To solve this problem, data aggregation and policy aggregation was applied, which will be discussed in sequential learning reductions. </p>
<h5 id="Interactive-Expert"><a href="#Interactive-Expert" class="headerlink" title="Interactive Expert"></a>Interactive Expert</h5><p>The human or computation, etc that is available at training time to provide feedback on state distribution induced by the policy that we are training. The expert that we can query an at state, which can be use to construct loss function to get the mismatch between our policy and expert for any state (give feedback). </p>
<h5 id="Sequential-Learning-Reductions"><a href="#Sequential-Learning-Reductions" class="headerlink" title="Sequential Learning Reductions"></a>Sequential Learning Reductions</h5><p>Start from a initial predictor: π<sub>0</sub>, which is initial expert demonstrations.</p>
<p>For m=1 (m is an index)</p>
<p>​        Collect trajectories τ via previous policy π<sub>m-1</sub> and rolling out mutiple times</p>
<p>​        Estimate state distribution P<sub>m</sub> using these trajectories (s∈τ)</p>
<p>​        For each state in the rollout, we collection interactive feedback from experts {π*(s)|s∈τ}</p>
<p>​        Do data aggregation, which means train the policy on all data or distributions that we have </p>
<p>​                        e.g. do online learning on our data to train the policy</p>
<p>​        Or do policy aggregation, train policy on current data or distribution and then blend them.</p>
<p>​                        e.g. train π’<sub>m</sub> on the data or distribution at current stage P<sub>m</sub> and blend them with                         previous trained policy, which can be expressed as below.<br>$$<br>π<em>m=β π_m’+(1-β)π</em>{m-1}<br>$$<br>​                                        and finally (M mean the final index,π<sub>0</sub> is the expert)<br>$$<br>π<em>m=(1-β)^M π_0+β∑</em>{m’} (1-β)^{(M-m’)}π_{m’}<br>$$<br>​                            The ways to analyse policy aggregation :</p>
<p>​                                                    π<sub>m</sub> not much worse than π<sub>m-1</sub> </p>
<p>​                                                    The final policy π<sub>M</sub> not much worse than expert π<sub>0</sub> </p>
<h4 id="Inverse-reinforcement-learning-IRL-—-do-training-to-learn-a-good-reward-function"><a href="#Inverse-reinforcement-learning-IRL-—-do-training-to-learn-a-good-reward-function" class="headerlink" title="Inverse reinforcement learning (IRL) — do training to learn a good reward function"></a>Inverse reinforcement learning (IRL) — do training to learn a good reward function</h4><p>For given trajectories (or data) from expert π*
$$<br>D=\left{τ_1,τ_2,…\right}=\left{ (s_0^i,s_0^i,s_1^i,a_1^i,……)\right} \sim π^*
$$<br>The goal is to learn a reward function r* so as to maximize the reward of expert policy<br>$$<br>π^<em>=argmax( E_π[r^</em>(s,a)])<br>$$</p>
<h5 id="General-recipe-for-IRL"><a href="#General-recipe-for-IRL" class="headerlink" title="General recipe for IRL"></a>General recipe for IRL</h5><ol>
<li>Get a set of expert demonstration </li>
</ol>
<p>$$<br>D=\left{τ_1,…,τ_m\right}<br>$$</p>
<ol start="2">
<li><p>Learn reward function: r<sub>θ</sub>(s<sub>t</sub>,a<sub>t</sub>)</p>
</li>
<li><p>Learn policy for given reward function with reinforcement learning</p>
</li>
<li><p>Compare the learned policy with expert to see whether the policy is good enough. If not, then repeat the same process from the second step. </p>
<p><img src="/.io//D:%5Cblog%5Csource_posts%5CIRL.png" alt="IRL"></p>
</li>
</ol>
<p>Relaxation of IRL</p>
<p>The first method: Instead of trying to find the optimal reward function r* and policy P<em>, as long as a reward function was found so that the performance of the optimal function policy with respect to that reward is not much worse than the performance of the true expert performance then “I’m happy”.<br>$$<br>\operatorname</em>{max}\limits_{π∈Π}(E_π[r(s,a)])≥E_{π^<em>}[r</em>(s,a)]-ε<br>$$<br>Then second method: The goal is to find policy π performing better than expert π* over a restricted class of rewards function. In this case, IRL is formulated as two player game: one player tries to find the best reward function, another player tries to maximize the difference between the trained policy and the experts.<br>$$<br>\operatorname<em>{max}\limits_{π∈Π}(\operatorname</em>{min}\limits_{r∈R}(E_π[r(s,a)])-E_{π*}[r(s,a)])<br>$$<br>Assumptions behand these two method:</p>
<ol>
<li>the model of the environment is given</li>
<li>the oracle of the reinforcement learning is given</li>
<li>The reward is linear is known features of state in actions (r(s|θ)=θ<sup>T</sup> φ(s), where ‖θ‖<sub>1</sub>=1, ‖θ‖<sub>2</sub>≤1, φ(s) is features of state in actions)</li>
</ol>
<p>How the third assumption linear reward function can help us:</p>
<p>Rewrite the reward function<br>$$<br>r(s)=θ·φ(s)<br>$$<br>The value function of a policy would be<br>$$<br>V(π|s_0)=E[\sum_{π∈Π}^∞ γ^tθ·φ(s_t)|π]=θ·E[\sum_{π∈Π}^∞ γ^tφ(s_t)|π]<br>$$<br>From the equation above, we can see that the parameter θ can be pulled out and leave a term that only depend on feature φ(s). The remain term  is also called feature expectations pf the policy π<br>$$<br>μ(π)=E[\sum_{π∈Π}^∞ γ^tφ(s_t)|π]<br>$$<br>Another way of interpreting feature expectation μ(π) is that is is the average of visited state features given a policy and if the reward is linear then finding a policy that matches the expert features imply that the policy is optimal<br>$$<br>μ(π)=E[visited, state, feature|π]<br>$$</p>
<p>$$<br>μ(π)=μ(π^<em>)→V(π)=V(π^</em>)
$$</p>
<p>so, ideally,  the reinforcement learning problem can be reduced to a feature matching problem.</p>
<p>However, since we only have limited demonstration, we cannot estimate μ(π*) exactly. To solve this problem, some solution was proposed. </p>
<h6 id="Solution-1-Find-a-policy-that-have-feature-expectations-difference-between-expert-policy-by-no-more-than-some-ε"><a href="#Solution-1-Find-a-policy-that-have-feature-expectations-difference-between-expert-policy-by-no-more-than-some-ε" class="headerlink" title="Solution 1: Find a policy that have feature expectations difference between expert policy by no more than some ε"></a>Solution 1: Find a policy that have feature expectations difference between expert policy by no more than some ε</h6><p>$$<br>‖μ(π)-μ(π^*)‖_2≤ε<br>$$<br>​    Algorithm for update reward for this ( learn reward function step in ‘General recipe for IRL section’): solve max-margin problem in each loop which maximize the margin of the expert feature from all the features that the policy we have found so far. At every time during this optimization, we can get a new reward estimated. —-I think this can view as to train a reward function that can distinguish the performance different between expert policy and our policy.</p>
<p>After some iteration with this reward-updating method, the train will stop until the error ε is less than certain value.</p>
<p>Theory has proved that it requires at most<br>$$<br>O(\frac{k}{(1-γ)^2ε^2}log\frac{k}{(1-γ)ε})<br>$$<br>iteration.</p>
<h6 id="Maximum-entropy"><a href="#Maximum-entropy" class="headerlink" title="Maximum entropy"></a>Maximum entropy</h6><p> If we want to find a policy that can explain the policy behaviour of expert, then we create any policy that match the expert’s behavior so that we can create any stochastic combination of them and the resulting policy will also satisfy the feature matching requirement. Thus, in this case we still don’t know which one is the right policy, which can be resolved by maximum entropy.</p>
<p>Suppose policy π induces distribution over trajectories P(τ), which satisfy<br>$$<br>\sum_{τ}P(τ)μ(τ)=μ(π^*)<br>$$</p>
<p>$$<br>\sum_{τ}P(τ)=1<br>$$</p>
<p>Then we should choose the one with the largest entropy (pick one the explained the data).</p>
<p><em>Maximum entropy principle: The probability distribution which best represents the current state of knowledge is the one with the largest entropy</em></p>
<p>After some mathematical derivation if the reward is linear the max entropy distribution trajectories given the reward parameters θ will depend exponentially on the inner product of  θ in the feature of trajectory τ<br>$$<br>r_θ(s)=θ^\topφ(s)→P(τ|θ)∝e^{θ^\top φ(s)}<br>$$<br>For IRL, it means the probability of a trajectories with high reward is exponentially more likely to be sampled from an expert than trajectory with low reward. </p>
<p>Then we express maximum entropy formulation as<br>$$<br>P(τ|θ)=\frac{1}{Z(θ)}e^{\sum_{s_t∈τ}θ^\top φ(s)}<br>$$</p>
<p>$$<br>Z(θ)=\int e^{r(τ|θ)}dτ<br>$$</p>
<p>We will learning the reward function for given θ by maximizing log likelihood of the observed data under the distributions<br>$$<br>θ^<em>=\operatorname</em>{argmax}\limits_{θ}L(θ)=\operatorname*{argmax}\limits_{θ}\sum_{τ<em>i∈D}log P(τ_i|θ)<br>$$<br>If we use gradient descent over log-likelihood, then the gradient can be expressed as<br>$$<br>\nabla_θ L(θ)=\frac{1}{m}\sum</em>{τ<em>i∈D}μ(τ_i)-\sum</em>{s}d^θ<em>s φ(s)<br>$$<br>where<br>$$<br>μ(τ_i)=\frac{1}{m}\sum</em>{s’∈τ_i}φ(s’)<br>$$<br>is the expert state feature </p>
<p>d<sub>s</sub> is the state occupancy measure can be calculated as<br>$$<br>d_{t+1,s’}=\sum_{a}\sum_{s}d_{d,s}π_θ(a|s)P(s’|s,a)<br>$$</p>
<h4 id="Model-free-IRL"><a href="#Model-free-IRL" class="headerlink" title="Model-free IRL"></a>Model-free IRL</h4><p>The IRL discussed above is just to find the reward for given state dynamic. In this section, we will talk about the cases when both reward and state dynamic are not given.</p>
<p>One of the setting for that is to set the reward function r<sub>θ</sub>(s<sub>t</sub>, a<sub>t</sub>) as neural net.</p>
<p>The max entropy distribution trajectories formulation is </p>
<p>$$<br>P(τ|θ)=\frac{1}{Z(θ)}e^{r_θ(r)}<br>$$<br>Learn θ with max likelihood.<br>$$<br>θ^<em>=\operatorname</em>{argmax}\limits_{θ}L(θ)=\operatorname<em>{argmax}\limits_{θ}\sum_{τ_i∈D}log P(τ_i|θ)=\operatorname</em>{argmax}\limits_{θ}\frac{1}{|D|}\sum_{τ_i∈D}r_θ(τ_i)-logZ(θ)<br>$$</p>
<p>Z(θ), in this case, can be estimated by using proposal distribution q(τ) to sample trajectories D<sub>samp</sub>， which can be used for reward optimization<br>$$<br>Z(θ)≈\operatorname*{arerage}\limits_{τ∈D_{samp}}(\frac{e^{r_θ(τ)}}{q(τ)})<br>$$<br>Except reward function optimization, policy optimization is still needed to be considered. </p>
<p>In the model-free optimization process, the high level overview consists of two steps for each loop:</p>
<ol>
<li>Generate sampling distributions that are a set of trajectories not far away from the ones induced by the current policy and then combine these sampled trajectories with existing export demonstration to estimate and update reward parameters</li>
<li>update the policy with the estimated reward function</li>
</ol>
<p><img src="/.io//D:%5Cblog%5Csource_posts%5CIRL2.png" alt="IRL2"></p>
<h5 id="Generative-adversarial-imitation-learning-GAIL"><a href="#Generative-adversarial-imitation-learning-GAIL" class="headerlink" title="Generative adversarial imitation learning (GAIL)"></a>Generative adversarial imitation learning (GAIL)</h5><p><em>Occupancy measure</em></p>
<p>The occupancy measure discussed before is visitation frequency of state action pair<br>$$<br>d^π_{sa}=visitation;frequency ;of ;(s,a)|following; π<br>$$</p>
<p>$$<br>d^π<em>{sa}=E[\sum</em>{t=0}^∞ γ^t(s_t=s,a_t=a)|π]<br>$$</p>
<p>A fact about occupancy measure is we can represent value function with the reward function and occupancy measure.<br>$$<br>V(π)=E_π[r(s,a)]=\sum_{s,a}r(s,a)d^π<em>{sa}<br>$$<br>So apart from reduce imitation learning to feature matching, we can also reduce is to occupancy matching, which means if we can find the occupancy measure that match optimal occupancy matching, then for any reward function, the value will match the optimal one<br>$$<br>d^π</em>{sa}=d^<em><em>{sa}=E_π[r(s,a)]=E</em>{π^</em>}[r(s,a)]<br>$$<br>Still same as the policy chosen issue, in this case, we still need to select the one with the maximum entropy of policy, so the problems here will become minimize the occupancy measure between expert and our policy and maximize the entropy of our policy (H(π) is the casual entropy )<br>$$<br>max_π ,H(π)<br>$$</p>
<p>$$<br>distance(d^π<em>{sa},d^*</em>{sa})&lt;ε<br>$$</p>
<p>The final formulations for it is<br>$$<br>min_π , distance(d^π<em>{sa},d^*</em>{sa})-λH(π)<br>$$<br>The distance here is Jensen–Shannon divergency<br>$$<br>\begin{split}<br>distance(d^π<em>{sa},d^*</em>{sa})&amp;=max_{D∈(0,1)^{S×A}, }E_π[log(D(s,a))]+E_{π^<em>}[log(1-D(s,a))]\<br>&amp;≈D_{KL}(d^π||(d^π+d^</em>)/2)+D_{KL}(d^<em>||(d^π+d^</em>)/2)<br>\end{split}\tag{1.3}<br>$$<br>Thus, the final equation of GAIL is<br>$$<br>\operatorname<em>{min}\limits_{π},\operatorname</em>{max}\limits_{D∈(0,1)^{S×A}}E_π[log(D(s,a))]+E_{π^<em>}[log(1-D(s,a))]-λH(π)<br>$$<br>In this case, D is the discriminator, which is<br>$$<br>\operatorname</em>{max}\limits_{D∈(0,1)^{S×A}}E_π[log(D(s,a))]+E_{π^*}[log(1-D(s,a))]<br>$$<br> which can be updated by the gradient of (w is the parameters of discriminator, i represent the steps )<br>$$<br>\widehat{E}<em>{τ_i}[\nabla_w log(D(s,a))]+\widehat{E}</em>{τ<em>E}[\nabla_w log(1-D(s,a))]<br>$$<br>and our policy π is the generator, which has the gradient of  (θ is the parameters of our policy,, i represent the steps )<br>$$<br>\widehat{E}</em>{τ_i}[\nabla_θ log,π_θ( a|s)Q(s,a)]-λ\nabla_θH(π_θ)<br>$$</p>
<p>$$<br>Q(\overline{s},\overline{a})=\widehat{E}<em>{τ_i}[log(D</em>{w_{w+1}}(s,a))|s_0=\overline{s}, a_0=\overline{a}]<br>$$</p>
<p>The discriminator aim to distinguish the action from our policy and expert policy and generator aim to generate the action that is similar to the action from expert to fool the discriminator. </p>
<h4 id="comparation"><a href="#comparation" class="headerlink" title="comparation"></a>comparation</h4><p>The following is the comparation between the three main imitation learning.</p>
<p><img src="/.io//D:%5Cblog%5Csource_posts%5Ctable.png" alt="table"></p>
<h3 id="Appendix"><a href="#Appendix" class="headerlink" title="Appendix"></a>Appendix</h3><p>Kullback-Leibler divergence (KL divergence) : this can measure the difference between two distribution, which is non-symmetrical, always greater than 0, and the KL divergence of the distribution it self is 0.<br>$$<br>KL(q||p)=\int q(x)log\frac{q(x)}{p(x)}dx<br>$$<br>Jensen–Shannon divergence (JS divergence ) is a symmetrized and smoothed version of KL divergence. </p>
<p>$$<br>JS(q||p)=\frac{1}{2}KL(q||p)+\frac{1}{2}KL(p||q)<br>$$</p>
<p>&lt;! –</p>
<h5 id="Basic-Results-for-Behavioral-Cloning"><a href="#Basic-Results-for-Behavioral-Cloning" class="headerlink" title="Basic Results for Behavioral Cloning"></a>Basic Results for Behavioral Cloning</h5><p>Suppose the training error between our policy’s action and expert’s action is ε</p>
<p>–&gt;</p>

  </div>
</article>



        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">Inici</a></li>
         
          <li><a href="/about/">Qui som</a></li>
         
          <li><a href="/archives/">Articles</a></li>
         
          <li><a href="/projects_url">Projectes</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-4"><a class="toc-link" href="#Notation-amp-Setup"><span class="toc-number">1.</span> <span class="toc-text">Notation &amp; Setup</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Behavioral-Cloning-Reduction-to-supervised-learning"><span class="toc-number">1.1.</span> <span class="toc-text">Behavioral Cloning (Reduction to supervised learning ):</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#The-learning-objective"><span class="toc-number">1.2.</span> <span class="toc-text">The learning objective:</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Two-interpretations-of-what-behavioral-cloning-is-doing"><span class="toc-number">1.3.</span> <span class="toc-text">Two interpretations of what behavioral cloning is doing:</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Compared-with-general-imitation-learning"><span class="toc-number">1.4.</span> <span class="toc-text">Compared with general imitation learning</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#When-to-use-behavioral-cloning"><span class="toc-number">1.5.</span> <span class="toc-text">When to use behavioral cloning</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Direct-policy-learning"><span class="toc-number">1.6.</span> <span class="toc-text">Direct policy learning</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Interactive-Expert"><span class="toc-number">1.7.</span> <span class="toc-text">Interactive Expert</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Sequential-Learning-Reductions"><span class="toc-number">1.8.</span> <span class="toc-text">Sequential Learning Reductions</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Inverse-reinforcement-learning-IRL-—-do-training-to-learn-a-good-reward-function"><span class="toc-number">2.</span> <span class="toc-text">Inverse reinforcement learning (IRL) — do training to learn a good reward function</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#General-recipe-for-IRL"><span class="toc-number">2.1.</span> <span class="toc-text">General recipe for IRL</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#Solution-1-Find-a-policy-that-have-feature-expectations-difference-between-expert-policy-by-no-more-than-some-ε"><span class="toc-number">2.1.1.</span> <span class="toc-text">Solution 1: Find a policy that have feature expectations difference between expert policy by no more than some ε</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#Maximum-entropy"><span class="toc-number">2.1.2.</span> <span class="toc-text">Maximum entropy</span></a></li></ol></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Model-free-IRL"><span class="toc-number">3.</span> <span class="toc-text">Model-free IRL</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Generative-adversarial-imitation-learning-GAIL"><span class="toc-number">3.1.</span> <span class="toc-text">Generative adversarial imitation learning (GAIL)</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#comparation"><span class="toc-number">4.</span> <span class="toc-text">comparation</span></a></li></ol><li class="toc-item toc-level-3"><a class="toc-link" href="#Appendix"><span class="toc-number"></span> <span class="toc-text">Appendix</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Basic-Results-for-Behavioral-Cloning"><span class="toc-number">0.1.</span> <span class="toc-text">Basic Results for Behavioral Cloning</span></a></li></ol></li>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" href="http://www.facebook.com/sharer.php?u=https://num-github.github.io/2019/08/12/imitation-learning-md/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://twitter.com/share?url=https://num-github.github.io/2019/08/12/imitation-learning-md/&text=imitation-learning.md"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.linkedin.com/shareArticle?url=https://num-github.github.io/2019/08/12/imitation-learning-md/&title=imitation-learning.md"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=https://num-github.github.io/2019/08/12/imitation-learning-md/&is_video=false&description=imitation-learning.md"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=imitation-learning.md&body=Check out this article: https://num-github.github.io/2019/08/12/imitation-learning-md/"><i class="fas fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://getpocket.com/save?url=https://num-github.github.io/2019/08/12/imitation-learning-md/&title=imitation-learning.md"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://reddit.com/submit?url=https://num-github.github.io/2019/08/12/imitation-learning-md/&title=imitation-learning.md"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.stumbleupon.com/submit?url=https://num-github.github.io/2019/08/12/imitation-learning-md/&title=imitation-learning.md"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://digg.com/submit?url=https://num-github.github.io/2019/08/12/imitation-learning-md/&title=imitation-learning.md"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.tumblr.com/share/link?url=https://num-github.github.io/2019/08/12/imitation-learning-md/&name=imitation-learning.md&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden="true"></i> Menú</a>
        <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> Compartir</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> Cap amunt</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy; 2019 Fenger
  </div>
  <div class="footer-right">
    <nav>
      <ul>
         
          <li><a href="/">Inici</a></li>
         
          <li><a href="/about/">Qui som</a></li>
         
          <li><a href="/archives/">Articles</a></li>
         
          <li><a href="/projects_url">Projectes</a></li>
        
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
<link rel="stylesheet" href="/lib/justified-gallery/css/justifiedGallery.min.css">

    <!-- jquery -->
<script src="/lib/jquery/jquery.min.js"></script>
<script src="/lib/justified-gallery/js/jquery.justifiedGallery.min.js"></script>
<!-- clipboard -->

  <script src="/lib/clipboard/clipboard.min.js"></script>
  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"Copy to clipboard!\">";
    btn += '<i class="far fa-clone"></i>';
    btn += '</span>'; 
    // mount it!
    $(".highlight .code pre").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      target: function(trigger) {
        return trigger.nextElementSibling;
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "Copied!");
      e.clearSelection();
    })
  })
  </script>

<script src="/js/main.js"></script>
<!-- search -->

<!-- Google Analytics -->

<!-- Baidu Analytics -->

<!-- Disqus Comments -->


</body>
</html>
