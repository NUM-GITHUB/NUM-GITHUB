<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title></title>
      <link href="/2019/06/30/Problems-for-using-gpu-for-Tensorflow-or-Keras-network-on-linux/"/>
      <url>/2019/06/30/Problems-for-using-gpu-for-Tensorflow-or-Keras-network-on-linux/</url>
      
        <content type="html"><![CDATA[<h1 id="Problems-for-using-gpu-for-Tensorflow-or-Keras-network-on-linux-tensorflow-can-only-see-CPU-Could-not-dlopen-library-‘libcuda-so-1’-failed-call-to-cuInit-UNKNOWN-ERROR-303"><a href="#Problems-for-using-gpu-for-Tensorflow-or-Keras-network-on-linux-tensorflow-can-only-see-CPU-Could-not-dlopen-library-‘libcuda-so-1’-failed-call-to-cuInit-UNKNOWN-ERROR-303" class="headerlink" title="Problems for using gpu for Tensorflow or Keras network on linux: tensorflow can only see CPU: Could not dlopen library ‘libcuda.so.1’;  failed call to cuInit: UNKNOWN ERROR (303);"></a>Problems for using gpu for Tensorflow or Keras network on linux: tensorflow can only see CPU: Could not dlopen library ‘libcuda.so.1’;  failed call to cuInit: UNKNOWN ERROR (303);</h1><p>The problem that I meet here is that when using an GPU cluster, my code ‘’with tf.device(‘/gpu:0’):’’ cannot run. If you meet the same problem, you can firstly run the following code:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">'CUDA_VISIBLE_DEVICES'</span>] = <span class="string">"0"</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.python.client <span class="keyword">import</span> device_lib</span><br><span class="line">print(device_lib.list_local_devices())</span><br></pre></td></tr></table></figure><p>If the code above cannot show any information about your GPU, then you need to check the cuda and cudnn that you installed. The version must meet your tensorflow version: <a href="https://www.tensorflow.org/install/source#tested_build_configurations" target="_blank" rel="noopener">https://www.tensorflow.org/install/source#tested_build_configurations</a></p><p>If you don’t have cude or cudnn, you can following the steps of AndrewPt in <a href="https://stackoverflow.com/questions/41402409/tensorflow-doesnt-seem-to-see-my-gpu" target="_blank" rel="noopener">https://stackoverflow.com/questions/41402409/tensorflow-doesnt-seem-to-see-my-gpu</a> to install them (but it is windows, be careful for you linux, you need to down different documents)</p><p>Then check whether you LD_LIBRARY_PATH contain the path for right cuda path, the path mush contain ‘libcuda.so.1’ ( for me it is in ‘’/home/.usr/local/cuda-10.0/lib64/stubs’), </p><p>and  ‘libcuda.so.XX.X (XX.X is your cuda version for me it is in /home/.usr/local/cuda-10.0/lib64)</p><p>Then, you must also ensure the cudnn file that contain libcudnn.so.7 is in LD_LIBRARY_PATH. For this step, you need to download cudnn. For me, I was download cudnn 7.4 for cuda 10.0 in <a href="https://developer.nvidia.com/compute/machine-learning/cudnn/secure/v7.4.1.5/prod/10.0_20181108/RHEL7_3-x64/libcudnn7-7.4.1.5-1.cuda10.0.x86_64.rpm" target="_blank" rel="noopener">cuDNN Runtime Library for RedHat/Centos 7.3 (RPM)</a> (libcudnn7-7.4.1.5-1.cuda10.0.x86_64), and there is libcudnn.so.7 in \libcudnn7-7.4.1.5-1.cuda10.0.x86_64\usr\lib64 and you can also see l ibcudnn.so.7.4.1 in it. </p><p>Extract and copy this to your linux and then, you need to delete  libcudnn.so.7, the run the following code to generate it </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ln -s libcudnn.so.7.4.1 libcudnn.so.7</span><br><span class="line">ln -s libcudnn.so.7 libcudnn.so </span><br><span class="line">ldconfig</span><br></pre></td></tr></table></figure><p>You can see both libcudnn.so.7.4.1 and libcudnn.so.7 is in the file. You can add it to  LD_LIBRARY_PATH and then run </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">'CUDA_VISIBLE_DEVICES'</span>] = <span class="string">"0"</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.python.client <span class="keyword">import</span> device_lib</span><br><span class="line">print(device_lib.list_local_devices())</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">The tensorflow should see the gpu without an error</span><br></pre></td></tr></table></figure><p>2019-06-30 14:24:17.432330: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties:<br>name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582<br>pciBusID: 0000:db:00.0<br>2019-06-30 14:24:17.432437: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0<br>2019-06-30 14:24:17.432473: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0<br>2019-06-30 14:24:17.432502: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0<br>2019-06-30 14:24:17.432529: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0<br>2019-06-30 14:24:17.432556: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0<br>2019-06-30 14:24:17.432582: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0<br>2019-06-30 14:24:17.432610: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7<br>2019-06-30 14:24:17.478617: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0<br>2019-06-30 14:24:17.478691: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:<br>2019-06-30 14:24:17.478715: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0<br>2019-06-30 14:24:17.478734: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N<br>2019-06-30 14:24:17.498562: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11439 MB memory) -&gt; physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:db:00.0, compute capability: 6.1)<br>[name: “/device:CPU:0”<br>device_type: “CPU”<br>memory_limit: 268435456<br>locality {<br>}incarnation: 7765454328739896397<br>, name: “/device:XLA_GPU:0”<br>device_type: “XLA_GPU”<br>memory_limit: 17179869184<br>locality {<br>}incarnation: 14436236814517186873<br>physical_device_desc: “device: XLA_GPU device”<br>, name: “/device:XLA_CPU:0”<br>device_type: “XLA_CPU”<br>memory_limit: 17179869184<br>locality {<br>}incarnation: 10506527593758283137<br>physical_device_desc: “device: XLA_CPU device”<br>, name: “/device:GPU:0”<br>device_type: “GPU”<br>memory_limit: 11994670695<br>locality {<br>  bus_id: 2<br>  numa_node: 1<br>  links {<br>  }<br>}incarnation: 9840900236876381126<br>physical_device_desc: “device: 0, name: TITAN Xp, pci bus id: 0000:db:00.0, compute capability: 6.1”<br>]</p><pre><code></code></pre>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Distribution related function in Seaborn</title>
      <link href="/2019/06/10/seaborn-1/"/>
      <url>/2019/06/10/seaborn-1/</url>
      
        <content type="html"><![CDATA[<h1 id="Some-useful-distribution-related-function-in-Seaborn"><a href="#Some-useful-distribution-related-function-in-Seaborn" class="headerlink" title="Some useful distribution related function in Seaborn"></a>Some useful distribution related function in Seaborn</h1><p>*<font size="1" color="gray">I recently wrote some code for training some data with ML algorithm and I found that some functions are pretty used for data analysis</font>*</p><details open><summary><font size="4">  distplot() </font></summary><p>distplot() function are able to show univariate distribution. For more detailed information, click <a href="https://seaborn.pydata.org/tutorial/distributions.html#distribution-tutorial" target="_blank" rel="noopener"> here</a>. </p><p>*<font size="2" color="gray">   If you want to know a distribution of a numerical feature, you can try this.</font>*</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import seaborn as sns </span><br><span class="line">sns.distplot(Input[0],label=&apos;Input&apos;,color=&apos;g&apos;)</span><br></pre></td></tr></table></figure><h4 id="The-commonly-used-parameters"><a href="#The-commonly-used-parameters" class="headerlink" title="The commonly used parameters :"></a>The commonly used parameters :</h4><ul><li>The input data </li><li>label: It will show on legend once you call <strong>plt.legend()</strong>. </li><li>Color: The color of the histogram and lines <em>(parameters are similar to matlab and the default color for first line is blue)</em>.</li></ul><p>You can draw distribution of one dimensional data by this with matplotlib.pyplot.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import seaborn as sns </span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">plt.figure()</span><br><span class="line"># Sometimes you may need to transpose(.T) your numpy array </span><br><span class="line">sns.distplot(fail_data[0],label=&apos;fail_data&apos;,color=&apos;g&apos;)</span><br><span class="line">plt.legend();</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>fail_data[0] is the first dimension of my data. The Result looks like that.</p><div align="center"><img src="https://num-github.github.io/2019/06/10/seaborn-1/chrome_2019-06-10_00-12-30.png" height="300" width="400"></div><p>Your image could be quite different, but both <strong>histogram</strong> and  <a href="https://en.wikipedia.org/wiki/Kernel_density_estimation" target="_blank" rel="noopener"> kernel density estimate (KDE)</a> line .</p><p>You could choose to remove histogram by setting <strong><font color="blue">hist=False</font></strong> or KDE by <strong><font color="blue">kde=False</font></strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plt.figure()</span><br><span class="line">sns.distplot(hist=False,fail_data[0],label=&apos;fail_data&apos;,color=&apos;g&apos;)</span><br><span class="line">plt.legend();</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><div align="center"><img src="https://num-github.github.io/2019/06/10/seaborn-1/chrome_2019-06-10_00-37-08.png" height="300" width="400"></div><p>If you want to draw more than one distribution on same figure, just add anther above plt.show()</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plt.figure()</span><br><span class="line">sns.distplot(scuss_data[2],label=&apos;scuss_data&apos;)</span><br><span class="line">sns.distplot(fail_data[2],color=&apos;g&apos;,label=&apos;fail_data&apos;)</span><br><span class="line">plt.legend();</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><div align="center"><img src="https://num-github.github.io/2019/06/10/seaborn-1/chrome_2019-06-10_00-26-08.png" height="300" width="400"></div></details><details open><summary><font size="4">  violinplot() </font></summary>It is still used for showing distribution. The data I used here are look like that:<div align="center"><table><thead><tr><th align="center">0</th><th align="center">1</th><th align="center">2</th><th align="center">3</th></tr></thead><tbody><tr><td align="center">0.000020</td><td align="center">0.001459</td><td align="center">7.992451</td><td align="center">1.0</td></tr><tr><td align="center">0.000021</td><td align="center">0.001149</td><td align="center">6.903953</td><td align="center">0.0</td></tr><tr><td align="center">0.000029</td><td align="center">0.001637</td><td align="center">6.512294</td><td align="center">0.0</td></tr><tr><td align="center">0.000028</td><td align="center">0.001208</td><td align="center">1.936095</td><td align="center">1.0</td></tr><tr><td align="center">。。。。。。。。。。。。。。。。</td><td align="center"></td><td align="center"></td><td align="center"></td></tr></tbody></table></div>The first 3 rows are values correspond to feature 0,1,2.The third row is classes (only 0 and 1)<p>Thus, if I want to find the distribution of feature 1 in each class, we can just write </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plt.figure()</span><br><span class="line">sns.violinplot(x=3,y=1, data=pd.DataFrame(data))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><div align="center"><img src="https://num-github.github.io/2019/06/10/seaborn-1/chrome_2019-06-10_01-12-02.png" height="300" width="400"></div><ul><li>x is class row</li><li>y is feature row</li><li>Input data must be pandas data structure.</li><li>You can also input the name (string) of each row to x and y, if you have.</li></ul><p>You can also plot many figures in this way </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">f, ax1 = plt.subplots(ncols=2, nrows=2, figsize=(8, 8))</span><br><span class="line">sns.violinplot(x=3,y=0, data=pd.DataFrame(data),ax=ax1[0][0])</span><br><span class="line">ax1[0][0].set_title(&apos;x&apos;)</span><br><span class="line">sns.violinplot(x=3,y=1, data=pd.DataFrame(data),ax=ax1[0][1])</span><br><span class="line">ax1[0][1].set_title(&apos;y&apos;)</span><br><span class="line">sns.violinplot(x=3,y=2, data=pd.DataFrame(data),ax=ax1[1][0])</span><br><span class="line">ax1[1][0].set_title(&apos;z&apos;)</span><br><span class="line">f.tight_layout()</span><br></pre></td></tr></table></figure><ul><li>ax can be used to set the position of figures</li></ul><div align="center"><img src="https://num-github.github.io/2019/06/10/seaborn-1/chrome_2019-06-10_01-19-24.png" height="300" width="400"></div></details>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Get start !</title>
      <link href="/2019/06/06/Get-start/"/>
      <url>/2019/06/06/Get-start/</url>
      
        <content type="html"><![CDATA[<p>The start of my career.</p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
